// Unified LLM Client - Supports all 4 connection types
// browser-cloud, proxy-cloud, browser-local (WebLLM), proxy-local (Ollama)

const LLMClient = {
  metadata: {
    name: 'LLMClient',
    version: '1.0.0'
  },

  factory: (deps) => {
    const PROXY_URL = 'http://localhost:8000';
    let webllmEngine = null;
    let currentAbortController = null; // Track active request for cancellation

    // Provider API endpoints
    const PROVIDER_ENDPOINTS = {
      gemini: 'https://generativelanguage.googleapis.com/v1beta/models/',
      openai: 'https://api.openai.com/v1/chat/completions',
      anthropic: 'https://api.anthropic.com/v1/messages'
    };

    // Initialize WebLLM engine (lazy load)
    const initWebLLM = async () => {
      if (webllmEngine) return webllmEngine;

      if (!navigator.gpu) {
        throw new Error('WebGPU not supported in this browser');
      }

      // Dynamically import WebLLM
      const { CreateMLCEngine } = await import('https://esm.run/@mlc-ai/web-llm');
      webllmEngine = await CreateMLCEngine({
        initProgressCallback: (progress) => {
          console.log(`[WebLLM] Loading: ${progress.text}`);
        }
      });

      return webllmEngine;
    };

    // Call browser-cloud (direct API call with user's key)
    const callBrowserCloud = async (messages, modelConfig) => {
      const provider = modelConfig.provider;
      const apiKey = modelConfig.apiKey || localStorage.getItem(`${provider.toUpperCase()}_API_KEY`);

      if (!apiKey) {
        throw new Error(`No API key found for ${provider}. Please add one in model settings.`);
      }

      if (provider === 'gemini') {
        const endpoint = `${PROVIDER_ENDPOINTS.gemini}${modelConfig.id}:generateContent?key=${apiKey}`;
        const response = await fetch(endpoint, {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify({
            contents: messages.map(m => ({
              role: m.role === 'assistant' ? 'model' : 'user',
              parts: [{ text: m.content }]
            }))
          })
        });

        if (!response.ok) {
          throw new Error(`Gemini API error: ${response.status} ${response.statusText}`);
        }

        const data = await response.json();
        return {
          content: data.candidates[0].content.parts[0].text,
          usage: data.usageMetadata
        };
      }

      if (provider === 'openai') {
        const response = await fetch(PROVIDER_ENDPOINTS.openai, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'Authorization': `Bearer ${apiKey}`
          },
          body: JSON.stringify({ model: modelConfig.id, messages })
        });

        if (!response.ok) {
          throw new Error(`OpenAI API error: ${response.status} ${response.statusText}`);
        }

        const data = await response.json();
        return {
          content: data.choices[0].message.content,
          usage: data.usage
        };
      }

      if (provider === 'anthropic') {
        const response = await fetch(PROVIDER_ENDPOINTS.anthropic, {
          method: 'POST',
          headers: {
            'Content-Type': 'application/json',
            'x-api-key': apiKey,
            'anthropic-version': '2023-06-01'
          },
          body: JSON.stringify({
            model: modelConfig.id,
            messages,
            max_tokens: 4096
          })
        });

        if (!response.ok) {
          throw new Error(`Anthropic API error: ${response.status} ${response.statusText}`);
        }

        const data = await response.json();
        return {
          content: data.content[0].text,
          usage: data.usage
        };
      }

      throw new Error(`Unsupported provider: ${provider}`);
    };

    // Call proxy-cloud or proxy-local (via server proxy) with streaming
    const callProxy = async (messages, modelConfig, onStreamUpdate) => {
      // Create abort controller for this request
      currentAbortController = new AbortController();

      const response = await fetch(`${PROXY_URL}/api/chat`, {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({
          provider: modelConfig.provider,
          model: modelConfig.id,
          messages
        }),
        signal: currentAbortController.signal
      });

      if (!response.ok) {
        throw new Error(`Proxy error: ${response.status} ${response.statusText}`);
      }

      // Check if response is streaming (SSE)
      const contentType = response.headers.get('content-type');
      console.log('[LLMClient] Response content-type:', contentType);
      if (contentType && contentType.includes('text/event-stream')) {
        console.log('[LLMClient] Streaming response detected, processing...');
        // Handle streaming response
        const reader = response.body.getReader();
        const decoder = new TextDecoder();
        let buffer = '';
        let fullContent = '';
        let tokenCount = 0;
        let tokensInCurrentWindow = 0; // Track tokens in last second for rate calculation
        let lastWindowTime = null;
        const requestStartTime = Date.now();
        let firstTokenTime = null;

        // Helper to estimate token count from text
        const estimateTokens = (text) => {
          if (!text || text.trim().length === 0) return 0;
          // Rough estimation: ~0.7 words per token (i.e., 1 token â‰ˆ 0.7 words)
          // So tokens = words / 0.7 = words * 1.43
          const words = text.split(/\s+/).filter(w => w.length > 0).length;
          return Math.ceil(words / 0.7);
        };

        while (true) {
          const { done, value } = await reader.read();
          if (done) break;

          buffer += decoder.decode(value, { stream: true });
          const lines = buffer.split('\n');
          buffer = lines.pop(); // Keep incomplete line

          for (const line of lines) {
            if (line.startsWith('data: ')) {
              const dataStr = line.substring(6);
              try {
                const data = JSON.parse(dataStr);

                // Handle both thinking and content tokens
                if (data.message && (data.message.content || data.message.thinking)) {
                  const now = Date.now();

                  // Record first token time (for either thinking or content)
                  if (firstTokenTime === null) {
                    firstTokenTime = now;
                    lastWindowTime = now;
                  }

                  // Accumulate content (thinking doesn't get added to final content)
                  const previousContentLength = fullContent.length;
                  if (data.message.content) {
                    fullContent += data.message.content;
                  }

                  // Calculate new tokens in this chunk
                  const previousTokenCount = tokenCount;
                  tokenCount = estimateTokens(fullContent);
                  const newTokensInChunk = tokenCount - previousTokenCount;

                  // Track tokens in current window for rate calculation
                  tokensInCurrentWindow += newTokensInChunk;

                  // Reset window every second
                  const timeSinceLastWindow = (now - lastWindowTime) / 1000;
                  if (timeSinceLastWindow >= 1.0) {
                    lastWindowTime = now;
                    tokensInCurrentWindow = newTokensInChunk; // Reset to current chunk
                  }

                  const ttft = ((firstTokenTime - requestStartTime) / 1000).toFixed(2);
                  const streamingElapsed = (now - firstTokenTime) / 1000;
                  // Use windowed rate for more accurate tok/s, fallback to average if window too small
                  const windowedRate = timeSinceLastWindow > 0 ? (tokensInCurrentWindow / timeSinceLastWindow) : 0;
                  const averageRate = streamingElapsed > 0 ? (tokenCount / streamingElapsed) : 0;
                  const streamingRate = (windowedRate > 0 ? windowedRate : averageRate).toFixed(2);
                  const totalElapsed = ((now - requestStartTime) / 1000).toFixed(1);

                  // Call update callback for any token activity
                  if (onStreamUpdate) {
                    onStreamUpdate({
                      content: fullContent,
                      tokens: tokenCount,
                      ttft: ttft,
                      tokensPerSecond: streamingRate,
                      elapsedSeconds: totalElapsed
                    });
                  }
                }

                if (data.done) {
                  console.log(`[LLMClient] Stream complete. Total content length: ${fullContent.length}, tokens: ${tokenCount}`);
                  return {
                    content: fullContent,
                    usage: { tokens: tokenCount }
                  };
                }
              } catch (e) {
                console.error('[LLMClient] Failed to parse SSE data:', dataStr);
              }
            }
          }
        }

        return {
          content: fullContent,
          usage: { tokens: tokenCount }
        };
      } else {
        // Non-streaming response (fallback for other providers)
        const data = await response.json();
        return {
          content: data.response || data.content,
          usage: data.usage
        };
      }
    };

    // Call browser-local (WebLLM)
    const callBrowserLocal = async (messages, modelConfig) => {
      const engine = await initWebLLM();

      // Load model if not already loaded
      await engine.reload(modelConfig.id);

      const response = await engine.chat.completions.create({
        messages,
        temperature: 0.7,
        max_tokens: 2048
      });

      return {
        content: response.choices[0].message.content,
        usage: response.usage
      };
    };

    // Main chat interface
    const chat = async (messages, modelConfig, onStreamUpdate) => {
      console.log(`[LLMClient] Calling ${modelConfig.hostType} - ${modelConfig.provider}/${modelConfig.id}`);

      try {
        let result;

        switch (modelConfig.hostType) {
          case 'browser-cloud':
            result = await callBrowserCloud(messages, modelConfig);
            break;

          case 'proxy-cloud':
          case 'proxy-local':
            result = await callProxy(messages, modelConfig, onStreamUpdate);
            break;

          case 'browser-local':
            result = await callBrowserLocal(messages, modelConfig);
            break;

          default:
            throw new Error(`Unknown hostType: ${modelConfig.hostType}`);
        }

        console.log(`[LLMClient] Response received (${result.content.length} chars)`);
        return result;

      } catch (error) {
        console.error(`[LLMClient] Error:`, error);
        throw error;
      }
    };

    // Streaming interface (for future enhancement)
    const stream = async (messages, modelConfig, onToken) => {
      // TODO: Implement streaming for each provider
      // For now, fall back to regular chat
      const result = await chat(messages, modelConfig);
      onToken(result.content);
      return result;
    };

    // Abort ongoing request
    const abort = () => {
      if (currentAbortController) {
        console.log('[LLMClient] Aborting request');
        currentAbortController.abort();
        currentAbortController = null;
      }
    };

    return {
      chat,
      stream,
      abort
    };
  }
};

export default LLMClient;
