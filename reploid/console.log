[2025-10-20T21:05:21.915Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-20T21:05:21.916Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-20T21:05:21.922Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": null
}
[2025-10-20T21:05:21.922Z] [LOG] [API] Checking server status...
[2025-10-20T21:05:21.926Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-20T21:05:21.923Z"
}
[2025-10-20T21:05:21.927Z] [LOG] [API] WebGPU available
[2025-10-20T21:05:21.928Z] [LOG] [Boot] Proxy server available: true
[2025-10-20T21:05:23.460Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-20T21:05:23.460Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-20T21:05:23.464Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": null
}
[2025-10-20T21:05:23.464Z] [LOG] [API] Checking server status...
[2025-10-20T21:05:23.468Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-20T21:05:23.465Z"
}
[2025-10-20T21:05:23.468Z] [LOG] [API] WebGPU available
[2025-10-20T21:05:23.470Z] [LOG] [Boot] Proxy server available: true
[2025-10-20T21:05:25.588Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-20T21:05:25.588Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-20T21:05:25.605Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": null
}
[2025-10-20T21:05:25.605Z] [LOG] [API] Checking server status...
[2025-10-20T21:05:25.607Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-20T21:05:25.606Z"
}
[2025-10-20T21:05:25.607Z] [LOG] [API] WebGPU available
[2025-10-20T21:05:25.608Z] [LOG] [Boot] Proxy server available: true
[2025-10-20T21:05:27.248Z] [INFO] {"timestamp":"2025-10-20T21:05:27.248Z","level":"INFO","message":"[CoreLogic] Registered module: ToastNotifications from /upgrades/toast-notifications.js","details":{}}
[2025-10-20T21:05:27.248Z] [INFO] {"timestamp":"2025-10-20T21:05:27.248Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-20T21:05:27.249Z] [INFO] {"timestamp":"2025-10-20T21:05:27.248Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-20T21:05:27.249Z] [INFO] {"timestamp":"2025-10-20T21:05:27.249Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-20T21:05:27.249Z] [INFO] {"timestamp":"2025-10-20T21:05:27.249Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-20T21:05:27.251Z] [INFO] {"timestamp":"2025-10-20T21:05:27.251Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-20T21:05:27.251Z] [INFO] {"timestamp":"2025-10-20T21:05:27.251Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-20T21:05:27.252Z] [INFO] {"timestamp":"2025-10-20T21:05:27.252Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-20T21:05:27.252Z] [INFO] {"timestamp":"2025-10-20T21:05:27.252Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-20T21:05:27.253Z] [INFO] {"timestamp":"2025-10-20T21:05:27.253Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-20T21:05:27.253Z] [INFO] {"timestamp":"2025-10-20T21:05:27.253Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-20T21:05:27.254Z] [INFO] {"timestamp":"2025-10-20T21:05:27.254Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-20T21:05:27.254Z] [INFO] {"timestamp":"2025-10-20T21:05:27.254Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-20T21:05:27.255Z] [INFO] {"timestamp":"2025-10-20T21:05:27.255Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-20T21:05:27.255Z] [INFO] {"timestamp":"2025-10-20T21:05:27.255Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-20T21:05:27.256Z] [INFO] {"timestamp":"2025-10-20T21:05:27.256Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-20T21:05:27.256Z] [INFO] {"timestamp":"2025-10-20T21:05:27.256Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-20T21:05:27.257Z] [INFO] {"timestamp":"2025-10-20T21:05:27.257Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-20T21:05:27.257Z] [INFO] {"timestamp":"2025-10-20T21:05:27.257Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-20T21:05:27.258Z] [INFO] {"timestamp":"2025-10-20T21:05:27.258Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-20T21:05:27.258Z] [INFO] {"timestamp":"2025-10-20T21:05:27.258Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-20T21:05:27.259Z] [INFO] {"timestamp":"2025-10-20T21:05:27.259Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-20T21:05:27.259Z] [INFO] {"timestamp":"2025-10-20T21:05:27.259Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-20T21:05:27.259Z] [INFO] {"timestamp":"2025-10-20T21:05:27.259Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-20T21:05:27.259Z] [INFO] {"timestamp":"2025-10-20T21:05:27.259Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-20T21:05:27.260Z] [INFO] {"timestamp":"2025-10-20T21:05:27.260Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-20T21:05:27.261Z] [INFO] {"timestamp":"2025-10-20T21:05:27.260Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-20T21:05:27.261Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-20T21:05:27.261Z] [ERROR] {"timestamp":"2025-10-20T21:05:27.261Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-20T21:05:27.261Z] [WARN] {"timestamp":"2025-10-20T21:05:27.261Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-20T21:05:27.261Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-20T21:05:27.261Z] [ERROR] {"timestamp":"2025-10-20T21:05:27.261Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-20T21:05:27.261Z] [WARN] {"timestamp":"2025-10-20T21:05:27.261Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-20T21:05:27.262Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-20T21:05:27.262Z] [ERROR] {"timestamp":"2025-10-20T21:05:27.262Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-20T21:05:27.262Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-20T21:05:27.262Z] [WARN] {"timestamp":"2025-10-20T21:05:27.262Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-20T21:05:27.263Z] [INFO] {"timestamp":"2025-10-20T21:05:27.263Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-20T21:05:27.263Z] [INFO] {"timestamp":"2025-10-20T21:05:27.263Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-20T21:05:27.263Z] [INFO] {"timestamp":"2025-10-20T21:05:27.263Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-20T21:05:27.263Z] [INFO] {"timestamp":"2025-10-20T21:05:27.263Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-20T21:05:27.265Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-20T21:05:27.265Z] [INFO] {"timestamp":"2025-10-20T21:05:27.265Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-20T21:05:27.265Z] [INFO] {"timestamp":"2025-10-20T21:05:27.265Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-20T21:05:27.266Z] [INFO] {"timestamp":"2025-10-20T21:05:27.266Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-20T21:05:27.266Z] [INFO] {"timestamp":"2025-10-20T21:05:27.266Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-20T21:05:27.267Z] [INFO] {"timestamp":"2025-10-20T21:05:27.267Z","level":"INFO","message":"[DIContainer] Registered module: GitVFS","details":{}}
[2025-10-20T21:05:27.267Z] [INFO] {"timestamp":"2025-10-20T21:05:27.267Z","level":"INFO","message":"[CoreLogic] Registered module: GitVFS from /upgrades/git-vfs.js","details":{}}
[2025-10-20T21:05:27.268Z] [INFO] {"timestamp":"2025-10-20T21:05:27.268Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-20T21:05:27.268Z] [INFO] {"timestamp":"2025-10-20T21:05:27.268Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-20T21:05:27.269Z] [INFO] {"timestamp":"2025-10-20T21:05:27.269Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-20T21:05:27.269Z] [INFO] {"timestamp":"2025-10-20T21:05:27.269Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-20T21:05:27.270Z] [INFO] {"timestamp":"2025-10-20T21:05:27.270Z","level":"INFO","message":"[DIContainer] Registered module: DiffViewerUI","details":{}}
[2025-10-20T21:05:27.270Z] [INFO] {"timestamp":"2025-10-20T21:05:27.270Z","level":"INFO","message":"[CoreLogic] Registered module: DiffViewerUI from /upgrades/diff-viewer-ui.js","details":{}}
[2025-10-20T21:05:27.272Z] [INFO] {"timestamp":"2025-10-20T21:05:27.272Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCSwarm","details":{}}
[2025-10-20T21:05:27.272Z] [INFO] {"timestamp":"2025-10-20T21:05:27.272Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCSwarm from /upgrades/webrtc-swarm.js","details":{}}
[2025-10-20T21:05:27.273Z] [INFO] {"timestamp":"2025-10-20T21:05:27.273Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCCoordinator","details":{}}
[2025-10-20T21:05:27.273Z] [INFO] {"timestamp":"2025-10-20T21:05:27.273Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCCoordinator from /upgrades/webrtc-coordinator.js","details":{}}
[2025-10-20T21:05:27.274Z] [INFO] {"timestamp":"2025-10-20T21:05:27.274Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-20T21:05:27.274Z] [INFO] {"timestamp":"2025-10-20T21:05:27.274Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-20T21:05:27.275Z] [INFO] {"timestamp":"2025-10-20T21:05:27.275Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-20T21:05:27.275Z] [INFO] {"timestamp":"2025-10-20T21:05:27.275Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-20T21:05:27.276Z] [INFO] {"timestamp":"2025-10-20T21:05:27.276Z","level":"INFO","message":"[DIContainer] Registered module: MultiModelPaxos","details":{}}
[2025-10-20T21:05:27.276Z] [INFO] {"timestamp":"2025-10-20T21:05:27.276Z","level":"INFO","message":"[CoreLogic] Registered module: MultiModelPaxos from /upgrades/multi-model-paxos.js","details":{}}
[2025-10-20T21:05:27.276Z] [INFO] {"timestamp":"2025-10-20T21:05:27.276Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-20T21:05:27.278Z] [INFO] {"timestamp":"2025-10-20T21:05:27.277Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-20T21:05:27.279Z] [WARN] {"timestamp":"2025-10-20T21:05:27.279Z","level":"WARN","message":"[Storage-Git] No Git repository found, initializing a new one.","details":{}}
[2025-10-20T21:05:27.281Z] [INFO] {"timestamp":"2025-10-20T21:05:27.281Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-20T21:05:27.282Z] [INFO] {"timestamp":"2025-10-20T21:05:27.282Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-20T21:05:27.282Z] [WARN] {"timestamp":"2025-10-20T21:05:27.282Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-20T21:05:27.283Z] [INFO] {"timestamp":"2025-10-20T21:05:27.282Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-20T21:05:27.283Z] [INFO] {"timestamp":"2025-10-20T21:05:27.283Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-20T21:05:27.283Z] [WARN] {"timestamp":"2025-10-20T21:05:27.283Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-20T21:05:27.284Z] [INFO] {"timestamp":"2025-10-20T21:05:27.284Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-20T21:05:27.284Z] [WARN] {"timestamp":"2025-10-20T21:05:27.284Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-20T21:05:27.285Z] [INFO] {"timestamp":"2025-10-20T21:05:27.285Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-20T21:05:27.285Z] [INFO] {"timestamp":"2025-10-20T21:05:27.285Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-20T21:05:27.286Z] [INFO] {"timestamp":"2025-10-20T21:05:27.286Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-20T21:05:27.289Z] [INFO] {"timestamp":"2025-10-20T21:05:27.289Z","level":"INFO","message":"[ReflectionStore] Initializing reflection persistence","details":{}}
[2025-10-20T21:05:27.289Z] [INFO] {"timestamp":"2025-10-20T21:05:27.289Z","level":"INFO","message":"[ReflectionStore] Creating database schema","details":{}}
[2025-10-20T21:05:27.289Z] [INFO] {"timestamp":"2025-10-20T21:05:27.289Z","level":"INFO","message":"[ReflectionStore] Database schema created","details":{}}
[2025-10-20T21:05:27.289Z] [INFO] {"timestamp":"2025-10-20T21:05:27.289Z","level":"INFO","message":"[ReflectionStore] Database opened successfully","details":{}}
[2025-10-20T21:05:27.290Z] [INFO] {"timestamp":"2025-10-20T21:05:27.290Z","level":"INFO","message":"[SelfTester] Initialized","details":{}}
[2025-10-20T21:05:27.290Z] [INFO] {"timestamp":"2025-10-20T21:05:27.290Z","level":"INFO","message":"[BrowserAPIs] Initializing web API integration...","details":{}}
[2025-10-20T21:05:27.290Z] [INFO] {"timestamp":"2025-10-20T21:05:27.290Z","level":"INFO","message":"[BrowserAPIs] Capabilities detected:","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-20T21:05:27.290Z] [INFO] {"timestamp":"2025-10-20T21:05:27.290Z","level":"INFO","message":"[EventBus] Emitting event: browser-apis:initialized","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-20T21:05:27.292Z] [INFO] {"timestamp":"2025-10-20T21:05:27.292Z","level":"INFO","message":"[GitVFS] Initializing new repository","details":{}}
[2025-10-20T21:05:27.308Z] [INFO] {"timestamp":"2025-10-20T21:05:27.308Z","level":"INFO","message":"[GitVFS] Repository initialized","details":{}}
[2025-10-20T21:05:27.308Z] [INFO] {"timestamp":"2025-10-20T21:05:27.308Z","level":"INFO","message":"[GitVFS] Initialized successfully","details":{}}
[2025-10-20T21:05:27.309Z] [INFO] {"timestamp":"2025-10-20T21:05:27.309Z","level":"INFO","message":"[WebRTCSwarm] Initializing swarm module","details":{}}
[2025-10-20T21:05:27.309Z] [INFO] {"timestamp":"2025-10-20T21:05:27.309Z","level":"INFO","message":"[WebRTCSwarm] Local peer ID: reploid-id_1760994327309_yxs9eybrn","details":{}}
[2025-10-20T21:05:27.309Z] [INFO] {"timestamp":"2025-10-20T21:05:27.309Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-20T21:05:27.309Z] [INFO] {"timestamp":"2025-10-20T21:05:27.309Z","level":"INFO","message":"[WebRTCSwarm] Swarm initialized","details":{}}
[2025-10-20T21:05:27.310Z] [INFO] {"timestamp":"2025-10-20T21:05:27.310Z","level":"INFO","message":"[SwarmOrch] Initializing swarm orchestrator","details":{}}
[2025-10-20T21:05:27.310Z] [WARN] {"timestamp":"2025-10-20T21:05:27.310Z","level":"WARN","message":"[WebRTCSwarm] Cannot send signaling message: not connected","details":{}}
[2025-10-20T21:05:27.310Z] [INFO] {"timestamp":"2025-10-20T21:05:27.310Z","level":"INFO","message":"[SwarmOrch] Swarm orchestrator initialized","details":{"capabilities":["code-generation","file-management"]}}
[2025-10-20T21:05:27.312Z] [INFO] {"timestamp":"2025-10-20T21:05:27.312Z","level":"INFO","message":"[PyodideRuntime] Initializing Pyodide runtime...","details":{}}
[2025-10-20T21:05:27.312Z] [INFO] {"timestamp":"2025-10-20T21:05:27.312Z","level":"INFO","message":"[PyodideRuntime] Worker created","details":{}}
[2025-10-20T21:05:27.313Z] [INFO] {"timestamp":"2025-10-20T21:05:27.313Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-20T21:05:27.314Z] [INFO] {"timestamp":"2025-10-20T21:05:27.314Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-20T21:05:28.251Z] [INFO] {"timestamp":"2025-10-20T21:05:28.251Z","level":"INFO","message":"[PyodideRuntime] Pyodide initialized","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-20T21:05:28.251Z] [INFO] {"timestamp":"2025-10-20T21:05:28.251Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:ready","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-20T21:05:28.251Z] [INFO] {"timestamp":"2025-10-20T21:05:28.251Z","level":"INFO","message":"[PyodideRuntime] Pyodide runtime ready","details":{}}
[2025-10-20T21:05:28.252Z] [INFO] {"timestamp":"2025-10-20T21:05:28.251Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:initialized","details":{"ready":true}}
[2025-10-20T21:05:28.254Z] [INFO] {"timestamp":"2025-10-20T21:05:28.254Z","level":"INFO","message":"Dashboard UI Manager (Event-Driven) taking control of DOM...","details":{}}
[2025-10-20T21:05:28.264Z] [INFO] {"timestamp":"2025-10-20T21:05:28.264Z","level":"INFO","message":"[UIManager] Initializing modular panel support...","details":{}}
[2025-10-20T21:05:28.264Z] [INFO] {"timestamp":"2025-10-20T21:05:28.264Z","level":"INFO","message":"[UIManager] Modular panel initialization complete","details":{}}
[2025-10-20T21:05:28.264Z] [INFO] {"timestamp":"2025-10-20T21:05:28.264Z","level":"INFO","message":"Dashboard UI Initialized. Listening for events.","details":{}}
[2025-10-20T21:05:28.264Z] [LOG] [CoreLogic] Attempting to initialize DiffViewerUI...
[2025-10-20T21:05:28.264Z] [LOG] [CoreLogic] Resolving DiffViewerUI from container...
[2025-10-20T21:05:28.264Z] [LOG] [CoreLogic] DiffViewerUI resolved: undefined
[2025-10-20T21:05:28.264Z] [LOG] [CoreLogic] DiffViewerUI.init exists? undefined
[2025-10-20T21:05:28.264Z] [WARN] [CoreLogic] DiffViewerUI or DiffViewerUI.init not available
[2025-10-20T21:05:28.264Z] [INFO] {"timestamp":"2025-10-20T21:05:28.264Z","level":"INFO","message":"[CoreLogic] Agent initialization complete. System is operational.","details":{}}
[2025-10-20T21:05:28.264Z] [INFO] {"timestamp":"2025-10-20T21:05:28.264Z","level":"INFO","message":"[CoreLogic] Creating genesis snapshot of initial boot state...","details":{}}
[2025-10-20T21:05:28.264Z] [WARN] {"timestamp":"2025-10-20T21:05:28.264Z","level":"WARN","message":"[CoreLogic] Failed to create genesis snapshot (non-fatal):","details":"[DIContainer] Service not found: GenesisSnapshot\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Storage, AuditLogger, RateLimiter, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, DiffViewerUI, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, MultiModelPaxos\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-20T21:05:28.265Z] [INFO] {"timestamp":"2025-10-20T21:05:28.265Z","level":"INFO","message":"[CoreLogic] Starting agent with initial goal:","details":"Final verification - agent operational"}
[2025-10-20T21:05:28.265Z] [INFO] {"timestamp":"2025-10-20T21:05:28.265Z","level":"INFO","message":"[UI] updateGoal called with:","details":{"text":"Final verification - agent operational","hasGoalTextRef":true}}
[2025-10-20T21:05:28.265Z] [INFO] {"timestamp":"2025-10-20T21:05:28.265Z","level":"INFO","message":"[UI] Goal text element updated successfully","details":{}}
[2025-10-20T21:05:28.281Z] [INFO] {"timestamp":"2025-10-20T21:05:28.280Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1760994328274_b5e2c389e7037d52/session.json (SHA: e146cad)","details":{}}
[2025-10-20T21:05:28.281Z] [INFO] {"timestamp":"2025-10-20T21:05:28.281Z","level":"INFO","message":"[SessionManager] Created new session: session_1760994328274_b5e2c389e7037d52","details":{}}
[2025-10-20T21:05:28.287Z] [INFO] {"timestamp":"2025-10-20T21:05:28.287Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.checkpoints/checkpoint_1760994328281.json (SHA: 2fde409)","details":{}}
[2025-10-20T21:05:28.287Z] [INFO] {"timestamp":"2025-10-20T21:05:28.287Z","level":"INFO","message":"[StateManager] Created checkpoint: checkpoint_1760994328281 - Session session_1760994328274_b5e2c389e7037d52 - Turn 0 start","details":{}}
[2025-10-20T21:05:28.287Z] [INFO] {"timestamp":"2025-10-20T21:05:28.287Z","level":"INFO","message":"[SessionManager] Created turn checkpoint: checkpoint_1760994328281","details":{}}
[2025-10-20T21:05:28.291Z] [INFO] {"timestamp":"2025-10-20T21:05:28.291Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1760994328274_b5e2c389e7037d52/session.json (SHA: 2051c8b)","details":{}}
[2025-10-20T21:05:28.291Z] [INFO] {"timestamp":"2025-10-20T21:05:28.291Z","level":"INFO","message":"[SessionManager] Created turn 0 for session session_1760994328274_b5e2c389e7037d52","details":{}}
[2025-10-20T21:05:28.292Z] [INFO] {"timestamp":"2025-10-20T21:05:28.291Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"","progress":null}}
[2025-10-20T21:05:28.292Z] [INFO] {"timestamp":"2025-10-20T21:05:28.292Z","level":"INFO","message":"[SentinelFSM] State transition: IDLE -> CURATING_CONTEXT","details":{}}
[2025-10-20T21:05:28.292Z] [INFO] {"timestamp":"2025-10-20T21:05:28.292Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"IDLE","newState":"CURATING_CONTEXT","context":{"goal":"Final verification - agent operational","sessionId":"session_1760994328274_b5e2c389e7037d52","turn":{"turn":0,"cats_path":"/sessions/session_1760994328274_b5e2c389e7037d52/turn-0.cats.md","dogs_path":"/sessions/session_1760994328274_b5e2c389e7037d52/turn-0.dogs.md","status":"pending_context","checkpointId":"checkpoint_1760994328281","createdAt":"2025-10-20T21:05:28.287Z"},"startTime":1760994328291,"iterations":0,"maxIterations":10}}}
[2025-10-20T21:05:28.292Z] [INFO] {"timestamp":"2025-10-20T21:05:28.292Z","level":"INFO","message":"[SentinelFSM] Executing state: CURATING_CONTEXT","details":{}}
[2025-10-20T21:05:28.292Z] [INFO] {"timestamp":"2025-10-20T21:05:28.292Z","level":"INFO","message":"[EventBus] Emitting event: agent:curating","details":{"goal":"Final verification - agent operational"}}
[2025-10-20T21:05:28.292Z] [INFO] {"timestamp":"2025-10-20T21:05:28.292Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Analyzing project files...","progress":null}}
[2025-10-20T21:05:28.292Z] [WARN] {"timestamp":"2025-10-20T21:05:28.292Z","level":"WARN","message":"[SentinelTools] No files available for curation","details":{}}
[2025-10-20T21:05:28.292Z] [INFO] {"timestamp":"2025-10-20T21:05:28.292Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Found 0 relevant files","progress":50}}
[2025-10-20T21:05:28.298Z] [INFO] {"timestamp":"2025-10-20T21:05:28.297Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1760994328274_b5e2c389e7037d52/turn-0.cats.md (SHA: 23b3892)","details":{}}
[2025-10-20T21:05:28.298Z] [INFO] {"timestamp":"2025-10-20T21:05:28.298Z","level":"INFO","message":"[AuditLogger] VFS_CREATE","details":{"path":"/sessions/session_1760994328274_b5e2c389e7037d52/turn-0.cats.md","type":"markdown","size":123,"description":"Context bundle for turn: Context for goal: Final verification - agent operational"}}
[2025-10-20T21:05:28.306Z] [INFO] {"timestamp":"2025-10-20T21:05:28.306Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.audit/2025-10-20.jsonl (SHA: 7bc6611)","details":{}}
[2025-10-20T21:05:28.306Z] [INFO] {"timestamp":"2025-10-20T21:05:28.306Z","level":"INFO","message":"[StateManager-Git] Created artifact: /sessions/session_1760994328274_b5e2c389e7037d52/turn-0.cats.md","details":{}}
[2025-10-20T21:05:28.306Z] [INFO] {"timestamp":"2025-10-20T21:05:28.306Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"AWAITING_CONTEXT_APPROVAL","detail":"","progress":null}}
[2025-10-20T21:05:28.306Z] [INFO] {"timestamp":"2025-10-20T21:05:28.306Z","level":"INFO","message":"[SentinelFSM] State transition: CURATING_CONTEXT -> AWAITING_CONTEXT_APPROVAL","details":{}}
[2025-10-20T21:05:28.306Z] [INFO] {"timestamp":"2025-10-20T21:05:28.306Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"CURATING_CONTEXT","newState":"AWAITING_CONTEXT_APPROVAL","context":{"goal":"Final verification - agent operational","sessionId":"session_1760994328274_b5e2c389e7037d52","turn":{"turn":0,"cats_path":"/sessions/session_1760994328274_b5e2c389e7037d52/turn-0.cats.md","dogs_path":"/sessions/session_1760994328274_b5e2c389e7037d52/turn-0.dogs.md","status":"pending_context","checkpointId":"checkpoint_1760994328281","createdAt":"2025-10-20T21:05:28.287Z"},"startTime":1760994328291,"iterations":0,"maxIterations":10,"catsPath":"/sessions/session_1760994328274_b5e2c389e7037d52/turn-0.cats.md"}}}
[2025-10-20T21:05:28.306Z] [INFO] {"timestamp":"2025-10-20T21:05:28.306Z","level":"INFO","message":"[SentinelFSM] Executing state: AWAITING_CONTEXT_APPROVAL","details":{}}
[2025-10-20T21:05:28.307Z] [INFO] {"timestamp":"2025-10-20T21:05:28.306Z","level":"INFO","message":"[EventBus] Emitting event: agent:awaiting:context","details":{"cats_path":"/sessions/session_1760994328274_b5e2c389e7037d52/turn-0.cats.md","session_id":"session_1760994328274_b5e2c389e7037d52"}}
[2025-10-20T21:05:28.307Z] [INFO] {"timestamp":"2025-10-20T21:05:28.307Z","level":"INFO","message":"[CoreLogic] Agent cycle started successfully","details":{}}
[2025-10-20T21:05:28.307Z] [LOG] [Boot] Agent system awakened successfully
[2025-10-20T21:13:36.211Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-20T21:13:36.212Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-20T21:13:36.219Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-20T21:13:36.219Z] [LOG] [API] Checking server status...
[2025-10-20T21:13:36.223Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-20T21:13:36.220Z"
}
[2025-10-20T21:13:36.223Z] [LOG] [API] WebGPU available
[2025-10-20T21:13:36.226Z] [LOG] [Boot] Proxy server available: true
[2025-10-20T21:13:41.599Z] [INFO] {"timestamp":"2025-10-20T21:13:41.599Z","level":"INFO","message":"[CoreLogic] Registered module: ToastNotifications from /upgrades/toast-notifications.js","details":{}}
[2025-10-20T21:13:41.600Z] [INFO] {"timestamp":"2025-10-20T21:13:41.599Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-20T21:13:41.600Z] [INFO] {"timestamp":"2025-10-20T21:13:41.600Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-20T21:13:41.600Z] [INFO] {"timestamp":"2025-10-20T21:13:41.600Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-20T21:13:41.600Z] [INFO] {"timestamp":"2025-10-20T21:13:41.600Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-20T21:13:41.601Z] [INFO] {"timestamp":"2025-10-20T21:13:41.601Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-20T21:13:41.601Z] [INFO] {"timestamp":"2025-10-20T21:13:41.601Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-20T21:13:41.602Z] [INFO] {"timestamp":"2025-10-20T21:13:41.602Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-20T21:13:41.602Z] [INFO] {"timestamp":"2025-10-20T21:13:41.602Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-20T21:13:41.603Z] [INFO] {"timestamp":"2025-10-20T21:13:41.603Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-20T21:13:41.603Z] [INFO] {"timestamp":"2025-10-20T21:13:41.603Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-20T21:13:41.604Z] [INFO] {"timestamp":"2025-10-20T21:13:41.604Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-20T21:13:41.604Z] [INFO] {"timestamp":"2025-10-20T21:13:41.604Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-20T21:13:41.605Z] [INFO] {"timestamp":"2025-10-20T21:13:41.605Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-20T21:13:41.605Z] [INFO] {"timestamp":"2025-10-20T21:13:41.605Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-20T21:13:41.606Z] [INFO] {"timestamp":"2025-10-20T21:13:41.605Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-20T21:13:41.606Z] [INFO] {"timestamp":"2025-10-20T21:13:41.606Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-20T21:13:41.606Z] [INFO] {"timestamp":"2025-10-20T21:13:41.606Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-20T21:13:41.606Z] [INFO] {"timestamp":"2025-10-20T21:13:41.606Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-20T21:13:41.607Z] [INFO] {"timestamp":"2025-10-20T21:13:41.607Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-20T21:13:41.607Z] [INFO] {"timestamp":"2025-10-20T21:13:41.607Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-20T21:13:41.608Z] [INFO] {"timestamp":"2025-10-20T21:13:41.608Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-20T21:13:41.608Z] [INFO] {"timestamp":"2025-10-20T21:13:41.608Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-20T21:13:41.608Z] [INFO] {"timestamp":"2025-10-20T21:13:41.608Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-20T21:13:41.608Z] [INFO] {"timestamp":"2025-10-20T21:13:41.608Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-20T21:13:41.609Z] [INFO] {"timestamp":"2025-10-20T21:13:41.609Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-20T21:13:41.609Z] [INFO] {"timestamp":"2025-10-20T21:13:41.609Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-20T21:13:41.609Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-20T21:13:41.609Z] [ERROR] {"timestamp":"2025-10-20T21:13:41.609Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-20T21:13:41.609Z] [WARN] {"timestamp":"2025-10-20T21:13:41.609Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-20T21:13:41.610Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-20T21:13:41.610Z] [ERROR] {"timestamp":"2025-10-20T21:13:41.610Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-20T21:13:41.610Z] [WARN] {"timestamp":"2025-10-20T21:13:41.610Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-20T21:13:41.610Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-20T21:13:41.610Z] [ERROR] {"timestamp":"2025-10-20T21:13:41.610Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-20T21:13:41.610Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-20T21:13:41.610Z] [WARN] {"timestamp":"2025-10-20T21:13:41.610Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-20T21:13:41.612Z] [INFO] {"timestamp":"2025-10-20T21:13:41.612Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-20T21:13:41.612Z] [INFO] {"timestamp":"2025-10-20T21:13:41.612Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-20T21:13:41.613Z] [INFO] {"timestamp":"2025-10-20T21:13:41.613Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-20T21:13:41.613Z] [INFO] {"timestamp":"2025-10-20T21:13:41.613Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-20T21:13:41.615Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-20T21:13:41.615Z] [INFO] {"timestamp":"2025-10-20T21:13:41.615Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-20T21:13:41.615Z] [INFO] {"timestamp":"2025-10-20T21:13:41.615Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-20T21:13:41.616Z] [INFO] {"timestamp":"2025-10-20T21:13:41.616Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-20T21:13:41.616Z] [INFO] {"timestamp":"2025-10-20T21:13:41.616Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-20T21:13:41.617Z] [INFO] {"timestamp":"2025-10-20T21:13:41.617Z","level":"INFO","message":"[DIContainer] Registered module: GitVFS","details":{}}
[2025-10-20T21:13:41.617Z] [INFO] {"timestamp":"2025-10-20T21:13:41.617Z","level":"INFO","message":"[CoreLogic] Registered module: GitVFS from /upgrades/git-vfs.js","details":{}}
[2025-10-20T21:13:41.619Z] [INFO] {"timestamp":"2025-10-20T21:13:41.619Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-20T21:13:41.619Z] [INFO] {"timestamp":"2025-10-20T21:13:41.619Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-20T21:13:41.622Z] [INFO] {"timestamp":"2025-10-20T21:13:41.622Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-20T21:13:41.622Z] [INFO] {"timestamp":"2025-10-20T21:13:41.622Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-20T21:13:41.624Z] [INFO] {"timestamp":"2025-10-20T21:13:41.624Z","level":"INFO","message":"[DIContainer] Registered module: DiffViewerUI","details":{}}
[2025-10-20T21:13:41.624Z] [INFO] {"timestamp":"2025-10-20T21:13:41.624Z","level":"INFO","message":"[CoreLogic] Registered module: DiffViewerUI from /upgrades/diff-viewer-ui.js","details":{}}
[2025-10-20T21:13:41.625Z] [INFO] {"timestamp":"2025-10-20T21:13:41.625Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCSwarm","details":{}}
[2025-10-20T21:13:41.625Z] [INFO] {"timestamp":"2025-10-20T21:13:41.625Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCSwarm from /upgrades/webrtc-swarm.js","details":{}}
[2025-10-20T21:13:41.626Z] [INFO] {"timestamp":"2025-10-20T21:13:41.626Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCCoordinator","details":{}}
[2025-10-20T21:13:41.626Z] [INFO] {"timestamp":"2025-10-20T21:13:41.626Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCCoordinator from /upgrades/webrtc-coordinator.js","details":{}}
[2025-10-20T21:13:41.627Z] [INFO] {"timestamp":"2025-10-20T21:13:41.627Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-20T21:13:41.627Z] [INFO] {"timestamp":"2025-10-20T21:13:41.627Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-20T21:13:41.628Z] [INFO] {"timestamp":"2025-10-20T21:13:41.628Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-20T21:13:41.628Z] [INFO] {"timestamp":"2025-10-20T21:13:41.628Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-20T21:13:41.628Z] [INFO] {"timestamp":"2025-10-20T21:13:41.628Z","level":"INFO","message":"[DIContainer] Registered module: MultiModelPaxos","details":{}}
[2025-10-20T21:13:41.628Z] [INFO] {"timestamp":"2025-10-20T21:13:41.628Z","level":"INFO","message":"[CoreLogic] Registered module: MultiModelPaxos from /upgrades/multi-model-paxos.js","details":{}}
[2025-10-20T21:13:41.628Z] [INFO] {"timestamp":"2025-10-20T21:13:41.628Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-20T21:13:41.630Z] [INFO] {"timestamp":"2025-10-20T21:13:41.630Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-20T21:13:41.631Z] [INFO] {"timestamp":"2025-10-20T21:13:41.631Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-20T21:13:41.631Z] [INFO] {"timestamp":"2025-10-20T21:13:41.631Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-20T21:13:41.632Z] [INFO] {"timestamp":"2025-10-20T21:13:41.632Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-20T21:13:41.632Z] [WARN] {"timestamp":"2025-10-20T21:13:41.632Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-20T21:13:41.632Z] [INFO] {"timestamp":"2025-10-20T21:13:41.632Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-20T21:13:41.632Z] [INFO] {"timestamp":"2025-10-20T21:13:41.632Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-20T21:13:41.632Z] [WARN] {"timestamp":"2025-10-20T21:13:41.632Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-20T21:13:41.633Z] [INFO] {"timestamp":"2025-10-20T21:13:41.633Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-20T21:13:41.634Z] [WARN] {"timestamp":"2025-10-20T21:13:41.634Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-20T21:13:41.634Z] [INFO] {"timestamp":"2025-10-20T21:13:41.634Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-20T21:13:41.634Z] [INFO] {"timestamp":"2025-10-20T21:13:41.634Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-20T21:13:41.634Z] [INFO] {"timestamp":"2025-10-20T21:13:41.634Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-20T21:13:41.637Z] [INFO] {"timestamp":"2025-10-20T21:13:41.637Z","level":"INFO","message":"[ReflectionStore] Initializing reflection persistence","details":{}}
[2025-10-20T21:13:41.652Z] [INFO] {"timestamp":"2025-10-20T21:13:41.652Z","level":"INFO","message":"[ReflectionStore] Creating database schema","details":{}}
[2025-10-20T21:13:41.652Z] [INFO] {"timestamp":"2025-10-20T21:13:41.652Z","level":"INFO","message":"[ReflectionStore] Database schema created","details":{}}
[2025-10-20T21:13:41.659Z] [INFO] {"timestamp":"2025-10-20T21:13:41.659Z","level":"INFO","message":"[ReflectionStore] Database opened successfully","details":{}}
[2025-10-20T21:13:41.659Z] [INFO] {"timestamp":"2025-10-20T21:13:41.659Z","level":"INFO","message":"[SelfTester] Initialized","details":{}}
[2025-10-20T21:13:41.660Z] [INFO] {"timestamp":"2025-10-20T21:13:41.660Z","level":"INFO","message":"[BrowserAPIs] Initializing web API integration...","details":{}}
[2025-10-20T21:13:41.660Z] [INFO] {"timestamp":"2025-10-20T21:13:41.660Z","level":"INFO","message":"[BrowserAPIs] Capabilities detected:","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-20T21:13:41.660Z] [INFO] {"timestamp":"2025-10-20T21:13:41.660Z","level":"INFO","message":"[EventBus] Emitting event: browser-apis:initialized","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-20T21:13:41.676Z] [INFO] {"timestamp":"2025-10-20T21:13:41.676Z","level":"INFO","message":"[GitVFS] Initializing new repository","details":{}}
[2025-10-20T21:13:41.688Z] [INFO] {"timestamp":"2025-10-20T21:13:41.688Z","level":"INFO","message":"[GitVFS] Repository initialized","details":{}}
[2025-10-20T21:13:41.688Z] [INFO] {"timestamp":"2025-10-20T21:13:41.688Z","level":"INFO","message":"[GitVFS] Initialized successfully","details":{}}
[2025-10-20T21:13:41.688Z] [INFO] {"timestamp":"2025-10-20T21:13:41.688Z","level":"INFO","message":"[WebRTCSwarm] Initializing swarm module","details":{}}
[2025-10-20T21:13:41.688Z] [INFO] {"timestamp":"2025-10-20T21:13:41.688Z","level":"INFO","message":"[WebRTCSwarm] Local peer ID: reploid-id_1760994821688_k98zmlop1","details":{}}
[2025-10-20T21:13:41.689Z] [INFO] {"timestamp":"2025-10-20T21:13:41.688Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-20T21:13:41.689Z] [INFO] {"timestamp":"2025-10-20T21:13:41.689Z","level":"INFO","message":"[WebRTCSwarm] Swarm initialized","details":{}}
[2025-10-20T21:13:41.689Z] [INFO] {"timestamp":"2025-10-20T21:13:41.689Z","level":"INFO","message":"[SwarmOrch] Initializing swarm orchestrator","details":{}}
[2025-10-20T21:13:41.689Z] [WARN] {"timestamp":"2025-10-20T21:13:41.689Z","level":"WARN","message":"[WebRTCSwarm] Cannot send signaling message: not connected","details":{}}
[2025-10-20T21:13:41.689Z] [INFO] {"timestamp":"2025-10-20T21:13:41.689Z","level":"INFO","message":"[SwarmOrch] Swarm orchestrator initialized","details":{"capabilities":["code-generation","file-management"]}}
[2025-10-20T21:13:41.692Z] [INFO] {"timestamp":"2025-10-20T21:13:41.692Z","level":"INFO","message":"[PyodideRuntime] Initializing Pyodide runtime...","details":{}}
[2025-10-20T21:13:41.692Z] [INFO] {"timestamp":"2025-10-20T21:13:41.692Z","level":"INFO","message":"[PyodideRuntime] Worker created","details":{}}
[2025-10-20T21:13:41.692Z] [INFO] {"timestamp":"2025-10-20T21:13:41.692Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-20T21:13:41.693Z] [INFO] {"timestamp":"2025-10-20T21:13:41.693Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-20T21:13:42.735Z] [INFO] {"timestamp":"2025-10-20T21:13:42.735Z","level":"INFO","message":"[PyodideRuntime] Pyodide initialized","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-20T21:13:42.735Z] [INFO] {"timestamp":"2025-10-20T21:13:42.735Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:ready","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-20T21:13:42.735Z] [INFO] {"timestamp":"2025-10-20T21:13:42.735Z","level":"INFO","message":"[PyodideRuntime] Pyodide runtime ready","details":{}}
[2025-10-20T21:13:42.735Z] [INFO] {"timestamp":"2025-10-20T21:13:42.735Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:initialized","details":{"ready":true}}
[2025-10-20T21:13:42.736Z] [INFO] {"timestamp":"2025-10-20T21:13:42.736Z","level":"INFO","message":"Dashboard UI Manager (Event-Driven) taking control of DOM...","details":{}}
[2025-10-20T21:13:42.747Z] [INFO] {"timestamp":"2025-10-20T21:13:42.747Z","level":"INFO","message":"[UIManager] Initializing modular panel support...","details":{}}
[2025-10-20T21:13:42.747Z] [INFO] {"timestamp":"2025-10-20T21:13:42.747Z","level":"INFO","message":"[UIManager] Modular panel initialization complete","details":{}}
[2025-10-20T21:13:42.747Z] [INFO] {"timestamp":"2025-10-20T21:13:42.747Z","level":"INFO","message":"Dashboard UI Initialized. Listening for events.","details":{}}
[2025-10-20T21:13:42.747Z] [LOG] [CoreLogic] Attempting to initialize DiffViewerUI...
[2025-10-20T21:13:42.747Z] [LOG] [CoreLogic] Resolving DiffViewerUI from container...
[2025-10-20T21:13:42.748Z] [LOG] [CoreLogic] DiffViewerUI resolved: undefined
[2025-10-20T21:13:42.748Z] [LOG] [CoreLogic] DiffViewerUI.init exists? undefined
[2025-10-20T21:13:42.748Z] [WARN] [CoreLogic] DiffViewerUI or DiffViewerUI.init not available
[2025-10-20T21:13:42.748Z] [INFO] {"timestamp":"2025-10-20T21:13:42.748Z","level":"INFO","message":"[CoreLogic] Agent initialization complete. System is operational.","details":{}}
[2025-10-20T21:13:42.748Z] [INFO] {"timestamp":"2025-10-20T21:13:42.748Z","level":"INFO","message":"[CoreLogic] Creating genesis snapshot of initial boot state...","details":{}}
[2025-10-20T21:13:42.748Z] [WARN] {"timestamp":"2025-10-20T21:13:42.748Z","level":"WARN","message":"[CoreLogic] Failed to create genesis snapshot (non-fatal):","details":"[DIContainer] Service not found: GenesisSnapshot\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Storage, AuditLogger, RateLimiter, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, DiffViewerUI, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, MultiModelPaxos\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-20T21:13:42.748Z] [INFO] {"timestamp":"2025-10-20T21:13:42.748Z","level":"INFO","message":"[CoreLogic] Starting agent with initial goal:","details":"make cool graphics in 3d canvas"}
[2025-10-20T21:13:42.748Z] [INFO] {"timestamp":"2025-10-20T21:13:42.748Z","level":"INFO","message":"[UI] updateGoal called with:","details":{"text":"make cool graphics in 3d canvas","hasGoalTextRef":true}}
[2025-10-20T21:13:42.748Z] [INFO] {"timestamp":"2025-10-20T21:13:42.748Z","level":"INFO","message":"[UI] Goal text element updated successfully","details":{}}
[2025-10-20T21:13:42.762Z] [INFO] {"timestamp":"2025-10-20T21:13:42.762Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1760994822754_b2b794c8f1a2835b/session.json (SHA: 82ff8a1)","details":{}}
[2025-10-20T21:13:42.762Z] [INFO] {"timestamp":"2025-10-20T21:13:42.762Z","level":"INFO","message":"[SessionManager] Created new session: session_1760994822754_b2b794c8f1a2835b","details":{}}
[2025-10-20T21:13:42.769Z] [INFO] {"timestamp":"2025-10-20T21:13:42.769Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.checkpoints/checkpoint_1760994822762.json (SHA: 070ac1e)","details":{}}
[2025-10-20T21:13:42.769Z] [INFO] {"timestamp":"2025-10-20T21:13:42.769Z","level":"INFO","message":"[StateManager] Created checkpoint: checkpoint_1760994822762 - Session session_1760994822754_b2b794c8f1a2835b - Turn 0 start","details":{}}
[2025-10-20T21:13:42.769Z] [INFO] {"timestamp":"2025-10-20T21:13:42.769Z","level":"INFO","message":"[SessionManager] Created turn checkpoint: checkpoint_1760994822762","details":{}}
[2025-10-20T21:13:42.774Z] [INFO] {"timestamp":"2025-10-20T21:13:42.774Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1760994822754_b2b794c8f1a2835b/session.json (SHA: e4e2770)","details":{}}
[2025-10-20T21:13:42.774Z] [INFO] {"timestamp":"2025-10-20T21:13:42.774Z","level":"INFO","message":"[SessionManager] Created turn 0 for session session_1760994822754_b2b794c8f1a2835b","details":{}}
[2025-10-20T21:13:42.774Z] [INFO] {"timestamp":"2025-10-20T21:13:42.774Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"","progress":null}}
[2025-10-20T21:13:42.774Z] [INFO] {"timestamp":"2025-10-20T21:13:42.774Z","level":"INFO","message":"[SentinelFSM] State transition: IDLE -> CURATING_CONTEXT","details":{}}
[2025-10-20T21:13:42.774Z] [INFO] {"timestamp":"2025-10-20T21:13:42.774Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"IDLE","newState":"CURATING_CONTEXT","context":{"goal":"make cool graphics in 3d canvas","sessionId":"session_1760994822754_b2b794c8f1a2835b","turn":{"turn":0,"cats_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","dogs_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","status":"pending_context","checkpointId":"checkpoint_1760994822762","createdAt":"2025-10-20T21:13:42.769Z"},"startTime":1760994822774,"iterations":0,"maxIterations":10}}}
[2025-10-20T21:13:42.775Z] [INFO] {"timestamp":"2025-10-20T21:13:42.775Z","level":"INFO","message":"[SentinelFSM] Executing state: CURATING_CONTEXT","details":{}}
[2025-10-20T21:13:42.775Z] [INFO] {"timestamp":"2025-10-20T21:13:42.775Z","level":"INFO","message":"[EventBus] Emitting event: agent:curating","details":{"goal":"make cool graphics in 3d canvas"}}
[2025-10-20T21:13:42.775Z] [INFO] {"timestamp":"2025-10-20T21:13:42.775Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Analyzing project files...","progress":null}}
[2025-10-20T21:13:42.775Z] [WARN] {"timestamp":"2025-10-20T21:13:42.775Z","level":"WARN","message":"[SentinelTools] No files available for curation","details":{}}
[2025-10-20T21:13:42.775Z] [INFO] {"timestamp":"2025-10-20T21:13:42.775Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Found 0 relevant files","progress":50}}
[2025-10-20T21:13:42.780Z] [INFO] {"timestamp":"2025-10-20T21:13:42.780Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md (SHA: ffeb26e)","details":{}}
[2025-10-20T21:13:42.780Z] [INFO] {"timestamp":"2025-10-20T21:13:42.780Z","level":"INFO","message":"[AuditLogger] VFS_CREATE","details":{"path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","type":"markdown","size":116,"description":"Context bundle for turn: Context for goal: make cool graphics in 3d canvas"}}
[2025-10-20T21:13:42.783Z] [INFO] {"timestamp":"2025-10-20T21:13:42.783Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.audit/2025-10-20.jsonl (SHA: 315afde)","details":{}}
[2025-10-20T21:13:42.783Z] [INFO] {"timestamp":"2025-10-20T21:13:42.783Z","level":"INFO","message":"[StateManager-Git] Created artifact: /sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","details":{}}
[2025-10-20T21:13:42.784Z] [INFO] {"timestamp":"2025-10-20T21:13:42.784Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"AWAITING_CONTEXT_APPROVAL","detail":"","progress":null}}
[2025-10-20T21:13:42.784Z] [INFO] {"timestamp":"2025-10-20T21:13:42.784Z","level":"INFO","message":"[SentinelFSM] State transition: CURATING_CONTEXT -> AWAITING_CONTEXT_APPROVAL","details":{}}
[2025-10-20T21:13:42.784Z] [INFO] {"timestamp":"2025-10-20T21:13:42.784Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"CURATING_CONTEXT","newState":"AWAITING_CONTEXT_APPROVAL","context":{"goal":"make cool graphics in 3d canvas","sessionId":"session_1760994822754_b2b794c8f1a2835b","turn":{"turn":0,"cats_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","dogs_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","status":"pending_context","checkpointId":"checkpoint_1760994822762","createdAt":"2025-10-20T21:13:42.769Z"},"startTime":1760994822774,"iterations":0,"maxIterations":10,"catsPath":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md"}}}
[2025-10-20T21:13:42.784Z] [INFO] {"timestamp":"2025-10-20T21:13:42.784Z","level":"INFO","message":"[SentinelFSM] Executing state: AWAITING_CONTEXT_APPROVAL","details":{}}
[2025-10-20T21:13:42.784Z] [INFO] {"timestamp":"2025-10-20T21:13:42.784Z","level":"INFO","message":"[EventBus] Emitting event: agent:awaiting:context","details":{"cats_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","session_id":"session_1760994822754_b2b794c8f1a2835b"}}
[2025-10-20T21:13:42.784Z] [INFO] {"timestamp":"2025-10-20T21:13:42.784Z","level":"INFO","message":"[CoreLogic] Agent cycle started successfully","details":{}}
[2025-10-20T21:13:42.784Z] [LOG] [Boot] Agent system awakened successfully
[2025-10-20T21:13:47.270Z] [INFO] {"timestamp":"2025-10-20T21:13:47.270Z","level":"INFO","message":"[EventBus] Emitting event: user:approve:context","details":{}}
[2025-10-20T21:13:47.270Z] [INFO] {"timestamp":"2025-10-20T21:13:47.270Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"PLANNING_WITH_CONTEXT","detail":"","progress":null}}
[2025-10-20T21:13:47.270Z] [INFO] {"timestamp":"2025-10-20T21:13:47.270Z","level":"INFO","message":"[SentinelFSM] State transition: AWAITING_CONTEXT_APPROVAL -> PLANNING_WITH_CONTEXT","details":{}}
[2025-10-20T21:13:47.270Z] [INFO] {"timestamp":"2025-10-20T21:13:47.270Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"AWAITING_CONTEXT_APPROVAL","newState":"PLANNING_WITH_CONTEXT","context":{"goal":"make cool graphics in 3d canvas","sessionId":"session_1760994822754_b2b794c8f1a2835b","turn":{"turn":0,"cats_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","dogs_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","status":"pending_context","checkpointId":"checkpoint_1760994822762","createdAt":"2025-10-20T21:13:42.769Z"},"startTime":1760994822774,"iterations":0,"maxIterations":10,"catsPath":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md"}}}
[2025-10-20T21:13:47.271Z] [INFO] {"timestamp":"2025-10-20T21:13:47.271Z","level":"INFO","message":"[SentinelFSM] Executing state: PLANNING_WITH_CONTEXT","details":{}}
[2025-10-20T21:13:47.271Z] [INFO] {"timestamp":"2025-10-20T21:13:47.271Z","level":"INFO","message":"[EventBus] Emitting event: agent:planning","details":{}}
[2025-10-20T21:13:47.287Z] [INFO] {"timestamp":"2025-10-20T21:13:47.287Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"GENERATING_PROPOSAL","detail":"","progress":null}}
[2025-10-20T21:13:47.287Z] [INFO] {"timestamp":"2025-10-20T21:13:47.287Z","level":"INFO","message":"[SentinelFSM] State transition: PLANNING_WITH_CONTEXT -> GENERATING_PROPOSAL","details":{}}
[2025-10-20T21:13:47.287Z] [INFO] {"timestamp":"2025-10-20T21:13:47.287Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"PLANNING_WITH_CONTEXT","newState":"GENERATING_PROPOSAL","context":{"goal":"make cool graphics in 3d canvas","sessionId":"session_1760994822754_b2b794c8f1a2835b","turn":{"turn":0,"cats_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","dogs_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","status":"pending_context","checkpointId":"checkpoint_1760994822762","createdAt":"2025-10-20T21:13:42.769Z"},"startTime":1760994822774,"iterations":0,"maxIterations":10,"catsPath":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","planPrompt":"Your goal is to: make cool graphics in 3d canvas\n\nContext Bundle (existing files that may be relevant):\n## PAWS Context Bundle (cats.md)\n\n**Reason:** Context for goal: make cool graphics in 3d canvas\n\n**Files:** 0\n\n---\n\n\n\n\nInstructions:\n1. Analyze the context to understand what already exists\n2. Determine what files need to be CREATED, MODIFIED, or DELETED to achieve the goal\n3. Generate COMPLETE, working code for each file\n4. If creating something new (like 3D graphics), create ALL necessary files with full implementations\n5. Don't reference external libraries unless they're already in the context\n6. Make sure your code is production-ready and fully functional\n\nNow generate your complete implementation to achieve this goal."}}}
[2025-10-20T21:13:47.287Z] [INFO] {"timestamp":"2025-10-20T21:13:47.287Z","level":"INFO","message":"[SentinelFSM] Executing state: GENERATING_PROPOSAL","details":{}}
[2025-10-20T21:13:47.287Z] [INFO] {"timestamp":"2025-10-20T21:13:47.287Z","level":"INFO","message":"[EventBus] Emitting event: agent:generating","details":{}}
[2025-10-20T21:13:47.287Z] [INFO] {"timestamp":"2025-10-20T21:13:47.287Z","level":"INFO","message":"[HybridLLM] Calling Gemini API via ApiClient (proxy mode)","details":{}}
[2025-10-20T21:13:47.291Z] [INFO] {"timestamp":"2025-10-20T21:13:47.291Z","level":"INFO","message":"Proxy status: Available","details":{}}
[2025-10-20T21:14:13.962Z] [INFO] {"timestamp":"2025-10-20T21:14:13.962Z","level":"INFO","message":"[HybridLLM] Cloud completion generated","details":{"type":"text","length":12911}}
[2025-10-20T21:14:13.962Z] [INFO] {"timestamp":"2025-10-20T21:14:13.962Z","level":"INFO","message":"[SentinelFSM] Parsed 3 changes from proposal","details":{}}
[2025-10-20T21:14:13.973Z] [INFO] {"timestamp":"2025-10-20T21:14:13.973Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md (SHA: fabc61b)","details":{}}
[2025-10-20T21:14:13.973Z] [INFO] {"timestamp":"2025-10-20T21:14:13.973Z","level":"INFO","message":"[AuditLogger] VFS_CREATE","details":{"path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","type":"markdown","size":12871,"description":"Change proposal: Proposal for: make cool graphics in 3d canvas"}}
[2025-10-20T21:14:13.979Z] [INFO] {"timestamp":"2025-10-20T21:14:13.979Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.audit/2025-10-20.jsonl (SHA: ccc3c17)","details":{}}
[2025-10-20T21:14:13.979Z] [INFO] {"timestamp":"2025-10-20T21:14:13.979Z","level":"INFO","message":"[StateManager-Git] Created artifact: /sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","details":{}}
[2025-10-20T21:14:13.980Z] [INFO] {"timestamp":"2025-10-20T21:14:13.980Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"AWAITING_PROPOSAL_APPROVAL","detail":"","progress":null}}
[2025-10-20T21:14:13.980Z] [INFO] {"timestamp":"2025-10-20T21:14:13.980Z","level":"INFO","message":"[SentinelFSM] State transition: GENERATING_PROPOSAL -> AWAITING_PROPOSAL_APPROVAL","details":{}}
[2025-10-20T21:14:13.980Z] [INFO] {"timestamp":"2025-10-20T21:14:13.980Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"GENERATING_PROPOSAL","newState":"AWAITING_PROPOSAL_APPROVAL","context":{"goal":"make cool graphics in 3d canvas","sessionId":"session_1760994822754_b2b794c8f1a2835b","turn":{"turn":0,"cats_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","dogs_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","status":"pending_context","checkpointId":"checkpoint_1760994822762","createdAt":"2025-10-20T21:13:42.769Z"},"startTime":1760994822774,"iterations":0,"maxIterations":10,"catsPath":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","planPrompt":"Your goal is to: make cool graphics in 3d canvas\n\nContext Bundle (existing files that may be relevant):\n## PAWS Context Bundle (cats.md)\n\n**Reason:** Context for goal: make cool graphics in 3d canvas\n\n**Files:** 0\n\n---\n\n\n\n\nInstructions:\n1. Analyze the context to understand what already exists\n2. Determine what files need to be CREATED, MODIFIED, or DELETED to achieve the goal\n3. Generate COMPLETE, working code for each file\n4. If creating something new (like 3D graphics), create ALL necessary files with full implementations\n5. Don't reference external libraries unless they're already in the context\n6. Make sure your code is production-ready and fully functional\n\nNow generate your complete implementation to achieve this goal.","dogsPath":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","proposedChanges":[{"operation":"CREATE","file_path":"/vfs/index.html","new_content":"<!DOCTYPE html>\n<html lang=\"en\">\n<head>\n    <meta charset=\"UTF-8\">\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n    <title>3D Rotating Cube</title>\n    <link rel=\"stylesheet\" href=\"/vfs/style.css\">\n</head>\n<body>\n    <canvas id=\"glCanvas\"></canvas>\n    <script type=\"module\" src=\"/vfs/main.js\"></script>\n</body>\n</html>"},{"operation":"CREATE","file_path":"/vfs/style.css","new_content":"html, body {\n    margin: 0;\n    overflow: hidden; /* Hide scrollbars */\n    background-color: #000; /* Black background */\n}\n\ncanvas {\n    display: block; /* Remove extra space below canvas */\n    width: 100vw;\n    height: 100vh;\n    background-color: #1a1a1a; /* Dark gray for canvas background */\n}"},{"operation":"CREATE","file_path":"/vfs/main.js","new_content":"import { mat4 } from 'https://cdn.jsdelivr.net/npm/gl-matrix@3.4.3/es6/index.js';\n\nlet cubeRotation = 0.0;\nlet deltaTime = 0;\nlet lastRenderTime = 0;\n\nfunction main() {\n    const canvas = document.querySelector('#glCanvas');\n    const gl = canvas.getContext('webgl');\n\n    if (!gl) {\n        alert('Unable to initialize WebGL. Your browser or machine may not support it.');\n        return;\n    }\n\n    // Vertex shader program\n    const vsSource = `\n        attribute vec4 aVertexPosition;\n        attribute vec4 aVertexColor;\n\n        uniform mat4 uModelViewMatrix;\n        uniform mat4 uProjectionMatrix;\n\n        varying lowp vec4 vColor;\n\n        void main(void) {\n            gl_Position = uProjectionMatrix * uModelViewMatrix * aVertexPosition;\n            vColor = aVertexColor;\n        }\n    `;\n\n    // Fragment shader program\n    const fsSource = `\n        varying lowp vec4 vColor;\n\n        void main(void) {\n            gl_FragColor = vColor;\n        }\n    `;\n\n    const shaderProgram = initShaderProgram(gl, vsSource, fsSource);\n\n    const programInfo = {\n        program: shaderProgram,\n        attribLocations: {\n            vertexPosition: gl.getAttribLocation(shaderProgram, 'aVertexPosition'),\n            vertexColor: gl.getAttribLocation(shaderProgram, 'aVertexColor'),\n        },\n        uniformLocations: {\n            projectionMatrix: gl.getUniformLocation(shaderProgram, 'uProjectionMatrix'),\n            modelViewMatrix: gl.getUniformLocation(shaderProgram, 'uModelViewMatrix'),\n        },\n    };\n\n    const buffers = initBuffers(gl);\n\n    function render(now) {\n        now *= 0.001; // convert to seconds\n        deltaTime = now - lastRenderTime;\n        lastRenderTime = now;\n\n        drawScene(gl, programInfo, buffers, deltaTime);\n\n        requestAnimationFrame(render);\n    }\n    requestAnimationFrame(render);\n}\n\n// Initialize a shader program, so WebGL knows how to draw our data\nfunction initShaderProgram(gl, vsSource, fsSource) {\n    const vertexShader = loadShader(gl, gl.VERTEX_SHADER, vsSource);\n    const fragmentShader = loadShader(gl, gl.FRAGMENT_SHADER, fsSource);\n\n    // Create the shader program\n    const shaderProgram = gl.createProgram();\n    gl.attachShader(shaderProgram, vertexShader);\n    gl.attachShader(shaderProgram, fragmentShader);\n    gl.linkProgram(shaderProgram);\n\n    // If creating the shader program failed, alert\n    if (!gl.getProgramParameter(shaderProgram, gl.LINK_STATUS)) {\n        alert('Unable to initialize the shader program: ' + gl.getProgramInfoLog(shaderProgram));\n        return null;\n    }\n\n    return shaderProgram;\n}\n\n// Creates a shader of the given type, uploads the source and compiles it.\nfunction loadShader(gl, type, source) {\n    const shader = gl.createShader(type);\n\n    // Send the source to the shader object\n    gl.shaderSource(shader, source);\n\n    // Compile the shader program\n    gl.compileShader(shader);\n\n    // See if it compiled successfully\n    if (!gl.getShaderParameter(shader, gl.COMPILE_STATUS)) {\n        alert('An error occurred compiling the shaders: ' + gl.getShaderInfoLog(shader));\n        gl.deleteShader(shader);\n        return null;\n    }\n\n    return shader;\n}\n\nfunction initBuffers(gl) {\n    // Create a buffer for the cube's vertex positions.\n    const positionBuffer = gl.createBuffer();\n    gl.bindBuffer(gl.ARRAY_BUFFER, positionBuffer);\n\n    // Now create an array of positions for the cube.\n    const positions = [\n        // Front face\n        -1.0, -1.0,  1.0,\n         1.0, -1.0,  1.0,\n         1.0,  1.0,  1.0,\n        -1.0,  1.0,  1.0,\n\n        // Back face\n        -1.0, -1.0, -1.0,\n        -1.0,  1.0, -1.0,\n         1.0,  1.0, -1.0,\n         1.0, -1.0, -1.0,\n\n        // Top face\n        -1.0,  1.0, -1.0,\n        -1.0,  1.0,  1.0,\n         1.0,  1.0,  1.0,\n         1.0,  1.0, -1.0,\n\n        // Bottom face\n        -1.0, -1.0, -1.0,\n         1.0, -1.0, -1.0,\n         1.0, -1.0,  1.0,\n        -1.0, -1.0,  1.0,\n\n        // Right face\n         1.0, -1.0, -1.0,\n         1.0,  1.0, -1.0,\n         1.0,  1.0,  1.0,\n         1.0, -1.0,  1.0,\n\n        // Left face\n        -1.0, -1.0, -1.0,\n        -1.0, -1.0,  1.0,\n        -1.0,  1.0,  1.0,\n        -1.0,  1.0, -1.0,\n    ];\n\n    gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(positions), gl.STATIC_DRAW);\n\n    // Now set up the colors for the faces. We'll use a different color for each face.\n    const faceColors = [\n        [1.0,  1.0,  1.0,  1.0],    // Front face: white\n        [1.0,  0.0,  0.0,  1.0],    // Back face: red\n        [0.0,  1.0,  0.0,  1.0],    // Top face: green\n        [0.0,  0.0,  1.0,  1.0],    // Bottom face: blue\n        [1.0,  1.0,  0.0,  1.0],    // Right face: yellow\n        [1.0,  0.0,  1.0,  1.0],    // Left face: purple\n    ];\n\n    // Convert the array of colors into a table for all the vertices.\n    let colors = [];\n    for (var j = 0; j < 6; j++) {\n        const c = faceColors[j];\n        // Repeat each color four times for the four vertices of the face\n        colors = colors.concat(c, c, c, c);\n    }\n\n    const colorBuffer = gl.createBuffer();\n    gl.bindBuffer(gl.ARRAY_BUFFER, colorBuffer);\n    gl.bufferData(gl.ARRAY_BUFFER, new Float32Array(colors), gl.STATIC_DRAW);\n\n    // Build the element array buffer; this specifies the indices in the vertex arrays to use to draw the cube.\n    const indexBuffer = gl.createBuffer();\n    gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, indexBuffer);\n\n    // This array defines each face as two triangles, using the\n    // indices into the vertex array to specify the vertices for each triangle.\n    const indices = [\n        0,  1,  2,      0,  2,  3,    // front\n        4,  5,  6,      4,  6,  7,    // back\n        8,  9,  10,     8,  10, 11,   // top\n        12, 13, 14,     12, 14, 15,   // bottom\n        16, 17, 18,     16, 18, 19,   // right\n        20, 21, 22,     20, 22, 23,   // left\n    ];\n\n    gl.bufferData(gl.ELEMENT_ARRAY_BUFFER, new Uint16Array(indices), gl.STATIC_DRAW);\n\n    return {\n        position: positionBuffer,\n        color: colorBuffer,\n        indices: indexBuffer,\n        vertexCount: indices.length,\n    };\n}\n\nfunction drawScene(gl, programInfo, buffers, deltaTime) {\n    gl.clearColor(0.0, 0.0, 0.0, 1.0);  // Clear to black, fully opaque\n    gl.clearDepth(1.0);                 // Clear everything\n    gl.enable(gl.DEPTH_TEST);           // Enable depth testing\n    gl.depthFunc(gl.LEQUAL);            // Near things obscure far things\n\n    // Clear the canvas before we start drawing on it.\n    gl.clear(gl.COLOR_BUFFER_BIT | gl.DEPTH_BUFFER_BIT);\n\n    // Create a perspective matrix, a special matrix that is\n    // used to simulate the distortion of perspective in a camera.\n    // Our field of view is 45 degrees, with a width/height\n    // ratio that matches the display size of the canvas\n    // and we only want to see objects between 0.1 and 100 units\n    // away from the camera.\n\n    const fieldOfView = 45 * Math.PI / 180;   // in radians\n    const aspect = gl.canvas.clientWidth / gl.canvas.clientHeight;\n    const zNear = 0.1;\n    const zFar = 100.0;\n    const projectionMatrix = mat4.create();\n\n    // note: glmatrix.js always has the first argument\n    // as the destination to receive the result.\n    mat4.perspective(projectionMatrix,\n                     fieldOfView,\n                     aspect,\n                     zNear,\n                     zFar);\n\n    // Set the drawing position to the \"identity\" point, which is\n    // the center of the scene.\n    const modelViewMatrix = mat4.create();\n\n    // Now move the drawing position a bit to where we want to start\n    // the cube.\n    mat4.translate(modelViewMatrix,     // destination matrix\n                   modelViewMatrix,     // matrix to translate\n                   [-0.0, 0.0, -6.0]);  // amount to translate\n    mat4.rotate(modelViewMatrix,     // destination matrix\n                modelViewMatrix,     // matrix to rotate\n                cubeRotation,        // amount to rotate in radians\n                [0, 0, 1]);          // axis to rotate around (Z)\n    mat4.rotate(modelViewMatrix,     // destination matrix\n                modelViewMatrix,     // matrix to rotate\n                cubeRotation * .7,   // amount to rotate in radians\n                [0, 1, 0]);          // axis to rotate around (Y)\n\n    // Tell WebGL how to pull out the positions from the position\n    // buffer into the vertexPosition attribute.\n    {\n        const numComponents = 3;  // pull out 3 values per iteration\n        const type = gl.FLOAT;    // the data in the buffer is 32bit floats\n        const normalize = false;  // don't normalize\n        const stride = 0;         // how many bytes to get from one set of values to the next\n                                  // 0 = use type and numComponents above\n        const offset = 0;         // how many bytes inside the buffer to start from\n        gl.bindBuffer(gl.ARRAY_BUFFER, buffers.position);\n        gl.vertexAttribPointer(\n            programInfo.attribLocations.vertexPosition,\n            numComponents,\n            type,\n            normalize,\n            stride,\n            offset);\n        gl.enableVertexAttribArray(\n            programInfo.attribLocations.vertexPosition);\n    }\n\n    // Tell WebGL how to pull out the colors from the color buffer\n    // into the vertexColor attribute.\n    {\n        const numComponents = 4;\n        const type = gl.FLOAT;\n        const normalize = false;\n        const stride = 0;\n        const offset = 0;\n        gl.bindBuffer(gl.ARRAY_BUFFER, buffers.color);\n        gl.vertexAttribPointer(\n            programInfo.attribLocations.vertexColor,\n            numComponents,\n            type,\n            normalize,\n            stride,\n            offset);\n        gl.enableVertexAttribArray(\n            programInfo.attribLocations.vertexColor);\n    }\n\n    // Tell WebGL which indices to use to index the vertices\n    gl.bindBuffer(gl.ELEMENT_ARRAY_BUFFER, buffers.indices);\n\n    // Tell WebGL to use our program when drawing\n    gl.useProgram(programInfo.program);\n\n    // Set the shader uniforms\n    gl.uniformMatrix4fv(\n        programInfo.uniformLocations.projectionMatrix,\n        false,\n        projectionMatrix);\n    gl.uniformMatrix4fv(\n        programInfo.uniformLocations.modelViewMatrix,\n        false,\n        modelViewMatrix);\n\n    {\n        const vertexCount = buffers.vertexCount;\n        const type = gl.UNSIGNED_SHORT;\n        const offset = 0;\n        gl.drawElements(gl.TRIANGLES, vertexCount, type, offset);\n    }\n\n    // Update the rotation for the next draw\n    cubeRotation += deltaTime * 0.8;\n}\n\n// Ensure the canvas resizes with the window\nfunction resizeCanvasToDisplaySize(canvas) {\n    // Lookup the size the browser is displaying the canvas.\n    const displayWidth  = canvas.clientWidth;\n    const displayHeight = canvas.clientHeight;\n\n    // Check if the canvas is not the same size.\n    const needResize = canvas.width  !== displayWidth ||\n                       canvas.height !== displayHeight;\n\n    if (needResize) {\n        // Make the canvas the same size\n        canvas.width  = displayWidth;\n        canvas.height = displayHeight;\n    }\n\n    return needResize;\n}\n\n// Call main when the document is ready\nwindow.onload = () => {\n    main();\n    const canvas = document.querySelector('#glCanvas');\n    // Initial resize and set WebGL viewport\n    if (resizeCanvasToDisplaySize(canvas)) {\n        const gl = canvas.getContext('webgl');\n        if (gl) {\n            gl.viewport(0, 0, gl.canvas.width, gl.canvas.height);\n        }\n    }\n    // Listen for window resize events\n    window.addEventListener('resize', () => {\n        if (resizeCanvasToDisplaySize(canvas)) {\n            const gl = canvas.getContext('webgl');\n            if (gl) {\n                gl.viewport(0, 0, gl.canvas.width, gl.canvas.height);\n            }\n        }\n    });\n};"}]}}}
[2025-10-20T21:14:13.980Z] [INFO] {"timestamp":"2025-10-20T21:14:13.980Z","level":"INFO","message":"[EventBus] Emitting event: diff:show","details":{"dogs_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","session_id":"session_1760994822754_b2b794c8f1a2835b","turn":{"turn":0,"cats_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","dogs_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","status":"pending_context","checkpointId":"checkpoint_1760994822762","createdAt":"2025-10-20T21:13:42.769Z"}}}
[2025-10-20T21:14:13.980Z] [INFO] {"timestamp":"2025-10-20T21:14:13.980Z","level":"INFO","message":"[SentinelFSM] Executing state: AWAITING_PROPOSAL_APPROVAL","details":{}}
[2025-10-20T21:14:13.980Z] [INFO] {"timestamp":"2025-10-20T21:14:13.980Z","level":"INFO","message":"[EventBus] Emitting event: diff:show","details":{"dogs_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","session_id":"session_1760994822754_b2b794c8f1a2835b","turn":{"turn":0,"cats_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.cats.md","dogs_path":"/sessions/session_1760994822754_b2b794c8f1a2835b/turn-0.dogs.md","status":"pending_context","checkpointId":"checkpoint_1760994822762","createdAt":"2025-10-20T21:13:42.769Z"}}}
[2025-10-25T21:01:37.671Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:01:37.671Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:01:37.682Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:01:37.682Z] [LOG] [API] Checking server status...
[2025-10-25T21:01:37.691Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:01:37.686Z"
}
[2025-10-25T21:01:37.692Z] [LOG] [API] WebGPU available
[2025-10-25T21:01:37.699Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:01:37.699Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:01:37.699Z] [LOG] [ModelConfig] Initializing single-screen model picker...
[2025-10-25T21:01:48.679Z] [ERROR] [LiveReload] Server appears to be down, stopping checks
[2025-10-25T21:03:13.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:03:13.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:03:13.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:03:13.370Z] [LOG] [API] Checking server status...
[2025-10-25T21:03:13.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:03:13.373Z"
}
[2025-10-25T21:03:13.378Z] [LOG] [API] WebGPU available
[2025-10-25T21:03:13.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:03:13.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:03:13.383Z] [LOG] [ModelConfig] Initializing single-screen model picker...
[2025-10-25T21:03:24.367Z] [ERROR] [LiveReload] Server appears to be down, stopping checks
[2025-10-25T21:05:54.514Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:05:54.514Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:05:54.536Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:05:54.536Z] [LOG] [API] Checking server status...
[2025-10-25T21:05:54.556Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:05:54.541Z"
}
[2025-10-25T21:05:54.556Z] [LOG] [API] WebGPU available
[2025-10-25T21:05:54.560Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:05:54.560Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:05:54.560Z] [LOG] [ModelConfig] Initializing single-screen model picker...
[2025-10-25T21:06:00.979Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:06:00.979Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:06:01.024Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:06:01.024Z] [LOG] [API] Checking server status...
[2025-10-25T21:06:01.031Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:06:01.026Z"
}
[2025-10-25T21:06:01.031Z] [LOG] [API] WebGPU available
[2025-10-25T21:06:01.040Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:06:01.040Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:06:01.040Z] [LOG] [ModelConfig] Initializing single-screen model picker...
[2025-10-25T21:06:11.984Z] [ERROR] [LiveReload] Server appears to be down, stopping checks
[2025-10-25T21:07:07.656Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:07:07.656Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:07:07.660Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:07:07.660Z] [LOG] [API] Checking server status...
[2025-10-25T21:07:07.668Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:07:07.663Z"
}
[2025-10-25T21:07:07.668Z] [LOG] [API] WebGPU available
[2025-10-25T21:07:07.674Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:07:07.674Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:07:07.674Z] [LOG] [ModelConfig] Initializing single-screen model picker...
[2025-10-25T21:08:09.008Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:08:09.009Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:08:09.016Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:08:09.016Z] [LOG] [API] Checking server status...
[2025-10-25T21:08:09.022Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:08:09.017Z"
}
[2025-10-25T21:08:09.023Z] [LOG] [API] WebGPU available
[2025-10-25T21:08:09.027Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:08:09.027Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:08:09.027Z] [LOG] [ModelConfig] Initializing single-screen model picker...
[2025-10-25T21:15:09.010Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:15:09.010Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:15:09.016Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:15:09.016Z] [LOG] [API] Checking server status...
[2025-10-25T21:15:09.020Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:15:09.016Z"
}
[2025-10-25T21:15:09.021Z] [LOG] [API] WebGPU available
[2025-10-25T21:15:09.024Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:15:09.024Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:15:09.024Z] [LOG] [ModelConfig] Initializing single-screen model picker...
[2025-10-25T21:23:32.124Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:23:32.124Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:23:32.130Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:23:32.130Z] [LOG] [API] Checking server status...
[2025-10-25T21:23:32.134Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:23:32.131Z"
}
[2025-10-25T21:23:32.135Z] [LOG] [API] WebGPU available
[2025-10-25T21:23:32.138Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:23:32.138Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:23:32.138Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:23:34.317Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:23:34.317Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:23:34.334Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:23:34.334Z] [LOG] [API] Checking server status...
[2025-10-25T21:23:34.339Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:23:34.335Z"
}
[2025-10-25T21:23:34.339Z] [LOG] [API] WebGPU available
[2025-10-25T21:23:34.343Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:23:34.343Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:23:34.343Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:23:41.872Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:23:41.873Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:23:41.907Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:23:41.908Z] [LOG] [API] Checking server status...
[2025-10-25T21:23:41.912Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:23:41.908Z"
}
[2025-10-25T21:23:41.912Z] [LOG] [API] WebGPU available
[2025-10-25T21:23:41.916Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:23:41.917Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:23:41.917Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:23:46.118Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:23:46.118Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:23:46.136Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:23:46.136Z] [LOG] [API] Checking server status...
[2025-10-25T21:23:46.142Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:23:46.137Z"
}
[2025-10-25T21:23:46.142Z] [LOG] [API] WebGPU available
[2025-10-25T21:23:46.148Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:23:46.148Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:23:46.148Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:24:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:24:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:24:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:24:50.398Z] [LOG] [API] Checking server status...
[2025-10-25T21:24:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:24:50.398Z"
}
[2025-10-25T21:24:50.406Z] [LOG] [API] WebGPU available
[2025-10-25T21:24:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:24:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:24:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:25:50.326Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:25:50.326Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:25:50.335Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:25:50.335Z] [LOG] [API] Checking server status...
[2025-10-25T21:25:50.341Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:25:50.338Z"
}
[2025-10-25T21:25:50.341Z] [LOG] [API] WebGPU available
[2025-10-25T21:25:50.345Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:25:50.345Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:25:50.345Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:26:42.265Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:26:42.265Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:26:42.278Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:26:42.278Z] [LOG] [API] Checking server status...
[2025-10-25T21:26:42.282Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:26:42.280Z"
}
[2025-10-25T21:26:42.283Z] [LOG] [API] WebGPU available
[2025-10-25T21:26:42.285Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:26:42.285Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:26:42.285Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:26:50.324Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:26:50.325Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:26:50.338Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:26:50.338Z] [LOG] [API] Checking server status...
[2025-10-25T21:26:50.349Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:26:50.340Z"
}
[2025-10-25T21:26:50.349Z] [LOG] [API] WebGPU available
[2025-10-25T21:26:50.351Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:26:50.351Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:26:50.351Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:27:50.349Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:27:50.349Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:27:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:27:50.361Z] [LOG] [API] Checking server status...
[2025-10-25T21:27:50.370Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:27:50.362Z"
}
[2025-10-25T21:27:50.370Z] [LOG] [API] WebGPU available
[2025-10-25T21:27:50.376Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:27:50.376Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:27:50.376Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:28:00.679Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:28:00.679Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:28:00.697Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:28:00.697Z] [LOG] [API] Checking server status...
[2025-10-25T21:28:00.701Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:28:00.698Z"
}
[2025-10-25T21:28:00.701Z] [LOG] [API] WebGPU available
[2025-10-25T21:28:00.703Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:28:00.703Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:28:00.704Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:28:01.640Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:28:01.640Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:28:01.666Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:28:01.666Z] [LOG] [API] Checking server status...
[2025-10-25T21:28:01.671Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:28:01.667Z"
}
[2025-10-25T21:28:01.671Z] [LOG] [API] WebGPU available
[2025-10-25T21:28:01.676Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:28:01.676Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:28:01.676Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:28:07.559Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:28:07.559Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:28:07.566Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:28:07.566Z] [LOG] [API] Checking server status...
[2025-10-25T21:28:07.573Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:28:07.568Z"
}
[2025-10-25T21:28:07.573Z] [LOG] [API] WebGPU available
[2025-10-25T21:28:07.580Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:28:07.580Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:28:07.580Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:28:10.060Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:28:10.060Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:28:10.066Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:28:10.067Z] [LOG] [API] Checking server status...
[2025-10-25T21:28:10.072Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:28:10.068Z"
}
[2025-10-25T21:28:10.073Z] [LOG] [API] WebGPU available
[2025-10-25T21:28:10.081Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:28:10.081Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:28:10.081Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:28:24.457Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:28:24.457Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:28:24.464Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:28:24.464Z] [LOG] [API] Checking server status...
[2025-10-25T21:28:24.469Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:28:24.465Z"
}
[2025-10-25T21:28:24.469Z] [LOG] [API] WebGPU available
[2025-10-25T21:28:24.471Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:28:24.471Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:28:24.471Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:28:26.249Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:28:26.249Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:28:26.253Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:28:26.253Z] [LOG] [API] Checking server status...
[2025-10-25T21:28:26.259Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:28:26.255Z"
}
[2025-10-25T21:28:26.259Z] [LOG] [API] WebGPU available
[2025-10-25T21:28:26.262Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:28:26.262Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:28:26.262Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:28:30.813Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:28:30.814Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:28:30.843Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:28:30.843Z] [LOG] [API] Checking server status...
[2025-10-25T21:28:30.849Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:28:30.845Z"
}
[2025-10-25T21:28:30.849Z] [LOG] [API] WebGPU available
[2025-10-25T21:28:30.853Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:28:30.853Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:28:30.853Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:28:50.320Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:28:50.320Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:28:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:28:50.364Z] [LOG] [API] Checking server status...
[2025-10-25T21:28:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:28:50.366Z"
}
[2025-10-25T21:28:50.378Z] [LOG] [API] WebGPU available
[2025-10-25T21:28:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:28:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:28:50.384Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:29:23.177Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:29:23.177Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:29:23.181Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:29:23.181Z] [LOG] [API] Checking server status...
[2025-10-25T21:29:23.186Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:29:23.181Z"
}
[2025-10-25T21:29:23.186Z] [LOG] [API] WebGPU available
[2025-10-25T21:29:23.190Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:29:23.190Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:29:23.190Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:29:50.340Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:29:50.341Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:29:50.349Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:29:50.349Z] [LOG] [API] Checking server status...
[2025-10-25T21:29:50.362Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:29:50.350Z"
}
[2025-10-25T21:29:50.363Z] [LOG] [API] WebGPU available
[2025-10-25T21:29:50.367Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:29:50.367Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:29:50.367Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:30:50.325Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:30:50.326Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:30:50.348Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:30:50.348Z] [LOG] [API] Checking server status...
[2025-10-25T21:30:50.358Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:30:50.349Z"
}
[2025-10-25T21:30:50.358Z] [LOG] [API] WebGPU available
[2025-10-25T21:30:50.362Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:30:50.362Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:30:50.362Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:31:50.305Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:31:50.306Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:31:50.314Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:31:50.314Z] [LOG] [API] Checking server status...
[2025-10-25T21:31:50.325Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:31:50.316Z"
}
[2025-10-25T21:31:50.325Z] [LOG] [API] WebGPU available
[2025-10-25T21:31:50.328Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:31:50.328Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:31:50.328Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:32:50.315Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:32:50.315Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:32:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:32:50.397Z] [LOG] [API] Checking server status...
[2025-10-25T21:32:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:32:50.399Z"
}
[2025-10-25T21:32:50.407Z] [LOG] [API] WebGPU available
[2025-10-25T21:32:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:32:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:32:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:33:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:33:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:33:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:33:50.400Z] [LOG] [API] Checking server status...
[2025-10-25T21:33:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:33:50.403Z"
}
[2025-10-25T21:33:50.409Z] [LOG] [API] WebGPU available
[2025-10-25T21:33:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:33:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:33:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:34:42.010Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:34:42.010Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:34:42.085Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:34:42.085Z] [LOG] [API] Checking server status...
[2025-10-25T21:34:42.092Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:34:42.087Z"
}
[2025-10-25T21:34:42.092Z] [LOG] [API] WebGPU available
[2025-10-25T21:34:42.095Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:34:42.095Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:34:42.095Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:34:45.133Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:34:45.133Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:34:45.156Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:34:45.156Z] [LOG] [API] Checking server status...
[2025-10-25T21:34:45.160Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:34:45.157Z"
}
[2025-10-25T21:34:45.160Z] [LOG] [API] WebGPU available
[2025-10-25T21:34:45.163Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:34:45.163Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:34:45.163Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:34:49.330Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:34:49.330Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:34:49.347Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "hybrid"
}
[2025-10-25T21:34:49.347Z] [LOG] [API] Checking server status...
[2025-10-25T21:34:49.362Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:34:49.349Z"
}
[2025-10-25T21:34:49.362Z] [LOG] [API] WebGPU available
[2025-10-25T21:34:49.370Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:34:49.370Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:34:49.370Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:35:01.107Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:35:01.107Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:35:01.115Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:35:01.115Z] [LOG] [API] Checking server status...
[2025-10-25T21:35:01.119Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:35:01.116Z"
}
[2025-10-25T21:35:01.120Z] [LOG] [API] WebGPU available
[2025-10-25T21:35:01.123Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:35:01.123Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:35:01.123Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:37:58.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:37:58.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:37:58.428Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:37:58.428Z] [LOG] [API] Checking server status...
[2025-10-25T21:37:58.434Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:37:58.429Z"
}
[2025-10-25T21:37:58.434Z] [LOG] [API] WebGPU available
[2025-10-25T21:37:58.440Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:37:58.440Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:37:58.440Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:38:02.818Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:38:02.818Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:38:02.856Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:38:02.856Z] [LOG] [API] Checking server status...
[2025-10-25T21:38:02.862Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:38:02.858Z"
}
[2025-10-25T21:38:02.862Z] [LOG] [API] WebGPU available
[2025-10-25T21:38:02.869Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:38:02.869Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:38:02.869Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:38:05.111Z] [LOG] [Boot] Boot mode selected: meta
[2025-10-25T21:38:05.528Z] [LOG] [Boot] Boot mode selected: essential
[2025-10-25T21:38:06.200Z] [LOG] [Boot] Boot mode selected: meta
[2025-10-25T21:39:24.105Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:39:24.105Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:39:24.144Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:39:24.144Z] [LOG] [API] Checking server status...
[2025-10-25T21:39:24.153Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:39:24.146Z"
}
[2025-10-25T21:39:24.153Z] [LOG] [API] WebGPU available
[2025-10-25T21:39:24.157Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:39:24.157Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:39:24.157Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:42:42.204Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:42:42.205Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:42:42.215Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:42:42.216Z] [LOG] [API] Checking server status...
[2025-10-25T21:42:42.224Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:42:42.217Z"
}
[2025-10-25T21:42:42.224Z] [LOG] [API] WebGPU available
[2025-10-25T21:42:42.228Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:42:42.228Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:42:42.228Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:43:46.160Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:43:46.160Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:43:46.217Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:43:46.217Z] [LOG] [API] Checking server status...
[2025-10-25T21:43:46.227Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:43:46.219Z"
}
[2025-10-25T21:43:46.228Z] [LOG] [API] WebGPU available
[2025-10-25T21:43:46.238Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:43:46.238Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:43:46.238Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:44:29.128Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:44:29.128Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:44:29.179Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:44:29.179Z] [LOG] [API] Checking server status...
[2025-10-25T21:44:29.191Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:44:29.187Z"
}
[2025-10-25T21:44:29.191Z] [LOG] [API] WebGPU available
[2025-10-25T21:44:29.196Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:44:29.196Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:44:29.196Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:47:52.439Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:47:52.439Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:47:52.443Z] [LOG] [Config] Loaded settings: {
  "selectedModel": null,
  "aiProvider": null,
  "selectedMode": "cloud"
}
[2025-10-25T21:47:52.443Z] [LOG] [API] Checking server status...
[2025-10-25T21:47:52.449Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:47:52.444Z"
}
[2025-10-25T21:47:52.449Z] [LOG] [API] WebGPU available
[2025-10-25T21:47:52.453Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:47:52.453Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:47:52.453Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:48:04.577Z] [LOG] [ModelConfig] Configuration saved
[2025-10-25T21:48:04.581Z] [LOG] [ModelConfig] Model saved: {
  "id": "gpt-oss:120b",
  "name": "gpt-oss:120b",
  "provider": "ollama",
  "hostType": "ollama-proxy",
  "queryMethod": "proxy",
  "keySource": "none",
  "keyId": null
}
[2025-10-25T21:48:08.017Z] [LOG] [Boot] Boot mode selected: meta
[2025-10-25T21:48:09.972Z] [LOG] [Boot] awakenAgent() called
[2025-10-25T21:48:09.972Z] [LOG] [Boot] Goal from input: 
[2025-10-25T21:48:09.972Z] [WARN] [Boot] No goal specified
[2025-10-25T21:48:17.880Z] [INFO] {"timestamp":"2025-10-25T21:48:17.880Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-25T21:48:17.882Z] [INFO] {"timestamp":"2025-10-25T21:48:17.882Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-25T21:48:17.882Z] [INFO] {"timestamp":"2025-10-25T21:48:17.882Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-25T21:48:17.883Z] [INFO] {"timestamp":"2025-10-25T21:48:17.883Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-25T21:48:17.883Z] [INFO] {"timestamp":"2025-10-25T21:48:17.883Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-25T21:48:17.885Z] [INFO] {"timestamp":"2025-10-25T21:48:17.885Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-25T21:48:17.885Z] [INFO] {"timestamp":"2025-10-25T21:48:17.885Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-25T21:48:17.887Z] [INFO] {"timestamp":"2025-10-25T21:48:17.887Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-25T21:48:17.887Z] [INFO] {"timestamp":"2025-10-25T21:48:17.887Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-25T21:48:17.888Z] [INFO] {"timestamp":"2025-10-25T21:48:17.888Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-25T21:48:17.888Z] [INFO] {"timestamp":"2025-10-25T21:48:17.888Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-25T21:48:17.890Z] [INFO] {"timestamp":"2025-10-25T21:48:17.890Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-25T21:48:17.890Z] [INFO] {"timestamp":"2025-10-25T21:48:17.890Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-25T21:48:17.891Z] [INFO] {"timestamp":"2025-10-25T21:48:17.891Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-25T21:48:17.891Z] [INFO] {"timestamp":"2025-10-25T21:48:17.891Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-25T21:48:17.892Z] [INFO] {"timestamp":"2025-10-25T21:48:17.892Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-25T21:48:17.892Z] [INFO] {"timestamp":"2025-10-25T21:48:17.892Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-25T21:48:17.893Z] [INFO] {"timestamp":"2025-10-25T21:48:17.893Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-25T21:48:17.893Z] [INFO] {"timestamp":"2025-10-25T21:48:17.893Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-25T21:48:17.895Z] [INFO] {"timestamp":"2025-10-25T21:48:17.895Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-25T21:48:17.895Z] [INFO] {"timestamp":"2025-10-25T21:48:17.895Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-25T21:48:17.896Z] [INFO] {"timestamp":"2025-10-25T21:48:17.896Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-25T21:48:17.896Z] [INFO] {"timestamp":"2025-10-25T21:48:17.896Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-25T21:48:17.897Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-25T21:48:17.897Z] [ERROR] {"timestamp":"2025-10-25T21:48:17.897Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T21:48:17.897Z] [WARN] {"timestamp":"2025-10-25T21:48:17.897Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-25T21:48:17.898Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-25T21:48:17.898Z] [ERROR] {"timestamp":"2025-10-25T21:48:17.898Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T21:48:17.898Z] [WARN] {"timestamp":"2025-10-25T21:48:17.898Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-25T21:48:17.898Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-25T21:48:17.898Z] [ERROR] {"timestamp":"2025-10-25T21:48:17.898Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T21:48:17.898Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T21:48:17.898Z] [WARN] {"timestamp":"2025-10-25T21:48:17.898Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-25T21:48:17.900Z] [INFO] {"timestamp":"2025-10-25T21:48:17.900Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-25T21:48:17.900Z] [INFO] {"timestamp":"2025-10-25T21:48:17.900Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-25T21:48:17.901Z] [INFO] {"timestamp":"2025-10-25T21:48:17.901Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-25T21:48:17.901Z] [INFO] {"timestamp":"2025-10-25T21:48:17.901Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-25T21:48:17.904Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-25T21:48:17.904Z] [INFO] {"timestamp":"2025-10-25T21:48:17.904Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-25T21:48:17.904Z] [INFO] {"timestamp":"2025-10-25T21:48:17.904Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-25T21:48:17.905Z] [INFO] {"timestamp":"2025-10-25T21:48:17.905Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-25T21:48:17.905Z] [INFO] {"timestamp":"2025-10-25T21:48:17.905Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-25T21:48:17.907Z] [INFO] {"timestamp":"2025-10-25T21:48:17.907Z","level":"INFO","message":"[DIContainer] Registered module: GitVFS","details":{}}
[2025-10-25T21:48:17.907Z] [INFO] {"timestamp":"2025-10-25T21:48:17.907Z","level":"INFO","message":"[CoreLogic] Registered module: GitVFS from /upgrades/git-vfs.js","details":{}}
[2025-10-25T21:48:17.909Z] [INFO] {"timestamp":"2025-10-25T21:48:17.909Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-25T21:48:17.909Z] [INFO] {"timestamp":"2025-10-25T21:48:17.909Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-25T21:48:17.910Z] [INFO] {"timestamp":"2025-10-25T21:48:17.910Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-25T21:48:17.910Z] [INFO] {"timestamp":"2025-10-25T21:48:17.910Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-25T21:48:17.912Z] [LOG] [CoreLogic] Loaded module from /upgrades/diff-viewer-ui.js: NO_METADATA
[2025-10-25T21:48:17.912Z] [ERROR] {"timestamp":"2025-10-25T21:48:17.912Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/diff-viewer-ui.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T21:48:17.912Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T21:48:17.912Z] [WARN] {"timestamp":"2025-10-25T21:48:17.912Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/diff-viewer-ui.js missing metadata. Module:","details":{}}
[2025-10-25T21:48:17.915Z] [INFO] {"timestamp":"2025-10-25T21:48:17.914Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCSwarm","details":{}}
[2025-10-25T21:48:17.915Z] [INFO] {"timestamp":"2025-10-25T21:48:17.915Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCSwarm from /upgrades/webrtc-swarm.js","details":{}}
[2025-10-25T21:48:17.916Z] [INFO] {"timestamp":"2025-10-25T21:48:17.916Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCCoordinator","details":{}}
[2025-10-25T21:48:17.916Z] [INFO] {"timestamp":"2025-10-25T21:48:17.916Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCCoordinator from /upgrades/webrtc-coordinator.js","details":{}}
[2025-10-25T21:48:17.918Z] [INFO] {"timestamp":"2025-10-25T21:48:17.918Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-25T21:48:17.918Z] [INFO] {"timestamp":"2025-10-25T21:48:17.918Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-25T21:48:17.919Z] [INFO] {"timestamp":"2025-10-25T21:48:17.919Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-25T21:48:17.919Z] [INFO] {"timestamp":"2025-10-25T21:48:17.919Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-25T21:48:17.921Z] [INFO] {"timestamp":"2025-10-25T21:48:17.921Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-25T21:48:17.921Z] [INFO] {"timestamp":"2025-10-25T21:48:17.921Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-25T21:48:17.923Z] [INFO] {"timestamp":"2025-10-25T21:48:17.922Z","level":"INFO","message":"[DIContainer] Registered module: PeerReviewConsensus","details":{}}
[2025-10-25T21:48:17.923Z] [INFO] {"timestamp":"2025-10-25T21:48:17.923Z","level":"INFO","message":"[CoreLogic] Registered module: PeerReviewConsensus from /upgrades/peer-review-consensus.js","details":{}}
[2025-10-25T21:48:17.923Z] [INFO] {"timestamp":"2025-10-25T21:48:17.923Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-25T21:48:17.924Z] [INFO] {"timestamp":"2025-10-25T21:48:17.924Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-25T21:48:17.946Z] [WARN] {"timestamp":"2025-10-25T21:48:17.946Z","level":"WARN","message":"[Storage-Git] No Git repository found, initializing a new one.","details":{}}
[2025-10-25T21:48:17.950Z] [INFO] {"timestamp":"2025-10-25T21:48:17.950Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-25T21:48:17.951Z] [INFO] {"timestamp":"2025-10-25T21:48:17.951Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-25T21:48:17.951Z] [WARN] {"timestamp":"2025-10-25T21:48:17.951Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-25T21:48:17.951Z] [INFO] {"timestamp":"2025-10-25T21:48:17.951Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-25T21:48:17.951Z] [INFO] {"timestamp":"2025-10-25T21:48:17.951Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-25T21:48:17.952Z] [WARN] {"timestamp":"2025-10-25T21:48:17.952Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-25T21:48:17.952Z] [INFO] {"timestamp":"2025-10-25T21:48:17.952Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-25T21:48:17.953Z] [WARN] {"timestamp":"2025-10-25T21:48:17.953Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-25T21:48:17.953Z] [INFO] {"timestamp":"2025-10-25T21:48:17.953Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-25T21:48:17.953Z] [INFO] {"timestamp":"2025-10-25T21:48:17.953Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-25T21:48:17.954Z] [INFO] {"timestamp":"2025-10-25T21:48:17.954Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-25T21:48:17.957Z] [INFO] {"timestamp":"2025-10-25T21:48:17.957Z","level":"INFO","message":"[ReflectionStore] Initializing reflection persistence","details":{}}
[2025-10-25T21:48:17.964Z] [INFO] {"timestamp":"2025-10-25T21:48:17.964Z","level":"INFO","message":"[ReflectionStore] Creating database schema","details":{}}
[2025-10-25T21:48:17.964Z] [INFO] {"timestamp":"2025-10-25T21:48:17.964Z","level":"INFO","message":"[ReflectionStore] Database schema created","details":{}}
[2025-10-25T21:48:17.971Z] [INFO] {"timestamp":"2025-10-25T21:48:17.971Z","level":"INFO","message":"[ReflectionStore] Database opened successfully","details":{}}
[2025-10-25T21:48:17.972Z] [INFO] {"timestamp":"2025-10-25T21:48:17.972Z","level":"INFO","message":"[SelfTester] Initialized","details":{}}
[2025-10-25T21:48:17.972Z] [INFO] {"timestamp":"2025-10-25T21:48:17.972Z","level":"INFO","message":"[BrowserAPIs] Initializing web API integration...","details":{}}
[2025-10-25T21:48:17.973Z] [INFO] {"timestamp":"2025-10-25T21:48:17.973Z","level":"INFO","message":"[BrowserAPIs] Capabilities detected:","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-25T21:48:17.973Z] [INFO] {"timestamp":"2025-10-25T21:48:17.973Z","level":"INFO","message":"[EventBus] Emitting event: browser-apis:initialized","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-25T21:48:17.987Z] [INFO] {"timestamp":"2025-10-25T21:48:17.987Z","level":"INFO","message":"[GitVFS] Initializing new repository","details":{}}
[2025-10-25T21:48:18.001Z] [INFO] {"timestamp":"2025-10-25T21:48:18.001Z","level":"INFO","message":"[GitVFS] Repository initialized","details":{}}
[2025-10-25T21:48:18.001Z] [INFO] {"timestamp":"2025-10-25T21:48:18.001Z","level":"INFO","message":"[GitVFS] Initialized successfully","details":{}}
[2025-10-25T21:48:18.002Z] [INFO] {"timestamp":"2025-10-25T21:48:18.002Z","level":"INFO","message":"[WebRTCSwarm] Initializing swarm module","details":{}}
[2025-10-25T21:48:18.002Z] [INFO] {"timestamp":"2025-10-25T21:48:18.002Z","level":"INFO","message":"[WebRTCSwarm] Local peer ID: reploid-id_1761428898002_6a4lgqgyo","details":{}}
[2025-10-25T21:48:18.002Z] [INFO] {"timestamp":"2025-10-25T21:48:18.002Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-25T21:48:18.002Z] [INFO] {"timestamp":"2025-10-25T21:48:18.002Z","level":"INFO","message":"[WebRTCSwarm] Swarm initialized","details":{}}
[2025-10-25T21:48:18.003Z] [INFO] {"timestamp":"2025-10-25T21:48:18.003Z","level":"INFO","message":"[SwarmOrch] Initializing swarm orchestrator","details":{}}
[2025-10-25T21:48:18.003Z] [WARN] {"timestamp":"2025-10-25T21:48:18.003Z","level":"WARN","message":"[WebRTCSwarm] Cannot send signaling message: not connected","details":{}}
[2025-10-25T21:48:18.003Z] [INFO] {"timestamp":"2025-10-25T21:48:18.003Z","level":"INFO","message":"[SwarmOrch] Swarm orchestrator initialized","details":{"capabilities":["code-generation","file-management"]}}
[2025-10-25T21:48:18.005Z] [INFO] {"timestamp":"2025-10-25T21:48:18.005Z","level":"INFO","message":"[PyodideRuntime] Initializing Pyodide runtime...","details":{}}
[2025-10-25T21:48:18.006Z] [INFO] {"timestamp":"2025-10-25T21:48:18.005Z","level":"INFO","message":"[PyodideRuntime] Worker created","details":{}}
[2025-10-25T21:48:18.007Z] [INFO] {"timestamp":"2025-10-25T21:48:18.007Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-25T21:48:18.009Z] [INFO] {"timestamp":"2025-10-25T21:48:18.009Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-25T21:48:19.667Z] [INFO] {"timestamp":"2025-10-25T21:48:19.667Z","level":"INFO","message":"[PyodideRuntime] Pyodide initialized","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-25T21:48:19.667Z] [INFO] {"timestamp":"2025-10-25T21:48:19.667Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:ready","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-25T21:48:19.667Z] [INFO] {"timestamp":"2025-10-25T21:48:19.667Z","level":"INFO","message":"[PyodideRuntime] Pyodide runtime ready","details":{}}
[2025-10-25T21:48:19.667Z] [INFO] {"timestamp":"2025-10-25T21:48:19.667Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:initialized","details":{"ready":true}}
[2025-10-25T21:48:19.669Z] [INFO] {"timestamp":"2025-10-25T21:48:19.669Z","level":"INFO","message":"Dashboard UI Manager (Event-Driven) taking control of DOM...","details":{}}
[2025-10-25T21:48:19.681Z] [INFO] {"timestamp":"2025-10-25T21:48:19.681Z","level":"INFO","message":"[UIManager] Initializing modular panel support...","details":{}}
[2025-10-25T21:48:19.681Z] [INFO] {"timestamp":"2025-10-25T21:48:19.681Z","level":"INFO","message":"[UIManager] Modular panel initialization complete","details":{}}
[2025-10-25T21:48:19.682Z] [INFO] {"timestamp":"2025-10-25T21:48:19.681Z","level":"INFO","message":"Dashboard UI Initialized. Listening for events.","details":{}}
[2025-10-25T21:48:19.682Z] [LOG] [CoreLogic] Attempting to initialize DiffViewerUI...
[2025-10-25T21:48:19.682Z] [LOG] [CoreLogic] Resolving DiffViewerUI from container...
[2025-10-25T21:48:19.682Z] [ERROR] [CoreLogic] DiffViewerUI initialization error: Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:514:33)), <anonymous>:61:23)
    at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:514:33), <anonymous>:317:46)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:535:9)
[2025-10-25T21:48:19.682Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T21:48:19.682Z] [WARN] {"timestamp":"2025-10-25T21:48:19.682Z","level":"WARN","message":"[CoreLogic] DiffViewerUI initialization failed:","details":"[DIContainer] Service not found: DiffViewerUI\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-25T21:48:19.682Z] [INFO] {"timestamp":"2025-10-25T21:48:19.682Z","level":"INFO","message":"[CoreLogic] Agent initialization complete. System is operational.","details":{}}
[2025-10-25T21:48:19.682Z] [INFO] {"timestamp":"2025-10-25T21:48:19.682Z","level":"INFO","message":"[CoreLogic] Creating genesis snapshot of initial boot state...","details":{}}
[2025-10-25T21:48:19.682Z] [WARN] {"timestamp":"2025-10-25T21:48:19.682Z","level":"WARN","message":"[CoreLogic] Failed to create genesis snapshot (non-fatal):","details":"[DIContainer] Service not found: GenesisSnapshot\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-25T21:48:19.682Z] [INFO] {"timestamp":"2025-10-25T21:48:19.682Z","level":"INFO","message":"[CoreLogic] Starting agent with initial goal:","details":"Create tools to make tools to make tools"}
[2025-10-25T21:48:19.682Z] [INFO] {"timestamp":"2025-10-25T21:48:19.682Z","level":"INFO","message":"[UI] updateGoal called with:","details":{"text":"Create tools to make tools to make tools","hasGoalTextRef":true}}
[2025-10-25T21:48:19.682Z] [INFO] {"timestamp":"2025-10-25T21:48:19.682Z","level":"INFO","message":"[UI] Goal text element updated successfully","details":{}}
[2025-10-25T21:48:19.715Z] [INFO] {"timestamp":"2025-10-25T21:48:19.715Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761428899688_8167f9cb16fb3cbc/session.json (SHA: 9080ab4)","details":{}}
[2025-10-25T21:48:19.715Z] [INFO] {"timestamp":"2025-10-25T21:48:19.715Z","level":"INFO","message":"[SessionManager] Created new session: session_1761428899688_8167f9cb16fb3cbc","details":{}}
[2025-10-25T21:48:19.724Z] [INFO] {"timestamp":"2025-10-25T21:48:19.723Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.checkpoints/checkpoint_1761428899715.json (SHA: 89b84f6)","details":{}}
[2025-10-25T21:48:19.724Z] [INFO] {"timestamp":"2025-10-25T21:48:19.724Z","level":"INFO","message":"[StateManager] Created checkpoint: checkpoint_1761428899715 - Session session_1761428899688_8167f9cb16fb3cbc - Turn 0 start","details":{}}
[2025-10-25T21:48:19.724Z] [INFO] {"timestamp":"2025-10-25T21:48:19.724Z","level":"INFO","message":"[SessionManager] Created turn checkpoint: checkpoint_1761428899715","details":{}}
[2025-10-25T21:48:19.730Z] [INFO] {"timestamp":"2025-10-25T21:48:19.730Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761428899688_8167f9cb16fb3cbc/session.json (SHA: 07fc480)","details":{}}
[2025-10-25T21:48:19.730Z] [INFO] {"timestamp":"2025-10-25T21:48:19.730Z","level":"INFO","message":"[SessionManager] Created turn 0 for session session_1761428899688_8167f9cb16fb3cbc","details":{}}
[2025-10-25T21:48:19.730Z] [INFO] {"timestamp":"2025-10-25T21:48:19.730Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"","progress":null}}
[2025-10-25T21:48:19.731Z] [INFO] {"timestamp":"2025-10-25T21:48:19.731Z","level":"INFO","message":"[SentinelFSM] State transition: IDLE -> CURATING_CONTEXT","details":{}}
[2025-10-25T21:48:19.731Z] [INFO] {"timestamp":"2025-10-25T21:48:19.731Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"IDLE","newState":"CURATING_CONTEXT","context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761428899688_8167f9cb16fb3cbc","turn":{"turn":0,"context_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.context.md","proposal_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761428899715","createdAt":"2025-10-25T21:48:19.724Z"},"startTime":1761428899730,"iterations":0,"maxIterations":10}}}
[2025-10-25T21:48:19.731Z] [INFO] {"timestamp":"2025-10-25T21:48:19.731Z","level":"INFO","message":"[SentinelFSM] Executing state: CURATING_CONTEXT","details":{}}
[2025-10-25T21:48:19.731Z] [INFO] {"timestamp":"2025-10-25T21:48:19.731Z","level":"INFO","message":"[EventBus] Emitting event: agent:curating","details":{"goal":"Create tools to make tools to make tools"}}
[2025-10-25T21:48:19.731Z] [INFO] {"timestamp":"2025-10-25T21:48:19.731Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Analyzing project files...","progress":null}}
[2025-10-25T21:48:19.731Z] [WARN] {"timestamp":"2025-10-25T21:48:19.731Z","level":"WARN","message":"[SentinelTools] No files available for curation","details":{}}
[2025-10-25T21:48:19.731Z] [INFO] {"timestamp":"2025-10-25T21:48:19.731Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Found 0 relevant files","progress":50}}
[2025-10-25T21:48:19.731Z] [ERROR] {"timestamp":"2025-10-25T21:48:19.731Z","level":"ERROR","message":"[SentinelFSM] Error in state CURATING_CONTEXT:","details":{}}
[2025-10-25T21:48:19.731Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T21:48:19.732Z] [INFO] {"timestamp":"2025-10-25T21:48:19.731Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"ERROR","detail":"","progress":null}}
[2025-10-25T21:48:19.732Z] [INFO] {"timestamp":"2025-10-25T21:48:19.732Z","level":"INFO","message":"[SentinelFSM] State transition: CURATING_CONTEXT -> ERROR","details":{}}
[2025-10-25T21:48:19.732Z] [INFO] {"timestamp":"2025-10-25T21:48:19.732Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"CURATING_CONTEXT","newState":"ERROR","context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761428899688_8167f9cb16fb3cbc","turn":{"turn":0,"context_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.context.md","proposal_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761428899715","createdAt":"2025-10-25T21:48:19.724Z"},"startTime":1761428899730,"iterations":0,"maxIterations":10}}}
[2025-10-25T21:48:19.732Z] [INFO] {"timestamp":"2025-10-25T21:48:19.732Z","level":"INFO","message":"[SentinelFSM] Executing state: ERROR","details":{}}
[2025-10-25T21:48:19.732Z] [INFO] {"timestamp":"2025-10-25T21:48:19.732Z","level":"INFO","message":"[EventBus] Emitting event: agent:error","details":{"state":{"from":"CURATING_CONTEXT","to":"ERROR","timestamp":1761428899731,"context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761428899688_8167f9cb16fb3cbc","turn":{"turn":0,"context_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.context.md","proposal_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761428899715","createdAt":"2025-10-25T21:48:19.724Z"},"startTime":1761428899730,"iterations":0,"maxIterations":10}},"context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761428899688_8167f9cb16fb3cbc","turn":{"turn":0,"context_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.context.md","proposal_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761428899715","createdAt":"2025-10-25T21:48:19.724Z"},"startTime":1761428899730,"iterations":0,"maxIterations":10}}}
[2025-10-25T21:48:19.732Z] [ERROR] {"timestamp":"2025-10-25T21:48:19.732Z","level":"ERROR","message":"[SentinelFSM] Error state reached","details":{"previous_state":{"from":"IDLE","to":"CURATING_CONTEXT","timestamp":1761428899730,"context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761428899688_8167f9cb16fb3cbc","turn":{"turn":0,"context_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.context.md","proposal_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761428899715","createdAt":"2025-10-25T21:48:19.724Z"},"startTime":1761428899730,"iterations":0,"maxIterations":10}},"context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761428899688_8167f9cb16fb3cbc","turn":{"turn":0,"context_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.context.md","proposal_path":"/sessions/session_1761428899688_8167f9cb16fb3cbc/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761428899715","createdAt":"2025-10-25T21:48:19.724Z"},"startTime":1761428899730,"iterations":0,"maxIterations":10}}}
[2025-10-25T21:48:19.732Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T21:48:19.732Z] [INFO] {"timestamp":"2025-10-25T21:48:19.732Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"IDLE","detail":"","progress":null}}
[2025-10-25T21:48:19.732Z] [INFO] {"timestamp":"2025-10-25T21:48:19.732Z","level":"INFO","message":"[SentinelFSM] State transition: ERROR -> IDLE","details":{}}
[2025-10-25T21:48:19.732Z] [INFO] {"timestamp":"2025-10-25T21:48:19.732Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"ERROR","newState":"IDLE","context":null}}
[2025-10-25T21:48:19.732Z] [INFO] {"timestamp":"2025-10-25T21:48:19.732Z","level":"INFO","message":"[CoreLogic] Agent cycle started successfully","details":{}}
[2025-10-25T21:48:19.732Z] [LOG] [Boot] Agent system awakened successfully
[2025-10-25T21:52:50.936Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:52:50.936Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:52:50.971Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T21:52:50.971Z] [LOG] [API] Checking server status...
[2025-10-25T21:52:50.977Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:52:50.973Z"
}
[2025-10-25T21:52:50.977Z] [LOG] [API] WebGPU available
[2025-10-25T21:52:50.985Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:52:50.985Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:52:50.985Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:52:51.003Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T21:54:07.879Z] [LOG] [ModelConfig] Configuration saved
[2025-10-25T21:54:07.881Z] [LOG] [ModelConfig] Model saved: {
  "id": "gemma-2b-it-q4f16_1-MLC",
  "name": "Gemma 2B",
  "provider": "webllm",
  "hostType": "webgpu-browser",
  "queryMethod": "browser",
  "keySource": "none",
  "keyId": null
}
[2025-10-25T21:54:26.992Z] [INFO] {"timestamp":"2025-10-25T21:54:26.992Z","level":"INFO","message":"[CoreLogic] Registered module: ToastNotifications from /upgrades/toast-notifications.js","details":{}}
[2025-10-25T21:54:26.993Z] [INFO] {"timestamp":"2025-10-25T21:54:26.993Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-25T21:54:26.993Z] [INFO] {"timestamp":"2025-10-25T21:54:26.993Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-25T21:54:26.994Z] [INFO] {"timestamp":"2025-10-25T21:54:26.994Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-25T21:54:26.994Z] [INFO] {"timestamp":"2025-10-25T21:54:26.994Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-25T21:54:26.996Z] [INFO] {"timestamp":"2025-10-25T21:54:26.996Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-25T21:54:26.996Z] [INFO] {"timestamp":"2025-10-25T21:54:26.996Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-25T21:54:26.997Z] [INFO] {"timestamp":"2025-10-25T21:54:26.997Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-25T21:54:26.997Z] [INFO] {"timestamp":"2025-10-25T21:54:26.997Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-25T21:54:26.998Z] [INFO] {"timestamp":"2025-10-25T21:54:26.998Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-25T21:54:26.998Z] [INFO] {"timestamp":"2025-10-25T21:54:26.998Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-25T21:54:27.000Z] [INFO] {"timestamp":"2025-10-25T21:54:27.000Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-25T21:54:27.000Z] [INFO] {"timestamp":"2025-10-25T21:54:27.000Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-25T21:54:27.001Z] [INFO] {"timestamp":"2025-10-25T21:54:27.001Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-25T21:54:27.001Z] [INFO] {"timestamp":"2025-10-25T21:54:27.001Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-25T21:54:27.003Z] [INFO] {"timestamp":"2025-10-25T21:54:27.003Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-25T21:54:27.003Z] [INFO] {"timestamp":"2025-10-25T21:54:27.003Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-25T21:54:27.004Z] [INFO] {"timestamp":"2025-10-25T21:54:27.004Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-25T21:54:27.004Z] [INFO] {"timestamp":"2025-10-25T21:54:27.004Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-25T21:54:27.005Z] [INFO] {"timestamp":"2025-10-25T21:54:27.005Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-25T21:54:27.005Z] [INFO] {"timestamp":"2025-10-25T21:54:27.005Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-25T21:54:27.006Z] [INFO] {"timestamp":"2025-10-25T21:54:27.006Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-25T21:54:27.006Z] [INFO] {"timestamp":"2025-10-25T21:54:27.006Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-25T21:54:27.007Z] [INFO] {"timestamp":"2025-10-25T21:54:27.007Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-25T21:54:27.008Z] [INFO] {"timestamp":"2025-10-25T21:54:27.008Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-25T21:54:27.009Z] [INFO] {"timestamp":"2025-10-25T21:54:27.009Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-25T21:54:27.009Z] [INFO] {"timestamp":"2025-10-25T21:54:27.009Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-25T21:54:27.010Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-25T21:54:27.010Z] [ERROR] {"timestamp":"2025-10-25T21:54:27.010Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T21:54:27.010Z] [WARN] {"timestamp":"2025-10-25T21:54:27.010Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-25T21:54:27.010Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-25T21:54:27.010Z] [ERROR] {"timestamp":"2025-10-25T21:54:27.010Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T21:54:27.011Z] [WARN] {"timestamp":"2025-10-25T21:54:27.010Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-25T21:54:27.011Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-25T21:54:27.011Z] [ERROR] {"timestamp":"2025-10-25T21:54:27.011Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T21:54:27.011Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T21:54:27.011Z] [WARN] {"timestamp":"2025-10-25T21:54:27.011Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-25T21:54:27.012Z] [INFO] {"timestamp":"2025-10-25T21:54:27.012Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-25T21:54:27.012Z] [INFO] {"timestamp":"2025-10-25T21:54:27.012Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-25T21:54:27.013Z] [INFO] {"timestamp":"2025-10-25T21:54:27.013Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-25T21:54:27.013Z] [INFO] {"timestamp":"2025-10-25T21:54:27.013Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-25T21:54:27.016Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-25T21:54:27.016Z] [INFO] {"timestamp":"2025-10-25T21:54:27.016Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-25T21:54:27.016Z] [INFO] {"timestamp":"2025-10-25T21:54:27.016Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-25T21:54:27.018Z] [INFO] {"timestamp":"2025-10-25T21:54:27.018Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-25T21:54:27.018Z] [INFO] {"timestamp":"2025-10-25T21:54:27.018Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-25T21:54:27.019Z] [INFO] {"timestamp":"2025-10-25T21:54:27.019Z","level":"INFO","message":"[DIContainer] Registered module: GitVFS","details":{}}
[2025-10-25T21:54:27.019Z] [INFO] {"timestamp":"2025-10-25T21:54:27.019Z","level":"INFO","message":"[CoreLogic] Registered module: GitVFS from /upgrades/git-vfs.js","details":{}}
[2025-10-25T21:54:27.021Z] [INFO] {"timestamp":"2025-10-25T21:54:27.021Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-25T21:54:27.021Z] [INFO] {"timestamp":"2025-10-25T21:54:27.021Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-25T21:54:27.022Z] [INFO] {"timestamp":"2025-10-25T21:54:27.022Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-25T21:54:27.022Z] [INFO] {"timestamp":"2025-10-25T21:54:27.022Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-25T21:54:27.023Z] [LOG] [CoreLogic] Loaded module from /upgrades/diff-viewer-ui.js: NO_METADATA
[2025-10-25T21:54:27.023Z] [ERROR] {"timestamp":"2025-10-25T21:54:27.023Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/diff-viewer-ui.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T21:54:27.023Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T21:54:27.023Z] [WARN] {"timestamp":"2025-10-25T21:54:27.023Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/diff-viewer-ui.js missing metadata. Module:","details":{}}
[2025-10-25T21:54:27.026Z] [INFO] {"timestamp":"2025-10-25T21:54:27.026Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCSwarm","details":{}}
[2025-10-25T21:54:27.026Z] [INFO] {"timestamp":"2025-10-25T21:54:27.026Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCSwarm from /upgrades/webrtc-swarm.js","details":{}}
[2025-10-25T21:54:27.027Z] [INFO] {"timestamp":"2025-10-25T21:54:27.027Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCCoordinator","details":{}}
[2025-10-25T21:54:27.027Z] [INFO] {"timestamp":"2025-10-25T21:54:27.027Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCCoordinator from /upgrades/webrtc-coordinator.js","details":{}}
[2025-10-25T21:54:27.028Z] [INFO] {"timestamp":"2025-10-25T21:54:27.028Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-25T21:54:27.028Z] [INFO] {"timestamp":"2025-10-25T21:54:27.028Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-25T21:54:27.030Z] [INFO] {"timestamp":"2025-10-25T21:54:27.030Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-25T21:54:27.030Z] [INFO] {"timestamp":"2025-10-25T21:54:27.030Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-25T21:54:27.031Z] [INFO] {"timestamp":"2025-10-25T21:54:27.031Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-25T21:54:27.031Z] [INFO] {"timestamp":"2025-10-25T21:54:27.031Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-25T21:54:27.032Z] [INFO] {"timestamp":"2025-10-25T21:54:27.032Z","level":"INFO","message":"[DIContainer] Registered module: PeerReviewConsensus","details":{}}
[2025-10-25T21:54:27.032Z] [INFO] {"timestamp":"2025-10-25T21:54:27.032Z","level":"INFO","message":"[CoreLogic] Registered module: PeerReviewConsensus from /upgrades/peer-review-consensus.js","details":{}}
[2025-10-25T21:54:27.032Z] [INFO] {"timestamp":"2025-10-25T21:54:27.032Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-25T21:54:27.033Z] [INFO] {"timestamp":"2025-10-25T21:54:27.033Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-25T21:54:27.035Z] [INFO] {"timestamp":"2025-10-25T21:54:27.035Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-25T21:54:27.036Z] [INFO] {"timestamp":"2025-10-25T21:54:27.035Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-25T21:54:27.036Z] [INFO] {"timestamp":"2025-10-25T21:54:27.036Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-25T21:54:27.036Z] [WARN] {"timestamp":"2025-10-25T21:54:27.036Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-25T21:54:27.037Z] [INFO] {"timestamp":"2025-10-25T21:54:27.037Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-25T21:54:27.037Z] [INFO] {"timestamp":"2025-10-25T21:54:27.037Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-25T21:54:27.037Z] [WARN] {"timestamp":"2025-10-25T21:54:27.037Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-25T21:54:27.038Z] [INFO] {"timestamp":"2025-10-25T21:54:27.038Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-25T21:54:27.038Z] [WARN] {"timestamp":"2025-10-25T21:54:27.038Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-25T21:54:27.039Z] [INFO] {"timestamp":"2025-10-25T21:54:27.038Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-25T21:54:27.039Z] [INFO] {"timestamp":"2025-10-25T21:54:27.039Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-25T21:54:27.039Z] [INFO] {"timestamp":"2025-10-25T21:54:27.039Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-25T21:54:27.042Z] [INFO] {"timestamp":"2025-10-25T21:54:27.042Z","level":"INFO","message":"[ReflectionStore] Initializing reflection persistence","details":{}}
[2025-10-25T21:54:27.043Z] [INFO] {"timestamp":"2025-10-25T21:54:27.043Z","level":"INFO","message":"[ReflectionStore] Database opened successfully","details":{}}
[2025-10-25T21:54:27.043Z] [INFO] {"timestamp":"2025-10-25T21:54:27.043Z","level":"INFO","message":"[SelfTester] Initialized","details":{}}
[2025-10-25T21:54:27.043Z] [INFO] {"timestamp":"2025-10-25T21:54:27.043Z","level":"INFO","message":"[BrowserAPIs] Initializing web API integration...","details":{}}
[2025-10-25T21:54:27.044Z] [INFO] {"timestamp":"2025-10-25T21:54:27.044Z","level":"INFO","message":"[BrowserAPIs] Capabilities detected:","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-25T21:54:27.044Z] [INFO] {"timestamp":"2025-10-25T21:54:27.044Z","level":"INFO","message":"[EventBus] Emitting event: browser-apis:initialized","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-25T21:54:27.045Z] [INFO] {"timestamp":"2025-10-25T21:54:27.045Z","level":"INFO","message":"[GitVFS] Initialized successfully","details":{}}
[2025-10-25T21:54:27.046Z] [INFO] {"timestamp":"2025-10-25T21:54:27.046Z","level":"INFO","message":"[WebRTCSwarm] Initializing swarm module","details":{}}
[2025-10-25T21:54:27.046Z] [INFO] {"timestamp":"2025-10-25T21:54:27.046Z","level":"INFO","message":"[WebRTCSwarm] Local peer ID: reploid-id_1761429267046_jcs46hiwv","details":{}}
[2025-10-25T21:54:27.046Z] [INFO] {"timestamp":"2025-10-25T21:54:27.046Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-25T21:54:27.046Z] [INFO] {"timestamp":"2025-10-25T21:54:27.046Z","level":"INFO","message":"[WebRTCSwarm] Swarm initialized","details":{}}
[2025-10-25T21:54:27.047Z] [INFO] {"timestamp":"2025-10-25T21:54:27.047Z","level":"INFO","message":"[SwarmOrch] Initializing swarm orchestrator","details":{}}
[2025-10-25T21:54:27.047Z] [WARN] {"timestamp":"2025-10-25T21:54:27.047Z","level":"WARN","message":"[WebRTCSwarm] Cannot send signaling message: not connected","details":{}}
[2025-10-25T21:54:27.047Z] [INFO] {"timestamp":"2025-10-25T21:54:27.047Z","level":"INFO","message":"[SwarmOrch] Swarm orchestrator initialized","details":{"capabilities":["code-generation","file-management"]}}
[2025-10-25T21:54:27.049Z] [INFO] {"timestamp":"2025-10-25T21:54:27.049Z","level":"INFO","message":"[PyodideRuntime] Initializing Pyodide runtime...","details":{}}
[2025-10-25T21:54:27.049Z] [INFO] {"timestamp":"2025-10-25T21:54:27.049Z","level":"INFO","message":"[PyodideRuntime] Worker created","details":{}}
[2025-10-25T21:54:27.051Z] [INFO] {"timestamp":"2025-10-25T21:54:27.051Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-25T21:54:27.053Z] [INFO] {"timestamp":"2025-10-25T21:54:27.053Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-25T21:54:28.817Z] [INFO] {"timestamp":"2025-10-25T21:54:28.817Z","level":"INFO","message":"[PyodideRuntime] Pyodide initialized","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-25T21:54:28.817Z] [INFO] {"timestamp":"2025-10-25T21:54:28.817Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:ready","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-25T21:54:28.817Z] [INFO] {"timestamp":"2025-10-25T21:54:28.817Z","level":"INFO","message":"[PyodideRuntime] Pyodide runtime ready","details":{}}
[2025-10-25T21:54:28.817Z] [INFO] {"timestamp":"2025-10-25T21:54:28.817Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:initialized","details":{"ready":true}}
[2025-10-25T21:54:28.819Z] [INFO] {"timestamp":"2025-10-25T21:54:28.819Z","level":"INFO","message":"Dashboard UI Manager (Event-Driven) taking control of DOM...","details":{}}
[2025-10-25T21:54:28.832Z] [INFO] {"timestamp":"2025-10-25T21:54:28.832Z","level":"INFO","message":"[UIManager] Initializing modular panel support...","details":{}}
[2025-10-25T21:54:28.832Z] [INFO] {"timestamp":"2025-10-25T21:54:28.832Z","level":"INFO","message":"[UIManager] Modular panel initialization complete","details":{}}
[2025-10-25T21:54:28.833Z] [INFO] {"timestamp":"2025-10-25T21:54:28.833Z","level":"INFO","message":"Dashboard UI Initialized. Listening for events.","details":{}}
[2025-10-25T21:54:28.833Z] [LOG] [CoreLogic] Attempting to initialize DiffViewerUI...
[2025-10-25T21:54:28.833Z] [LOG] [CoreLogic] Resolving DiffViewerUI from container...
[2025-10-25T21:54:28.833Z] [ERROR] [CoreLogic] DiffViewerUI initialization error: Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:514:33)), <anonymous>:61:23)
    at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:514:33), <anonymous>:317:46)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:535:9)
[2025-10-25T21:54:28.833Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T21:54:28.833Z] [WARN] {"timestamp":"2025-10-25T21:54:28.833Z","level":"WARN","message":"[CoreLogic] DiffViewerUI initialization failed:","details":"[DIContainer] Service not found: DiffViewerUI\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-25T21:54:28.833Z] [INFO] {"timestamp":"2025-10-25T21:54:28.833Z","level":"INFO","message":"[CoreLogic] Agent initialization complete. System is operational.","details":{}}
[2025-10-25T21:54:28.833Z] [INFO] {"timestamp":"2025-10-25T21:54:28.833Z","level":"INFO","message":"[CoreLogic] Creating genesis snapshot of initial boot state...","details":{}}
[2025-10-25T21:54:28.833Z] [WARN] {"timestamp":"2025-10-25T21:54:28.833Z","level":"WARN","message":"[CoreLogic] Failed to create genesis snapshot (non-fatal):","details":"[DIContainer] Service not found: GenesisSnapshot\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-25T21:54:28.833Z] [INFO] {"timestamp":"2025-10-25T21:54:28.833Z","level":"INFO","message":"[CoreLogic] Starting agent with initial goal:","details":"Analyze your own inefficiency patterns and improve yourself"}
[2025-10-25T21:54:28.833Z] [INFO] {"timestamp":"2025-10-25T21:54:28.833Z","level":"INFO","message":"[UI] updateGoal called with:","details":{"text":"Analyze your own inefficiency patterns and improve yourself","hasGoalTextRef":true}}
[2025-10-25T21:54:28.833Z] [INFO] {"timestamp":"2025-10-25T21:54:28.833Z","level":"INFO","message":"[UI] Goal text element updated successfully","details":{}}
[2025-10-25T21:54:28.856Z] [INFO] {"timestamp":"2025-10-25T21:54:28.856Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761429268835_edfc2fefe7dc0090/session.json (SHA: ca617c0)","details":{}}
[2025-10-25T21:54:28.856Z] [INFO] {"timestamp":"2025-10-25T21:54:28.856Z","level":"INFO","message":"[SessionManager] Created new session: session_1761429268835_edfc2fefe7dc0090","details":{}}
[2025-10-25T21:54:28.862Z] [INFO] {"timestamp":"2025-10-25T21:54:28.862Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.checkpoints/checkpoint_1761429268856.json (SHA: 6530b81)","details":{}}
[2025-10-25T21:54:28.862Z] [INFO] {"timestamp":"2025-10-25T21:54:28.862Z","level":"INFO","message":"[StateManager] Created checkpoint: checkpoint_1761429268856 - Session session_1761429268835_edfc2fefe7dc0090 - Turn 0 start","details":{}}
[2025-10-25T21:54:28.862Z] [INFO] {"timestamp":"2025-10-25T21:54:28.862Z","level":"INFO","message":"[SessionManager] Created turn checkpoint: checkpoint_1761429268856","details":{}}
[2025-10-25T21:54:28.869Z] [INFO] {"timestamp":"2025-10-25T21:54:28.869Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761429268835_edfc2fefe7dc0090/session.json (SHA: 8dcaf4f)","details":{}}
[2025-10-25T21:54:28.869Z] [INFO] {"timestamp":"2025-10-25T21:54:28.869Z","level":"INFO","message":"[SessionManager] Created turn 0 for session session_1761429268835_edfc2fefe7dc0090","details":{}}
[2025-10-25T21:54:28.869Z] [INFO] {"timestamp":"2025-10-25T21:54:28.869Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"","progress":null}}
[2025-10-25T21:54:28.870Z] [INFO] {"timestamp":"2025-10-25T21:54:28.870Z","level":"INFO","message":"[SentinelFSM] State transition: IDLE -> CURATING_CONTEXT","details":{}}
[2025-10-25T21:54:28.870Z] [INFO] {"timestamp":"2025-10-25T21:54:28.870Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"IDLE","newState":"CURATING_CONTEXT","context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761429268835_edfc2fefe7dc0090","turn":{"turn":0,"context_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.context.md","proposal_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429268856","createdAt":"2025-10-25T21:54:28.862Z"},"startTime":1761429268869,"iterations":0,"maxIterations":10}}}
[2025-10-25T21:54:28.870Z] [INFO] {"timestamp":"2025-10-25T21:54:28.870Z","level":"INFO","message":"[SentinelFSM] Executing state: CURATING_CONTEXT","details":{}}
[2025-10-25T21:54:28.870Z] [INFO] {"timestamp":"2025-10-25T21:54:28.870Z","level":"INFO","message":"[EventBus] Emitting event: agent:curating","details":{"goal":"Analyze your own inefficiency patterns and improve yourself"}}
[2025-10-25T21:54:28.870Z] [INFO] {"timestamp":"2025-10-25T21:54:28.870Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Analyzing project files...","progress":null}}
[2025-10-25T21:54:28.870Z] [WARN] {"timestamp":"2025-10-25T21:54:28.870Z","level":"WARN","message":"[SentinelTools] No files available for curation","details":{}}
[2025-10-25T21:54:28.870Z] [INFO] {"timestamp":"2025-10-25T21:54:28.870Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Found 0 relevant files","progress":50}}
[2025-10-25T21:54:28.871Z] [ERROR] {"timestamp":"2025-10-25T21:54:28.870Z","level":"ERROR","message":"[SentinelFSM] Error in state CURATING_CONTEXT:","details":{}}
[2025-10-25T21:54:28.871Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T21:54:28.871Z] [INFO] {"timestamp":"2025-10-25T21:54:28.871Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"ERROR","detail":"","progress":null}}
[2025-10-25T21:54:28.871Z] [INFO] {"timestamp":"2025-10-25T21:54:28.871Z","level":"INFO","message":"[SentinelFSM] State transition: CURATING_CONTEXT -> ERROR","details":{}}
[2025-10-25T21:54:28.871Z] [INFO] {"timestamp":"2025-10-25T21:54:28.871Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"CURATING_CONTEXT","newState":"ERROR","context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761429268835_edfc2fefe7dc0090","turn":{"turn":0,"context_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.context.md","proposal_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429268856","createdAt":"2025-10-25T21:54:28.862Z"},"startTime":1761429268869,"iterations":0,"maxIterations":10}}}
[2025-10-25T21:54:28.871Z] [INFO] {"timestamp":"2025-10-25T21:54:28.871Z","level":"INFO","message":"[SentinelFSM] Executing state: ERROR","details":{}}
[2025-10-25T21:54:28.871Z] [INFO] {"timestamp":"2025-10-25T21:54:28.871Z","level":"INFO","message":"[EventBus] Emitting event: agent:error","details":{"state":{"from":"CURATING_CONTEXT","to":"ERROR","timestamp":1761429268871,"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761429268835_edfc2fefe7dc0090","turn":{"turn":0,"context_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.context.md","proposal_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429268856","createdAt":"2025-10-25T21:54:28.862Z"},"startTime":1761429268869,"iterations":0,"maxIterations":10}},"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761429268835_edfc2fefe7dc0090","turn":{"turn":0,"context_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.context.md","proposal_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429268856","createdAt":"2025-10-25T21:54:28.862Z"},"startTime":1761429268869,"iterations":0,"maxIterations":10}}}
[2025-10-25T21:54:28.871Z] [ERROR] {"timestamp":"2025-10-25T21:54:28.871Z","level":"ERROR","message":"[SentinelFSM] Error state reached","details":{"previous_state":{"from":"IDLE","to":"CURATING_CONTEXT","timestamp":1761429268869,"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761429268835_edfc2fefe7dc0090","turn":{"turn":0,"context_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.context.md","proposal_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429268856","createdAt":"2025-10-25T21:54:28.862Z"},"startTime":1761429268869,"iterations":0,"maxIterations":10}},"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761429268835_edfc2fefe7dc0090","turn":{"turn":0,"context_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.context.md","proposal_path":"/sessions/session_1761429268835_edfc2fefe7dc0090/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429268856","createdAt":"2025-10-25T21:54:28.862Z"},"startTime":1761429268869,"iterations":0,"maxIterations":10}}}
[2025-10-25T21:54:28.871Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T21:54:28.871Z] [INFO] {"timestamp":"2025-10-25T21:54:28.871Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"IDLE","detail":"","progress":null}}
[2025-10-25T21:54:28.871Z] [INFO] {"timestamp":"2025-10-25T21:54:28.871Z","level":"INFO","message":"[SentinelFSM] State transition: ERROR -> IDLE","details":{}}
[2025-10-25T21:54:28.871Z] [INFO] {"timestamp":"2025-10-25T21:54:28.871Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"ERROR","newState":"IDLE","context":null}}
[2025-10-25T21:54:28.871Z] [INFO] {"timestamp":"2025-10-25T21:54:28.871Z","level":"INFO","message":"[CoreLogic] Agent cycle started successfully","details":{}}
[2025-10-25T21:54:28.871Z] [LOG] [Boot] Agent system awakened successfully
[2025-10-25T21:54:51.741Z] [INFO] {"timestamp":"2025-10-25T21:54:51.741Z","level":"INFO","message":"[UIManager] Theme changed","details":{"theme":"light"}}
[2025-10-25T21:54:53.378Z] [INFO] {"timestamp":"2025-10-25T21:54:53.378Z","level":"INFO","message":"[UIManager] Theme changed","details":{"theme":"dark"}}
[2025-10-25T21:54:58.585Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:54:58.585Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:54:58.590Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T21:54:58.590Z] [LOG] [API] Checking server status...
[2025-10-25T21:54:58.599Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:54:58.591Z"
}
[2025-10-25T21:54:58.599Z] [LOG] [API] WebGPU available
[2025-10-25T21:54:58.608Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:54:58.608Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:54:58.608Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:54:58.615Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  },
  {
    "id": "gemma-2b-it-q4f16_1-MLC",
    "name": "Gemma 2B",
    "provider": "webllm",
    "hostType": "webgpu-browser",
    "queryMethod": "browser",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T21:55:02.809Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:55:02.810Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:55:02.844Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T21:55:02.844Z] [LOG] [API] Checking server status...
[2025-10-25T21:55:02.852Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:55:02.846Z"
}
[2025-10-25T21:55:02.852Z] [LOG] [API] WebGPU available
[2025-10-25T21:55:02.857Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:55:02.857Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:55:02.857Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:55:02.875Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  },
  {
    "id": "gemma-2b-it-q4f16_1-MLC",
    "name": "Gemma 2B",
    "provider": "webllm",
    "hostType": "webgpu-browser",
    "queryMethod": "browser",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T21:55:11.889Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:55:11.889Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:55:11.894Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T21:55:11.894Z] [LOG] [API] Checking server status...
[2025-10-25T21:55:11.907Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:55:11.896Z"
}
[2025-10-25T21:55:11.908Z] [LOG] [API] WebGPU available
[2025-10-25T21:55:11.913Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:55:11.913Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:55:11.913Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:55:11.935Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T21:55:48.301Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:55:48.301Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:56:34.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:56:34.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:56:40.970Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:56:40.970Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:59:46.457Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T21:59:46.458Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T21:59:46.470Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T21:59:46.470Z] [LOG] [API] Checking server status...
[2025-10-25T21:59:46.479Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T21:59:46.475Z"
}
[2025-10-25T21:59:46.479Z] [LOG] [API] WebGPU available
[2025-10-25T21:59:46.485Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T21:59:46.485Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T21:59:46.485Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T21:59:46.507Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T21:59:53.013Z] [LOG] [Boot] Boot mode selected: essential
[2025-10-25T21:59:53.659Z] [LOG] [Boot] Boot mode selected: meta
[2025-10-25T22:00:30.290Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:00:30.290Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:00:30.294Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:00:30.294Z] [LOG] [API] Checking server status...
[2025-10-25T22:00:30.302Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:00:30.296Z"
}
[2025-10-25T22:00:30.302Z] [LOG] [API] WebGPU available
[2025-10-25T22:00:30.307Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:00:30.307Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:00:30.307Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:00:30.317Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:00:33.683Z] [INFO] {"timestamp":"2025-10-25T22:00:33.683Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-25T22:00:33.683Z] [INFO] {"timestamp":"2025-10-25T22:00:33.683Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-25T22:00:33.685Z] [INFO] {"timestamp":"2025-10-25T22:00:33.685Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-25T22:00:33.685Z] [INFO] {"timestamp":"2025-10-25T22:00:33.685Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-25T22:00:33.686Z] [INFO] {"timestamp":"2025-10-25T22:00:33.686Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-25T22:00:33.686Z] [INFO] {"timestamp":"2025-10-25T22:00:33.686Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-25T22:00:33.687Z] [INFO] {"timestamp":"2025-10-25T22:00:33.687Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-25T22:00:33.687Z] [INFO] {"timestamp":"2025-10-25T22:00:33.687Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-25T22:00:33.689Z] [INFO] {"timestamp":"2025-10-25T22:00:33.689Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-25T22:00:33.689Z] [INFO] {"timestamp":"2025-10-25T22:00:33.689Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-25T22:00:33.690Z] [INFO] {"timestamp":"2025-10-25T22:00:33.690Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-25T22:00:33.690Z] [INFO] {"timestamp":"2025-10-25T22:00:33.690Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-25T22:00:33.691Z] [INFO] {"timestamp":"2025-10-25T22:00:33.691Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-25T22:00:33.691Z] [INFO] {"timestamp":"2025-10-25T22:00:33.691Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-25T22:00:33.693Z] [INFO] {"timestamp":"2025-10-25T22:00:33.693Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-25T22:00:33.693Z] [INFO] {"timestamp":"2025-10-25T22:00:33.693Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-25T22:00:33.694Z] [INFO] {"timestamp":"2025-10-25T22:00:33.694Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-25T22:00:33.694Z] [INFO] {"timestamp":"2025-10-25T22:00:33.694Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-25T22:00:33.696Z] [INFO] {"timestamp":"2025-10-25T22:00:33.696Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-25T22:00:33.696Z] [INFO] {"timestamp":"2025-10-25T22:00:33.696Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-25T22:00:33.697Z] [INFO] {"timestamp":"2025-10-25T22:00:33.697Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-25T22:00:33.697Z] [INFO] {"timestamp":"2025-10-25T22:00:33.697Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-25T22:00:33.698Z] [INFO] {"timestamp":"2025-10-25T22:00:33.698Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-25T22:00:33.698Z] [INFO] {"timestamp":"2025-10-25T22:00:33.698Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-25T22:00:33.700Z] [INFO] {"timestamp":"2025-10-25T22:00:33.700Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-25T22:00:33.700Z] [INFO] {"timestamp":"2025-10-25T22:00:33.700Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-25T22:00:33.701Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-25T22:00:33.701Z] [ERROR] {"timestamp":"2025-10-25T22:00:33.701Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:00:33.701Z] [WARN] {"timestamp":"2025-10-25T22:00:33.701Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-25T22:00:33.701Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-25T22:00:33.701Z] [ERROR] {"timestamp":"2025-10-25T22:00:33.701Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:00:33.701Z] [WARN] {"timestamp":"2025-10-25T22:00:33.701Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-25T22:00:33.702Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-25T22:00:33.702Z] [ERROR] {"timestamp":"2025-10-25T22:00:33.702Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:00:33.702Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:00:33.702Z] [WARN] {"timestamp":"2025-10-25T22:00:33.702Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-25T22:00:33.703Z] [INFO] {"timestamp":"2025-10-25T22:00:33.703Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-25T22:00:33.703Z] [INFO] {"timestamp":"2025-10-25T22:00:33.703Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-25T22:00:33.704Z] [INFO] {"timestamp":"2025-10-25T22:00:33.704Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-25T22:00:33.704Z] [INFO] {"timestamp":"2025-10-25T22:00:33.704Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-25T22:00:33.707Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-25T22:00:33.707Z] [INFO] {"timestamp":"2025-10-25T22:00:33.707Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-25T22:00:33.707Z] [INFO] {"timestamp":"2025-10-25T22:00:33.707Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-25T22:00:33.708Z] [INFO] {"timestamp":"2025-10-25T22:00:33.708Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-25T22:00:33.709Z] [INFO] {"timestamp":"2025-10-25T22:00:33.708Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-25T22:00:33.710Z] [INFO] {"timestamp":"2025-10-25T22:00:33.710Z","level":"INFO","message":"[DIContainer] Registered module: GitVFS","details":{}}
[2025-10-25T22:00:33.710Z] [INFO] {"timestamp":"2025-10-25T22:00:33.710Z","level":"INFO","message":"[CoreLogic] Registered module: GitVFS from /upgrades/git-vfs.js","details":{}}
[2025-10-25T22:00:33.711Z] [INFO] {"timestamp":"2025-10-25T22:00:33.711Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-25T22:00:33.711Z] [INFO] {"timestamp":"2025-10-25T22:00:33.711Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-25T22:00:33.712Z] [INFO] {"timestamp":"2025-10-25T22:00:33.712Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-25T22:00:33.712Z] [INFO] {"timestamp":"2025-10-25T22:00:33.712Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-25T22:00:33.714Z] [LOG] [CoreLogic] Loaded module from /upgrades/diff-viewer-ui.js: NO_METADATA
[2025-10-25T22:00:33.714Z] [ERROR] {"timestamp":"2025-10-25T22:00:33.714Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/diff-viewer-ui.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:00:33.714Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:00:33.714Z] [WARN] {"timestamp":"2025-10-25T22:00:33.714Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/diff-viewer-ui.js missing metadata. Module:","details":{}}
[2025-10-25T22:00:33.716Z] [INFO] {"timestamp":"2025-10-25T22:00:33.715Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCSwarm","details":{}}
[2025-10-25T22:00:33.716Z] [INFO] {"timestamp":"2025-10-25T22:00:33.716Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCSwarm from /upgrades/webrtc-swarm.js","details":{}}
[2025-10-25T22:00:33.717Z] [INFO] {"timestamp":"2025-10-25T22:00:33.717Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCCoordinator","details":{}}
[2025-10-25T22:00:33.717Z] [INFO] {"timestamp":"2025-10-25T22:00:33.717Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCCoordinator from /upgrades/webrtc-coordinator.js","details":{}}
[2025-10-25T22:00:33.718Z] [INFO] {"timestamp":"2025-10-25T22:00:33.718Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-25T22:00:33.719Z] [INFO] {"timestamp":"2025-10-25T22:00:33.718Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-25T22:00:33.720Z] [INFO] {"timestamp":"2025-10-25T22:00:33.720Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-25T22:00:33.720Z] [INFO] {"timestamp":"2025-10-25T22:00:33.720Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-25T22:00:33.721Z] [INFO] {"timestamp":"2025-10-25T22:00:33.721Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-25T22:00:33.721Z] [INFO] {"timestamp":"2025-10-25T22:00:33.721Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-25T22:00:33.723Z] [INFO] {"timestamp":"2025-10-25T22:00:33.723Z","level":"INFO","message":"[DIContainer] Registered module: PeerReviewConsensus","details":{}}
[2025-10-25T22:00:33.723Z] [INFO] {"timestamp":"2025-10-25T22:00:33.723Z","level":"INFO","message":"[CoreLogic] Registered module: PeerReviewConsensus from /upgrades/peer-review-consensus.js","details":{}}
[2025-10-25T22:00:33.723Z] [INFO] {"timestamp":"2025-10-25T22:00:33.723Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-25T22:00:33.724Z] [INFO] {"timestamp":"2025-10-25T22:00:33.724Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-25T22:00:33.726Z] [INFO] {"timestamp":"2025-10-25T22:00:33.726Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-25T22:00:33.726Z] [INFO] {"timestamp":"2025-10-25T22:00:33.726Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-25T22:00:33.727Z] [INFO] {"timestamp":"2025-10-25T22:00:33.727Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-25T22:00:33.727Z] [WARN] {"timestamp":"2025-10-25T22:00:33.727Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-25T22:00:33.728Z] [INFO] {"timestamp":"2025-10-25T22:00:33.728Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-25T22:00:33.728Z] [INFO] {"timestamp":"2025-10-25T22:00:33.728Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-25T22:00:33.728Z] [WARN] {"timestamp":"2025-10-25T22:00:33.728Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-25T22:00:33.729Z] [INFO] {"timestamp":"2025-10-25T22:00:33.729Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-25T22:00:33.729Z] [WARN] {"timestamp":"2025-10-25T22:00:33.729Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-25T22:00:33.730Z] [INFO] {"timestamp":"2025-10-25T22:00:33.730Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-25T22:00:33.730Z] [INFO] {"timestamp":"2025-10-25T22:00:33.730Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-25T22:00:33.730Z] [INFO] {"timestamp":"2025-10-25T22:00:33.730Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-25T22:00:33.733Z] [INFO] {"timestamp":"2025-10-25T22:00:33.733Z","level":"INFO","message":"[ReflectionStore] Initializing reflection persistence","details":{}}
[2025-10-25T22:00:33.734Z] [INFO] {"timestamp":"2025-10-25T22:00:33.733Z","level":"INFO","message":"[ReflectionStore] Database opened successfully","details":{}}
[2025-10-25T22:00:33.734Z] [INFO] {"timestamp":"2025-10-25T22:00:33.734Z","level":"INFO","message":"[SelfTester] Initialized","details":{}}
[2025-10-25T22:00:33.734Z] [INFO] {"timestamp":"2025-10-25T22:00:33.734Z","level":"INFO","message":"[BrowserAPIs] Initializing web API integration...","details":{}}
[2025-10-25T22:00:33.735Z] [INFO] {"timestamp":"2025-10-25T22:00:33.735Z","level":"INFO","message":"[BrowserAPIs] Capabilities detected:","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-25T22:00:33.735Z] [INFO] {"timestamp":"2025-10-25T22:00:33.735Z","level":"INFO","message":"[EventBus] Emitting event: browser-apis:initialized","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-25T22:00:33.737Z] [INFO] {"timestamp":"2025-10-25T22:00:33.737Z","level":"INFO","message":"[GitVFS] Initialized successfully","details":{}}
[2025-10-25T22:00:33.737Z] [INFO] {"timestamp":"2025-10-25T22:00:33.737Z","level":"INFO","message":"[WebRTCSwarm] Initializing swarm module","details":{}}
[2025-10-25T22:00:33.737Z] [INFO] {"timestamp":"2025-10-25T22:00:33.737Z","level":"INFO","message":"[WebRTCSwarm] Local peer ID: reploid-id_1761429633737_hihcdum15","details":{}}
[2025-10-25T22:00:33.737Z] [INFO] {"timestamp":"2025-10-25T22:00:33.737Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-25T22:00:33.738Z] [INFO] {"timestamp":"2025-10-25T22:00:33.738Z","level":"INFO","message":"[WebRTCSwarm] Swarm initialized","details":{}}
[2025-10-25T22:00:33.738Z] [INFO] {"timestamp":"2025-10-25T22:00:33.738Z","level":"INFO","message":"[SwarmOrch] Initializing swarm orchestrator","details":{}}
[2025-10-25T22:00:33.738Z] [WARN] {"timestamp":"2025-10-25T22:00:33.738Z","level":"WARN","message":"[WebRTCSwarm] Cannot send signaling message: not connected","details":{}}
[2025-10-25T22:00:33.738Z] [INFO] {"timestamp":"2025-10-25T22:00:33.738Z","level":"INFO","message":"[SwarmOrch] Swarm orchestrator initialized","details":{"capabilities":["code-generation","file-management"]}}
[2025-10-25T22:00:33.741Z] [INFO] {"timestamp":"2025-10-25T22:00:33.741Z","level":"INFO","message":"[PyodideRuntime] Initializing Pyodide runtime...","details":{}}
[2025-10-25T22:00:33.741Z] [INFO] {"timestamp":"2025-10-25T22:00:33.741Z","level":"INFO","message":"[PyodideRuntime] Worker created","details":{}}
[2025-10-25T22:00:33.742Z] [ERROR] {"timestamp":"2025-10-25T22:00:33.742Z","level":"ERROR","message":"[WebRTCSwarm] Signaling WebSocket error:","details":{"isTrusted":true}}
[2025-10-25T22:00:33.742Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:00:33.742Z] [WARN] {"timestamp":"2025-10-25T22:00:33.742Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-25T22:00:35.452Z] [INFO] {"timestamp":"2025-10-25T22:00:35.451Z","level":"INFO","message":"[PyodideRuntime] Pyodide initialized","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-25T22:00:35.452Z] [INFO] {"timestamp":"2025-10-25T22:00:35.452Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:ready","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-25T22:00:35.452Z] [INFO] {"timestamp":"2025-10-25T22:00:35.452Z","level":"INFO","message":"[PyodideRuntime] Pyodide runtime ready","details":{}}
[2025-10-25T22:00:35.452Z] [INFO] {"timestamp":"2025-10-25T22:00:35.452Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:initialized","details":{"ready":true}}
[2025-10-25T22:00:35.454Z] [INFO] {"timestamp":"2025-10-25T22:00:35.454Z","level":"INFO","message":"Dashboard UI Manager (Event-Driven) taking control of DOM...","details":{}}
[2025-10-25T22:00:35.467Z] [INFO] {"timestamp":"2025-10-25T22:00:35.467Z","level":"INFO","message":"[UIManager] Initializing modular panel support...","details":{}}
[2025-10-25T22:00:35.467Z] [INFO] {"timestamp":"2025-10-25T22:00:35.467Z","level":"INFO","message":"[UIManager] Modular panel initialization complete","details":{}}
[2025-10-25T22:00:35.467Z] [INFO] {"timestamp":"2025-10-25T22:00:35.467Z","level":"INFO","message":"Dashboard UI Initialized. Listening for events.","details":{}}
[2025-10-25T22:00:35.467Z] [LOG] [CoreLogic] Attempting to initialize DiffViewerUI...
[2025-10-25T22:00:35.467Z] [LOG] [CoreLogic] Resolving DiffViewerUI from container...
[2025-10-25T22:00:35.467Z] [ERROR] [CoreLogic] DiffViewerUI initialization error: Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:591:33)), <anonymous>:61:23)
    at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:591:33), <anonymous>:317:46)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:611:9)
[2025-10-25T22:00:35.467Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:00:35.467Z] [WARN] {"timestamp":"2025-10-25T22:00:35.467Z","level":"WARN","message":"[CoreLogic] DiffViewerUI initialization failed:","details":"[DIContainer] Service not found: DiffViewerUI\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-25T22:00:35.467Z] [INFO] {"timestamp":"2025-10-25T22:00:35.467Z","level":"INFO","message":"[CoreLogic] Agent initialization complete. System is operational.","details":{}}
[2025-10-25T22:00:35.467Z] [INFO] {"timestamp":"2025-10-25T22:00:35.467Z","level":"INFO","message":"[CoreLogic] Creating genesis snapshot of initial boot state...","details":{}}
[2025-10-25T22:00:35.467Z] [WARN] {"timestamp":"2025-10-25T22:00:35.467Z","level":"WARN","message":"[CoreLogic] Failed to create genesis snapshot (non-fatal):","details":"[DIContainer] Service not found: GenesisSnapshot\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-25T22:00:35.467Z] [INFO] {"timestamp":"2025-10-25T22:00:35.467Z","level":"INFO","message":"[CoreLogic] Starting agent with initial goal:","details":"Create tools to make tools to make tools"}
[2025-10-25T22:00:35.467Z] [INFO] {"timestamp":"2025-10-25T22:00:35.467Z","level":"INFO","message":"[UI] updateGoal called with:","details":{"text":"Create tools to make tools to make tools","hasGoalTextRef":true}}
[2025-10-25T22:00:35.467Z] [INFO] {"timestamp":"2025-10-25T22:00:35.467Z","level":"INFO","message":"[UI] Goal text element updated successfully","details":{}}
[2025-10-25T22:00:35.490Z] [INFO] {"timestamp":"2025-10-25T22:00:35.489Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761429635470_06ab3ced8d82e2d6/session.json (SHA: 5f91027)","details":{}}
[2025-10-25T22:00:35.490Z] [INFO] {"timestamp":"2025-10-25T22:00:35.490Z","level":"INFO","message":"[SessionManager] Created new session: session_1761429635470_06ab3ced8d82e2d6","details":{}}
[2025-10-25T22:00:35.497Z] [INFO] {"timestamp":"2025-10-25T22:00:35.497Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.checkpoints/checkpoint_1761429635490.json (SHA: c2c5eda)","details":{}}
[2025-10-25T22:00:35.497Z] [INFO] {"timestamp":"2025-10-25T22:00:35.497Z","level":"INFO","message":"[StateManager] Created checkpoint: checkpoint_1761429635490 - Session session_1761429635470_06ab3ced8d82e2d6 - Turn 0 start","details":{}}
[2025-10-25T22:00:35.497Z] [INFO] {"timestamp":"2025-10-25T22:00:35.497Z","level":"INFO","message":"[SessionManager] Created turn checkpoint: checkpoint_1761429635490","details":{}}
[2025-10-25T22:00:35.504Z] [INFO] {"timestamp":"2025-10-25T22:00:35.504Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761429635470_06ab3ced8d82e2d6/session.json (SHA: e9f6fcd)","details":{}}
[2025-10-25T22:00:35.504Z] [INFO] {"timestamp":"2025-10-25T22:00:35.504Z","level":"INFO","message":"[SessionManager] Created turn 0 for session session_1761429635470_06ab3ced8d82e2d6","details":{}}
[2025-10-25T22:00:35.505Z] [INFO] {"timestamp":"2025-10-25T22:00:35.505Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"","progress":null}}
[2025-10-25T22:00:35.505Z] [INFO] {"timestamp":"2025-10-25T22:00:35.505Z","level":"INFO","message":"[SentinelFSM] State transition: IDLE -> CURATING_CONTEXT","details":{}}
[2025-10-25T22:00:35.505Z] [INFO] {"timestamp":"2025-10-25T22:00:35.505Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"IDLE","newState":"CURATING_CONTEXT","context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761429635470_06ab3ced8d82e2d6","turn":{"turn":0,"context_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.context.md","proposal_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429635490","createdAt":"2025-10-25T22:00:35.497Z"},"startTime":1761429635504,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:00:35.505Z] [INFO] {"timestamp":"2025-10-25T22:00:35.505Z","level":"INFO","message":"[SentinelFSM] Executing state: CURATING_CONTEXT","details":{}}
[2025-10-25T22:00:35.505Z] [INFO] {"timestamp":"2025-10-25T22:00:35.505Z","level":"INFO","message":"[EventBus] Emitting event: agent:curating","details":{"goal":"Create tools to make tools to make tools"}}
[2025-10-25T22:00:35.505Z] [INFO] {"timestamp":"2025-10-25T22:00:35.505Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Analyzing project files...","progress":null}}
[2025-10-25T22:00:35.505Z] [WARN] {"timestamp":"2025-10-25T22:00:35.505Z","level":"WARN","message":"[SentinelTools] No files available for curation","details":{}}
[2025-10-25T22:00:35.505Z] [INFO] {"timestamp":"2025-10-25T22:00:35.505Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Found 0 relevant files","progress":50}}
[2025-10-25T22:00:35.506Z] [ERROR] {"timestamp":"2025-10-25T22:00:35.506Z","level":"ERROR","message":"[SentinelFSM] Error in state CURATING_CONTEXT:","details":{}}
[2025-10-25T22:00:35.506Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:00:35.506Z] [INFO] {"timestamp":"2025-10-25T22:00:35.506Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"ERROR","detail":"","progress":null}}
[2025-10-25T22:00:35.506Z] [INFO] {"timestamp":"2025-10-25T22:00:35.506Z","level":"INFO","message":"[SentinelFSM] State transition: CURATING_CONTEXT -> ERROR","details":{}}
[2025-10-25T22:00:35.506Z] [INFO] {"timestamp":"2025-10-25T22:00:35.506Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"CURATING_CONTEXT","newState":"ERROR","context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761429635470_06ab3ced8d82e2d6","turn":{"turn":0,"context_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.context.md","proposal_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429635490","createdAt":"2025-10-25T22:00:35.497Z"},"startTime":1761429635504,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:00:35.506Z] [INFO] {"timestamp":"2025-10-25T22:00:35.506Z","level":"INFO","message":"[SentinelFSM] Executing state: ERROR","details":{}}
[2025-10-25T22:00:35.506Z] [INFO] {"timestamp":"2025-10-25T22:00:35.506Z","level":"INFO","message":"[EventBus] Emitting event: agent:error","details":{"state":{"from":"CURATING_CONTEXT","to":"ERROR","timestamp":1761429635506,"context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761429635470_06ab3ced8d82e2d6","turn":{"turn":0,"context_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.context.md","proposal_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429635490","createdAt":"2025-10-25T22:00:35.497Z"},"startTime":1761429635504,"iterations":0,"maxIterations":10}},"context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761429635470_06ab3ced8d82e2d6","turn":{"turn":0,"context_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.context.md","proposal_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429635490","createdAt":"2025-10-25T22:00:35.497Z"},"startTime":1761429635504,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:00:35.506Z] [ERROR] {"timestamp":"2025-10-25T22:00:35.506Z","level":"ERROR","message":"[SentinelFSM] Error state reached","details":{"previous_state":{"from":"IDLE","to":"CURATING_CONTEXT","timestamp":1761429635505,"context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761429635470_06ab3ced8d82e2d6","turn":{"turn":0,"context_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.context.md","proposal_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429635490","createdAt":"2025-10-25T22:00:35.497Z"},"startTime":1761429635504,"iterations":0,"maxIterations":10}},"context":{"goal":"Create tools to make tools to make tools","sessionId":"session_1761429635470_06ab3ced8d82e2d6","turn":{"turn":0,"context_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.context.md","proposal_path":"/sessions/session_1761429635470_06ab3ced8d82e2d6/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429635490","createdAt":"2025-10-25T22:00:35.497Z"},"startTime":1761429635504,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:00:35.506Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:00:35.506Z] [INFO] {"timestamp":"2025-10-25T22:00:35.506Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"IDLE","detail":"","progress":null}}
[2025-10-25T22:00:35.506Z] [INFO] {"timestamp":"2025-10-25T22:00:35.506Z","level":"INFO","message":"[SentinelFSM] State transition: ERROR -> IDLE","details":{}}
[2025-10-25T22:00:35.506Z] [INFO] {"timestamp":"2025-10-25T22:00:35.506Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"ERROR","newState":"IDLE","context":null}}
[2025-10-25T22:00:35.506Z] [INFO] {"timestamp":"2025-10-25T22:00:35.506Z","level":"INFO","message":"[CoreLogic] Agent cycle started successfully","details":{}}
[2025-10-25T22:00:35.506Z] [LOG] [Boot] Agent system awakened successfully
[2025-10-25T22:00:38.742Z] [INFO] {"timestamp":"2025-10-25T22:00:38.742Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-25T22:00:38.743Z] [INFO] {"timestamp":"2025-10-25T22:00:38.743Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-25T22:00:38.747Z] [INFO] {"timestamp":"2025-10-25T22:00:38.747Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-25T22:00:38.748Z] [INFO] {"timestamp":"2025-10-25T22:00:38.748Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-25T22:01:35.581Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:01:35.581Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:01:35.583Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:01:35.584Z] [LOG] [API] Checking server status...
[2025-10-25T22:01:35.591Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:01:35.587Z"
}
[2025-10-25T22:01:35.592Z] [LOG] [API] WebGPU available
[2025-10-25T22:01:35.595Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:01:35.595Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:01:35.595Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:01:35.615Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:02:43.673Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:02:43.673Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:02:43.677Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:02:43.677Z] [LOG] [API] Checking server status...
[2025-10-25T22:02:43.686Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:02:43.679Z"
}
[2025-10-25T22:02:43.686Z] [LOG] [API] WebGPU available
[2025-10-25T22:02:43.692Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:02:43.692Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:02:43.693Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:02:43.708Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:02:45.594Z] [INFO] {"timestamp":"2025-10-25T22:02:45.594Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-25T22:02:45.594Z] [INFO] {"timestamp":"2025-10-25T22:02:45.594Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-25T22:02:45.595Z] [INFO] {"timestamp":"2025-10-25T22:02:45.595Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-25T22:02:45.595Z] [INFO] {"timestamp":"2025-10-25T22:02:45.595Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-25T22:02:45.597Z] [INFO] {"timestamp":"2025-10-25T22:02:45.597Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-25T22:02:45.597Z] [INFO] {"timestamp":"2025-10-25T22:02:45.597Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-25T22:02:45.598Z] [INFO] {"timestamp":"2025-10-25T22:02:45.598Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-25T22:02:45.598Z] [INFO] {"timestamp":"2025-10-25T22:02:45.598Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-25T22:02:45.600Z] [INFO] {"timestamp":"2025-10-25T22:02:45.600Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-25T22:02:45.600Z] [INFO] {"timestamp":"2025-10-25T22:02:45.600Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-25T22:02:45.601Z] [INFO] {"timestamp":"2025-10-25T22:02:45.601Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-25T22:02:45.601Z] [INFO] {"timestamp":"2025-10-25T22:02:45.601Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-25T22:02:45.603Z] [INFO] {"timestamp":"2025-10-25T22:02:45.603Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-25T22:02:45.603Z] [INFO] {"timestamp":"2025-10-25T22:02:45.603Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-25T22:02:45.604Z] [INFO] {"timestamp":"2025-10-25T22:02:45.604Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-25T22:02:45.604Z] [INFO] {"timestamp":"2025-10-25T22:02:45.604Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-25T22:02:45.605Z] [INFO] {"timestamp":"2025-10-25T22:02:45.605Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-25T22:02:45.605Z] [INFO] {"timestamp":"2025-10-25T22:02:45.605Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-25T22:02:45.607Z] [INFO] {"timestamp":"2025-10-25T22:02:45.607Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-25T22:02:45.607Z] [INFO] {"timestamp":"2025-10-25T22:02:45.607Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-25T22:02:45.608Z] [INFO] {"timestamp":"2025-10-25T22:02:45.608Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-25T22:02:45.608Z] [INFO] {"timestamp":"2025-10-25T22:02:45.608Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-25T22:02:45.609Z] [INFO] {"timestamp":"2025-10-25T22:02:45.609Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-25T22:02:45.609Z] [INFO] {"timestamp":"2025-10-25T22:02:45.609Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-25T22:02:45.611Z] [INFO] {"timestamp":"2025-10-25T22:02:45.611Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-25T22:02:45.611Z] [INFO] {"timestamp":"2025-10-25T22:02:45.611Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-25T22:02:45.612Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-25T22:02:45.612Z] [ERROR] {"timestamp":"2025-10-25T22:02:45.612Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:02:45.612Z] [WARN] {"timestamp":"2025-10-25T22:02:45.612Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-25T22:02:45.612Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-25T22:02:45.613Z] [ERROR] {"timestamp":"2025-10-25T22:02:45.612Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:02:45.613Z] [WARN] {"timestamp":"2025-10-25T22:02:45.613Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-25T22:02:45.613Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-25T22:02:45.613Z] [ERROR] {"timestamp":"2025-10-25T22:02:45.613Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:02:45.613Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:02:45.613Z] [WARN] {"timestamp":"2025-10-25T22:02:45.613Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-25T22:02:45.614Z] [INFO] {"timestamp":"2025-10-25T22:02:45.614Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-25T22:02:45.614Z] [INFO] {"timestamp":"2025-10-25T22:02:45.614Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-25T22:02:45.615Z] [INFO] {"timestamp":"2025-10-25T22:02:45.615Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-25T22:02:45.615Z] [INFO] {"timestamp":"2025-10-25T22:02:45.615Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-25T22:02:45.618Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-25T22:02:45.618Z] [INFO] {"timestamp":"2025-10-25T22:02:45.618Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-25T22:02:45.618Z] [INFO] {"timestamp":"2025-10-25T22:02:45.618Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-25T22:02:45.619Z] [INFO] {"timestamp":"2025-10-25T22:02:45.619Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-25T22:02:45.619Z] [INFO] {"timestamp":"2025-10-25T22:02:45.619Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-25T22:02:45.621Z] [INFO] {"timestamp":"2025-10-25T22:02:45.621Z","level":"INFO","message":"[DIContainer] Registered module: GitVFS","details":{}}
[2025-10-25T22:02:45.621Z] [INFO] {"timestamp":"2025-10-25T22:02:45.621Z","level":"INFO","message":"[CoreLogic] Registered module: GitVFS from /upgrades/git-vfs.js","details":{}}
[2025-10-25T22:02:45.622Z] [INFO] {"timestamp":"2025-10-25T22:02:45.622Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-25T22:02:45.623Z] [INFO] {"timestamp":"2025-10-25T22:02:45.622Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-25T22:02:45.624Z] [INFO] {"timestamp":"2025-10-25T22:02:45.624Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-25T22:02:45.624Z] [INFO] {"timestamp":"2025-10-25T22:02:45.624Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-25T22:02:45.625Z] [LOG] [CoreLogic] Loaded module from /upgrades/diff-viewer-ui.js: NO_METADATA
[2025-10-25T22:02:45.625Z] [ERROR] {"timestamp":"2025-10-25T22:02:45.625Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/diff-viewer-ui.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:02:45.625Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:02:45.625Z] [WARN] {"timestamp":"2025-10-25T22:02:45.625Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/diff-viewer-ui.js missing metadata. Module:","details":{}}
[2025-10-25T22:02:45.627Z] [INFO] {"timestamp":"2025-10-25T22:02:45.627Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCSwarm","details":{}}
[2025-10-25T22:02:45.627Z] [INFO] {"timestamp":"2025-10-25T22:02:45.627Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCSwarm from /upgrades/webrtc-swarm.js","details":{}}
[2025-10-25T22:02:45.629Z] [INFO] {"timestamp":"2025-10-25T22:02:45.629Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCCoordinator","details":{}}
[2025-10-25T22:02:45.629Z] [INFO] {"timestamp":"2025-10-25T22:02:45.629Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCCoordinator from /upgrades/webrtc-coordinator.js","details":{}}
[2025-10-25T22:02:45.631Z] [INFO] {"timestamp":"2025-10-25T22:02:45.630Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-25T22:02:45.631Z] [INFO] {"timestamp":"2025-10-25T22:02:45.631Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-25T22:02:45.632Z] [INFO] {"timestamp":"2025-10-25T22:02:45.632Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-25T22:02:45.632Z] [INFO] {"timestamp":"2025-10-25T22:02:45.632Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-25T22:02:45.633Z] [INFO] {"timestamp":"2025-10-25T22:02:45.633Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-25T22:02:45.633Z] [INFO] {"timestamp":"2025-10-25T22:02:45.633Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-25T22:02:45.635Z] [INFO] {"timestamp":"2025-10-25T22:02:45.634Z","level":"INFO","message":"[DIContainer] Registered module: PeerReviewConsensus","details":{}}
[2025-10-25T22:02:45.635Z] [INFO] {"timestamp":"2025-10-25T22:02:45.635Z","level":"INFO","message":"[CoreLogic] Registered module: PeerReviewConsensus from /upgrades/peer-review-consensus.js","details":{}}
[2025-10-25T22:02:45.635Z] [INFO] {"timestamp":"2025-10-25T22:02:45.635Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-25T22:02:45.636Z] [INFO] {"timestamp":"2025-10-25T22:02:45.636Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-25T22:02:45.638Z] [INFO] {"timestamp":"2025-10-25T22:02:45.638Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-25T22:02:45.639Z] [INFO] {"timestamp":"2025-10-25T22:02:45.639Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-25T22:02:45.639Z] [INFO] {"timestamp":"2025-10-25T22:02:45.639Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-25T22:02:45.639Z] [WARN] {"timestamp":"2025-10-25T22:02:45.639Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-25T22:02:45.640Z] [INFO] {"timestamp":"2025-10-25T22:02:45.640Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-25T22:02:45.640Z] [INFO] {"timestamp":"2025-10-25T22:02:45.640Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-25T22:02:45.640Z] [WARN] {"timestamp":"2025-10-25T22:02:45.640Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-25T22:02:45.641Z] [INFO] {"timestamp":"2025-10-25T22:02:45.641Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-25T22:02:45.641Z] [WARN] {"timestamp":"2025-10-25T22:02:45.641Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-25T22:02:45.642Z] [INFO] {"timestamp":"2025-10-25T22:02:45.642Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-25T22:02:45.642Z] [INFO] {"timestamp":"2025-10-25T22:02:45.642Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-25T22:02:45.642Z] [INFO] {"timestamp":"2025-10-25T22:02:45.642Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-25T22:02:45.645Z] [INFO] {"timestamp":"2025-10-25T22:02:45.645Z","level":"INFO","message":"[ReflectionStore] Initializing reflection persistence","details":{}}
[2025-10-25T22:02:45.646Z] [INFO] {"timestamp":"2025-10-25T22:02:45.646Z","level":"INFO","message":"[ReflectionStore] Database opened successfully","details":{}}
[2025-10-25T22:02:45.646Z] [INFO] {"timestamp":"2025-10-25T22:02:45.646Z","level":"INFO","message":"[SelfTester] Initialized","details":{}}
[2025-10-25T22:02:45.647Z] [INFO] {"timestamp":"2025-10-25T22:02:45.647Z","level":"INFO","message":"[BrowserAPIs] Initializing web API integration...","details":{}}
[2025-10-25T22:02:45.647Z] [INFO] {"timestamp":"2025-10-25T22:02:45.647Z","level":"INFO","message":"[BrowserAPIs] Capabilities detected:","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-25T22:02:45.647Z] [INFO] {"timestamp":"2025-10-25T22:02:45.647Z","level":"INFO","message":"[EventBus] Emitting event: browser-apis:initialized","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-25T22:02:45.649Z] [INFO] {"timestamp":"2025-10-25T22:02:45.649Z","level":"INFO","message":"[GitVFS] Initialized successfully","details":{}}
[2025-10-25T22:02:45.649Z] [INFO] {"timestamp":"2025-10-25T22:02:45.649Z","level":"INFO","message":"[WebRTCSwarm] Initializing swarm module","details":{}}
[2025-10-25T22:02:45.649Z] [INFO] {"timestamp":"2025-10-25T22:02:45.649Z","level":"INFO","message":"[WebRTCSwarm] Local peer ID: reploid-id_1761429765649_hdi09qevc","details":{}}
[2025-10-25T22:02:45.650Z] [INFO] {"timestamp":"2025-10-25T22:02:45.650Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-25T22:02:45.650Z] [INFO] {"timestamp":"2025-10-25T22:02:45.650Z","level":"INFO","message":"[WebRTCSwarm] Swarm initialized","details":{}}
[2025-10-25T22:02:45.650Z] [INFO] {"timestamp":"2025-10-25T22:02:45.650Z","level":"INFO","message":"[SwarmOrch] Initializing swarm orchestrator","details":{}}
[2025-10-25T22:02:45.650Z] [WARN] {"timestamp":"2025-10-25T22:02:45.650Z","level":"WARN","message":"[WebRTCSwarm] Cannot send signaling message: not connected","details":{}}
[2025-10-25T22:02:45.651Z] [INFO] {"timestamp":"2025-10-25T22:02:45.651Z","level":"INFO","message":"[SwarmOrch] Swarm orchestrator initialized","details":{"capabilities":["code-generation","file-management"]}}
[2025-10-25T22:02:45.653Z] [INFO] {"timestamp":"2025-10-25T22:02:45.653Z","level":"INFO","message":"[PyodideRuntime] Initializing Pyodide runtime...","details":{}}
[2025-10-25T22:02:45.653Z] [INFO] {"timestamp":"2025-10-25T22:02:45.653Z","level":"INFO","message":"[PyodideRuntime] Worker created","details":{}}
[2025-10-25T22:02:45.653Z] [ERROR] {"timestamp":"2025-10-25T22:02:45.653Z","level":"ERROR","message":"[WebRTCSwarm] Signaling WebSocket error:","details":{"isTrusted":true}}
[2025-10-25T22:02:45.653Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:02:45.653Z] [WARN] {"timestamp":"2025-10-25T22:02:45.653Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-25T22:02:47.299Z] [INFO] {"timestamp":"2025-10-25T22:02:47.299Z","level":"INFO","message":"[PyodideRuntime] Pyodide initialized","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-25T22:02:47.299Z] [INFO] {"timestamp":"2025-10-25T22:02:47.299Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:ready","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-25T22:02:47.299Z] [INFO] {"timestamp":"2025-10-25T22:02:47.299Z","level":"INFO","message":"[PyodideRuntime] Pyodide runtime ready","details":{}}
[2025-10-25T22:02:47.299Z] [INFO] {"timestamp":"2025-10-25T22:02:47.299Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:initialized","details":{"ready":true}}
[2025-10-25T22:02:47.301Z] [INFO] {"timestamp":"2025-10-25T22:02:47.301Z","level":"INFO","message":"Dashboard UI Manager (Event-Driven) taking control of DOM...","details":{}}
[2025-10-25T22:02:47.311Z] [INFO] {"timestamp":"2025-10-25T22:02:47.311Z","level":"INFO","message":"[UIManager] Initializing modular panel support...","details":{}}
[2025-10-25T22:02:47.311Z] [INFO] {"timestamp":"2025-10-25T22:02:47.311Z","level":"INFO","message":"[UIManager] Modular panel initialization complete","details":{}}
[2025-10-25T22:02:47.311Z] [INFO] {"timestamp":"2025-10-25T22:02:47.311Z","level":"INFO","message":"Dashboard UI Initialized. Listening for events.","details":{}}
[2025-10-25T22:02:47.312Z] [LOG] [CoreLogic] Attempting to initialize DiffViewerUI...
[2025-10-25T22:02:47.312Z] [LOG] [CoreLogic] Resolving DiffViewerUI from container...
[2025-10-25T22:02:47.312Z] [ERROR] [CoreLogic] DiffViewerUI initialization error: Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:681:33)), <anonymous>:61:23)
    at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:681:33), <anonymous>:317:46)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:701:9)
[2025-10-25T22:02:47.312Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:02:47.312Z] [WARN] {"timestamp":"2025-10-25T22:02:47.312Z","level":"WARN","message":"[CoreLogic] DiffViewerUI initialization failed:","details":"[DIContainer] Service not found: DiffViewerUI\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-25T22:02:47.312Z] [INFO] {"timestamp":"2025-10-25T22:02:47.312Z","level":"INFO","message":"[CoreLogic] Agent initialization complete. System is operational.","details":{}}
[2025-10-25T22:02:47.312Z] [INFO] {"timestamp":"2025-10-25T22:02:47.312Z","level":"INFO","message":"[CoreLogic] Creating genesis snapshot of initial boot state...","details":{}}
[2025-10-25T22:02:47.312Z] [WARN] {"timestamp":"2025-10-25T22:02:47.312Z","level":"WARN","message":"[CoreLogic] Failed to create genesis snapshot (non-fatal):","details":"[DIContainer] Service not found: GenesisSnapshot\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-25T22:02:47.312Z] [INFO] {"timestamp":"2025-10-25T22:02:47.312Z","level":"INFO","message":"[CoreLogic] Starting agent with initial goal:","details":"Build a self-modifying code generation system"}
[2025-10-25T22:02:47.312Z] [INFO] {"timestamp":"2025-10-25T22:02:47.312Z","level":"INFO","message":"[UI] updateGoal called with:","details":{"text":"Build a self-modifying code generation system","hasGoalTextRef":true}}
[2025-10-25T22:02:47.312Z] [INFO] {"timestamp":"2025-10-25T22:02:47.312Z","level":"INFO","message":"[UI] Goal text element updated successfully","details":{}}
[2025-10-25T22:02:47.334Z] [INFO] {"timestamp":"2025-10-25T22:02:47.333Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761429767314_a76883dd9d279d5e/session.json (SHA: 58a1be5)","details":{}}
[2025-10-25T22:02:47.334Z] [INFO] {"timestamp":"2025-10-25T22:02:47.334Z","level":"INFO","message":"[SessionManager] Created new session: session_1761429767314_a76883dd9d279d5e","details":{}}
[2025-10-25T22:02:47.341Z] [INFO] {"timestamp":"2025-10-25T22:02:47.341Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.checkpoints/checkpoint_1761429767334.json (SHA: a26d781)","details":{}}
[2025-10-25T22:02:47.341Z] [INFO] {"timestamp":"2025-10-25T22:02:47.341Z","level":"INFO","message":"[StateManager] Created checkpoint: checkpoint_1761429767334 - Session session_1761429767314_a76883dd9d279d5e - Turn 0 start","details":{}}
[2025-10-25T22:02:47.341Z] [INFO] {"timestamp":"2025-10-25T22:02:47.341Z","level":"INFO","message":"[SessionManager] Created turn checkpoint: checkpoint_1761429767334","details":{}}
[2025-10-25T22:02:47.348Z] [INFO] {"timestamp":"2025-10-25T22:02:47.348Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761429767314_a76883dd9d279d5e/session.json (SHA: 97aba5a)","details":{}}
[2025-10-25T22:02:47.348Z] [INFO] {"timestamp":"2025-10-25T22:02:47.348Z","level":"INFO","message":"[SessionManager] Created turn 0 for session session_1761429767314_a76883dd9d279d5e","details":{}}
[2025-10-25T22:02:47.348Z] [INFO] {"timestamp":"2025-10-25T22:02:47.348Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"","progress":null}}
[2025-10-25T22:02:47.349Z] [INFO] {"timestamp":"2025-10-25T22:02:47.349Z","level":"INFO","message":"[SentinelFSM] State transition: IDLE -> CURATING_CONTEXT","details":{}}
[2025-10-25T22:02:47.349Z] [INFO] {"timestamp":"2025-10-25T22:02:47.349Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"IDLE","newState":"CURATING_CONTEXT","context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761429767314_a76883dd9d279d5e","turn":{"turn":0,"context_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.context.md","proposal_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429767334","createdAt":"2025-10-25T22:02:47.341Z"},"startTime":1761429767348,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:02:47.349Z] [INFO] {"timestamp":"2025-10-25T22:02:47.349Z","level":"INFO","message":"[SentinelFSM] Executing state: CURATING_CONTEXT","details":{}}
[2025-10-25T22:02:47.349Z] [INFO] {"timestamp":"2025-10-25T22:02:47.349Z","level":"INFO","message":"[EventBus] Emitting event: agent:curating","details":{"goal":"Build a self-modifying code generation system"}}
[2025-10-25T22:02:47.349Z] [INFO] {"timestamp":"2025-10-25T22:02:47.349Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Analyzing project files...","progress":null}}
[2025-10-25T22:02:47.349Z] [WARN] {"timestamp":"2025-10-25T22:02:47.349Z","level":"WARN","message":"[SentinelTools] No files available for curation","details":{}}
[2025-10-25T22:02:47.349Z] [INFO] {"timestamp":"2025-10-25T22:02:47.349Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Found 0 relevant files","progress":50}}
[2025-10-25T22:02:47.350Z] [ERROR] {"timestamp":"2025-10-25T22:02:47.349Z","level":"ERROR","message":"[SentinelFSM] Error in state CURATING_CONTEXT:","details":{}}
[2025-10-25T22:02:47.350Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:02:47.350Z] [INFO] {"timestamp":"2025-10-25T22:02:47.350Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"ERROR","detail":"","progress":null}}
[2025-10-25T22:02:47.350Z] [INFO] {"timestamp":"2025-10-25T22:02:47.350Z","level":"INFO","message":"[SentinelFSM] State transition: CURATING_CONTEXT -> ERROR","details":{}}
[2025-10-25T22:02:47.350Z] [INFO] {"timestamp":"2025-10-25T22:02:47.350Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"CURATING_CONTEXT","newState":"ERROR","context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761429767314_a76883dd9d279d5e","turn":{"turn":0,"context_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.context.md","proposal_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429767334","createdAt":"2025-10-25T22:02:47.341Z"},"startTime":1761429767348,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:02:47.350Z] [INFO] {"timestamp":"2025-10-25T22:02:47.350Z","level":"INFO","message":"[SentinelFSM] Executing state: ERROR","details":{}}
[2025-10-25T22:02:47.350Z] [INFO] {"timestamp":"2025-10-25T22:02:47.350Z","level":"INFO","message":"[EventBus] Emitting event: agent:error","details":{"state":{"from":"CURATING_CONTEXT","to":"ERROR","timestamp":1761429767350,"context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761429767314_a76883dd9d279d5e","turn":{"turn":0,"context_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.context.md","proposal_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429767334","createdAt":"2025-10-25T22:02:47.341Z"},"startTime":1761429767348,"iterations":0,"maxIterations":10}},"context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761429767314_a76883dd9d279d5e","turn":{"turn":0,"context_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.context.md","proposal_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429767334","createdAt":"2025-10-25T22:02:47.341Z"},"startTime":1761429767348,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:02:47.350Z] [ERROR] {"timestamp":"2025-10-25T22:02:47.350Z","level":"ERROR","message":"[SentinelFSM] Error state reached","details":{"previous_state":{"from":"IDLE","to":"CURATING_CONTEXT","timestamp":1761429767348,"context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761429767314_a76883dd9d279d5e","turn":{"turn":0,"context_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.context.md","proposal_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429767334","createdAt":"2025-10-25T22:02:47.341Z"},"startTime":1761429767348,"iterations":0,"maxIterations":10}},"context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761429767314_a76883dd9d279d5e","turn":{"turn":0,"context_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.context.md","proposal_path":"/sessions/session_1761429767314_a76883dd9d279d5e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761429767334","createdAt":"2025-10-25T22:02:47.341Z"},"startTime":1761429767348,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:02:47.350Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:02:47.350Z] [INFO] {"timestamp":"2025-10-25T22:02:47.350Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"IDLE","detail":"","progress":null}}
[2025-10-25T22:02:47.350Z] [INFO] {"timestamp":"2025-10-25T22:02:47.350Z","level":"INFO","message":"[SentinelFSM] State transition: ERROR -> IDLE","details":{}}
[2025-10-25T22:02:47.350Z] [INFO] {"timestamp":"2025-10-25T22:02:47.350Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"ERROR","newState":"IDLE","context":null}}
[2025-10-25T22:02:47.350Z] [INFO] {"timestamp":"2025-10-25T22:02:47.350Z","level":"INFO","message":"[CoreLogic] Agent cycle started successfully","details":{}}
[2025-10-25T22:02:47.350Z] [LOG] [Boot] Agent system awakened successfully
[2025-10-25T22:02:50.654Z] [INFO] {"timestamp":"2025-10-25T22:02:50.654Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-25T22:02:50.654Z] [INFO] {"timestamp":"2025-10-25T22:02:50.654Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-25T22:02:50.657Z] [INFO] {"timestamp":"2025-10-25T22:02:50.657Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-25T22:02:50.659Z] [INFO] {"timestamp":"2025-10-25T22:02:50.659Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-25T22:05:16.577Z] [WARN] {"timestamp":"2025-10-25T22:05:16.577Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-25T22:05:21.577Z] [INFO] {"timestamp":"2025-10-25T22:05:21.577Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-25T22:05:21.577Z] [INFO] {"timestamp":"2025-10-25T22:05:21.577Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-25T22:05:21.582Z] [INFO] {"timestamp":"2025-10-25T22:05:21.581Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-25T22:05:21.583Z] [INFO] {"timestamp":"2025-10-25T22:05:21.583Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-25T22:20:54.830Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:20:54.830Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:20:54.870Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:20:54.870Z] [LOG] [API] Checking server status...
[2025-10-25T22:20:54.883Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:20:54.872Z"
}
[2025-10-25T22:20:54.883Z] [LOG] [API] WebGPU available
[2025-10-25T22:20:54.896Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:20:54.896Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:20:54.896Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:20:54.920Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:21:03.956Z] [INFO] {"timestamp":"2025-10-25T22:21:03.956Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-25T22:21:03.956Z] [INFO] {"timestamp":"2025-10-25T22:21:03.956Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-25T22:21:03.957Z] [INFO] {"timestamp":"2025-10-25T22:21:03.957Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-25T22:21:03.957Z] [INFO] {"timestamp":"2025-10-25T22:21:03.957Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-25T22:21:03.959Z] [INFO] {"timestamp":"2025-10-25T22:21:03.959Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-25T22:21:03.959Z] [INFO] {"timestamp":"2025-10-25T22:21:03.959Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-25T22:21:03.960Z] [INFO] {"timestamp":"2025-10-25T22:21:03.960Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-25T22:21:03.960Z] [INFO] {"timestamp":"2025-10-25T22:21:03.960Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-25T22:21:03.961Z] [INFO] {"timestamp":"2025-10-25T22:21:03.961Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-25T22:21:03.961Z] [INFO] {"timestamp":"2025-10-25T22:21:03.961Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-25T22:21:03.963Z] [INFO] {"timestamp":"2025-10-25T22:21:03.963Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-25T22:21:03.963Z] [INFO] {"timestamp":"2025-10-25T22:21:03.963Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-25T22:21:03.964Z] [INFO] {"timestamp":"2025-10-25T22:21:03.964Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-25T22:21:03.964Z] [INFO] {"timestamp":"2025-10-25T22:21:03.964Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-25T22:21:03.965Z] [INFO] {"timestamp":"2025-10-25T22:21:03.965Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-25T22:21:03.965Z] [INFO] {"timestamp":"2025-10-25T22:21:03.965Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-25T22:21:03.967Z] [INFO] {"timestamp":"2025-10-25T22:21:03.967Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-25T22:21:03.967Z] [INFO] {"timestamp":"2025-10-25T22:21:03.967Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-25T22:21:03.968Z] [INFO] {"timestamp":"2025-10-25T22:21:03.968Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-25T22:21:03.968Z] [INFO] {"timestamp":"2025-10-25T22:21:03.968Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-25T22:21:03.969Z] [INFO] {"timestamp":"2025-10-25T22:21:03.969Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-25T22:21:03.969Z] [INFO] {"timestamp":"2025-10-25T22:21:03.969Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-25T22:21:03.970Z] [INFO] {"timestamp":"2025-10-25T22:21:03.970Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-25T22:21:03.970Z] [INFO] {"timestamp":"2025-10-25T22:21:03.970Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-25T22:21:03.971Z] [INFO] {"timestamp":"2025-10-25T22:21:03.971Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-25T22:21:03.971Z] [INFO] {"timestamp":"2025-10-25T22:21:03.971Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-25T22:21:03.972Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-25T22:21:03.972Z] [ERROR] {"timestamp":"2025-10-25T22:21:03.972Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:21:03.972Z] [WARN] {"timestamp":"2025-10-25T22:21:03.972Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-25T22:21:03.973Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-25T22:21:03.973Z] [ERROR] {"timestamp":"2025-10-25T22:21:03.973Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:21:03.973Z] [WARN] {"timestamp":"2025-10-25T22:21:03.973Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-25T22:21:03.973Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-25T22:21:03.973Z] [ERROR] {"timestamp":"2025-10-25T22:21:03.973Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:21:03.973Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:21:03.973Z] [WARN] {"timestamp":"2025-10-25T22:21:03.973Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-25T22:21:03.975Z] [INFO] {"timestamp":"2025-10-25T22:21:03.975Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-25T22:21:03.975Z] [INFO] {"timestamp":"2025-10-25T22:21:03.975Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-25T22:21:03.976Z] [INFO] {"timestamp":"2025-10-25T22:21:03.976Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-25T22:21:03.976Z] [INFO] {"timestamp":"2025-10-25T22:21:03.976Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-25T22:21:03.978Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-25T22:21:03.978Z] [INFO] {"timestamp":"2025-10-25T22:21:03.978Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-25T22:21:03.978Z] [INFO] {"timestamp":"2025-10-25T22:21:03.978Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-25T22:21:03.979Z] [INFO] {"timestamp":"2025-10-25T22:21:03.979Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-25T22:21:03.980Z] [INFO] {"timestamp":"2025-10-25T22:21:03.979Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-25T22:21:03.981Z] [INFO] {"timestamp":"2025-10-25T22:21:03.981Z","level":"INFO","message":"[DIContainer] Registered module: GitVFS","details":{}}
[2025-10-25T22:21:03.981Z] [INFO] {"timestamp":"2025-10-25T22:21:03.981Z","level":"INFO","message":"[CoreLogic] Registered module: GitVFS from /upgrades/git-vfs.js","details":{}}
[2025-10-25T22:21:03.982Z] [INFO] {"timestamp":"2025-10-25T22:21:03.982Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-25T22:21:03.982Z] [INFO] {"timestamp":"2025-10-25T22:21:03.982Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-25T22:21:03.984Z] [INFO] {"timestamp":"2025-10-25T22:21:03.984Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-25T22:21:03.984Z] [INFO] {"timestamp":"2025-10-25T22:21:03.984Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-25T22:21:03.985Z] [LOG] [CoreLogic] Loaded module from /upgrades/diff-viewer-ui.js: NO_METADATA
[2025-10-25T22:21:03.985Z] [ERROR] {"timestamp":"2025-10-25T22:21:03.985Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/diff-viewer-ui.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-25T22:21:03.985Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:21:03.985Z] [WARN] {"timestamp":"2025-10-25T22:21:03.985Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/diff-viewer-ui.js missing metadata. Module:","details":{}}
[2025-10-25T22:21:03.987Z] [INFO] {"timestamp":"2025-10-25T22:21:03.987Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCSwarm","details":{}}
[2025-10-25T22:21:03.987Z] [INFO] {"timestamp":"2025-10-25T22:21:03.987Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCSwarm from /upgrades/webrtc-swarm.js","details":{}}
[2025-10-25T22:21:03.988Z] [INFO] {"timestamp":"2025-10-25T22:21:03.988Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCCoordinator","details":{}}
[2025-10-25T22:21:03.988Z] [INFO] {"timestamp":"2025-10-25T22:21:03.988Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCCoordinator from /upgrades/webrtc-coordinator.js","details":{}}
[2025-10-25T22:21:03.990Z] [INFO] {"timestamp":"2025-10-25T22:21:03.990Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-25T22:21:03.990Z] [INFO] {"timestamp":"2025-10-25T22:21:03.990Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-25T22:21:03.991Z] [INFO] {"timestamp":"2025-10-25T22:21:03.991Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-25T22:21:03.991Z] [INFO] {"timestamp":"2025-10-25T22:21:03.991Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-25T22:21:03.993Z] [INFO] {"timestamp":"2025-10-25T22:21:03.993Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-25T22:21:03.993Z] [INFO] {"timestamp":"2025-10-25T22:21:03.993Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-25T22:21:03.994Z] [INFO] {"timestamp":"2025-10-25T22:21:03.994Z","level":"INFO","message":"[DIContainer] Registered module: PeerReviewConsensus","details":{}}
[2025-10-25T22:21:03.994Z] [INFO] {"timestamp":"2025-10-25T22:21:03.994Z","level":"INFO","message":"[CoreLogic] Registered module: PeerReviewConsensus from /upgrades/peer-review-consensus.js","details":{}}
[2025-10-25T22:21:03.994Z] [INFO] {"timestamp":"2025-10-25T22:21:03.994Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-25T22:21:03.996Z] [INFO] {"timestamp":"2025-10-25T22:21:03.995Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-25T22:21:03.997Z] [INFO] {"timestamp":"2025-10-25T22:21:03.997Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-25T22:21:03.997Z] [INFO] {"timestamp":"2025-10-25T22:21:03.997Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-25T22:21:03.998Z] [INFO] {"timestamp":"2025-10-25T22:21:03.998Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-25T22:21:03.998Z] [WARN] {"timestamp":"2025-10-25T22:21:03.998Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-25T22:21:03.999Z] [INFO] {"timestamp":"2025-10-25T22:21:03.999Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-25T22:21:03.999Z] [INFO] {"timestamp":"2025-10-25T22:21:03.999Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-25T22:21:03.999Z] [WARN] {"timestamp":"2025-10-25T22:21:03.999Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-25T22:21:04.000Z] [INFO] {"timestamp":"2025-10-25T22:21:04.000Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-25T22:21:04.000Z] [WARN] {"timestamp":"2025-10-25T22:21:04.000Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-25T22:21:04.000Z] [INFO] {"timestamp":"2025-10-25T22:21:04.000Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-25T22:21:04.000Z] [INFO] {"timestamp":"2025-10-25T22:21:04.000Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-25T22:21:04.001Z] [INFO] {"timestamp":"2025-10-25T22:21:04.001Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-25T22:21:04.004Z] [INFO] {"timestamp":"2025-10-25T22:21:04.004Z","level":"INFO","message":"[ReflectionStore] Initializing reflection persistence","details":{}}
[2025-10-25T22:21:04.005Z] [INFO] {"timestamp":"2025-10-25T22:21:04.005Z","level":"INFO","message":"[ReflectionStore] Database opened successfully","details":{}}
[2025-10-25T22:21:04.005Z] [INFO] {"timestamp":"2025-10-25T22:21:04.005Z","level":"INFO","message":"[SelfTester] Initialized","details":{}}
[2025-10-25T22:21:04.006Z] [INFO] {"timestamp":"2025-10-25T22:21:04.006Z","level":"INFO","message":"[BrowserAPIs] Initializing web API integration...","details":{}}
[2025-10-25T22:21:04.006Z] [INFO] {"timestamp":"2025-10-25T22:21:04.006Z","level":"INFO","message":"[BrowserAPIs] Capabilities detected:","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-25T22:21:04.006Z] [INFO] {"timestamp":"2025-10-25T22:21:04.006Z","level":"INFO","message":"[EventBus] Emitting event: browser-apis:initialized","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-25T22:21:04.008Z] [INFO] {"timestamp":"2025-10-25T22:21:04.008Z","level":"INFO","message":"[GitVFS] Initialized successfully","details":{}}
[2025-10-25T22:21:04.009Z] [INFO] {"timestamp":"2025-10-25T22:21:04.009Z","level":"INFO","message":"[WebRTCSwarm] Initializing swarm module","details":{}}
[2025-10-25T22:21:04.009Z] [INFO] {"timestamp":"2025-10-25T22:21:04.009Z","level":"INFO","message":"[WebRTCSwarm] Local peer ID: reploid-id_1761430864009_68luhnlrb","details":{}}
[2025-10-25T22:21:04.009Z] [INFO] {"timestamp":"2025-10-25T22:21:04.009Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-25T22:21:04.009Z] [INFO] {"timestamp":"2025-10-25T22:21:04.009Z","level":"INFO","message":"[WebRTCSwarm] Swarm initialized","details":{}}
[2025-10-25T22:21:04.010Z] [INFO] {"timestamp":"2025-10-25T22:21:04.010Z","level":"INFO","message":"[SwarmOrch] Initializing swarm orchestrator","details":{}}
[2025-10-25T22:21:04.010Z] [WARN] {"timestamp":"2025-10-25T22:21:04.010Z","level":"WARN","message":"[WebRTCSwarm] Cannot send signaling message: not connected","details":{}}
[2025-10-25T22:21:04.010Z] [INFO] {"timestamp":"2025-10-25T22:21:04.010Z","level":"INFO","message":"[SwarmOrch] Swarm orchestrator initialized","details":{"capabilities":["code-generation","file-management"]}}
[2025-10-25T22:21:04.012Z] [INFO] {"timestamp":"2025-10-25T22:21:04.012Z","level":"INFO","message":"[PyodideRuntime] Initializing Pyodide runtime...","details":{}}
[2025-10-25T22:21:04.013Z] [INFO] {"timestamp":"2025-10-25T22:21:04.013Z","level":"INFO","message":"[PyodideRuntime] Worker created","details":{}}
[2025-10-25T22:21:04.013Z] [ERROR] {"timestamp":"2025-10-25T22:21:04.013Z","level":"ERROR","message":"[WebRTCSwarm] Signaling WebSocket error:","details":{"isTrusted":true}}
[2025-10-25T22:21:04.013Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:21:04.013Z] [WARN] {"timestamp":"2025-10-25T22:21:04.013Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-25T22:21:05.688Z] [INFO] {"timestamp":"2025-10-25T22:21:05.688Z","level":"INFO","message":"[PyodideRuntime] Pyodide initialized","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-25T22:21:05.688Z] [INFO] {"timestamp":"2025-10-25T22:21:05.688Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:ready","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-25T22:21:05.689Z] [INFO] {"timestamp":"2025-10-25T22:21:05.689Z","level":"INFO","message":"[PyodideRuntime] Pyodide runtime ready","details":{}}
[2025-10-25T22:21:05.689Z] [INFO] {"timestamp":"2025-10-25T22:21:05.689Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:initialized","details":{"ready":true}}
[2025-10-25T22:21:05.691Z] [INFO] {"timestamp":"2025-10-25T22:21:05.691Z","level":"INFO","message":"Dashboard UI Manager (Event-Driven) taking control of DOM...","details":{}}
[2025-10-25T22:21:05.701Z] [INFO] {"timestamp":"2025-10-25T22:21:05.701Z","level":"INFO","message":"[UIManager] Initializing modular panel support...","details":{}}
[2025-10-25T22:21:05.701Z] [INFO] {"timestamp":"2025-10-25T22:21:05.701Z","level":"INFO","message":"[UIManager] Modular panel initialization complete","details":{}}
[2025-10-25T22:21:05.702Z] [INFO] {"timestamp":"2025-10-25T22:21:05.702Z","level":"INFO","message":"Dashboard UI Initialized. Listening for events.","details":{}}
[2025-10-25T22:21:05.702Z] [LOG] [CoreLogic] Attempting to initialize DiffViewerUI...
[2025-10-25T22:21:05.702Z] [LOG] [CoreLogic] Resolving DiffViewerUI from container...
[2025-10-25T22:21:05.702Z] [ERROR] [CoreLogic] DiffViewerUI initialization error: Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:681:33)), <anonymous>:61:23)
    at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:681:33), <anonymous>:317:46)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:701:9)
[2025-10-25T22:21:05.702Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:21:05.702Z] [WARN] {"timestamp":"2025-10-25T22:21:05.702Z","level":"WARN","message":"[CoreLogic] DiffViewerUI initialization failed:","details":"[DIContainer] Service not found: DiffViewerUI\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-25T22:21:05.702Z] [INFO] {"timestamp":"2025-10-25T22:21:05.702Z","level":"INFO","message":"[CoreLogic] Agent initialization complete. System is operational.","details":{}}
[2025-10-25T22:21:05.702Z] [INFO] {"timestamp":"2025-10-25T22:21:05.702Z","level":"INFO","message":"[CoreLogic] Creating genesis snapshot of initial boot state...","details":{}}
[2025-10-25T22:21:05.702Z] [WARN] {"timestamp":"2025-10-25T22:21:05.702Z","level":"WARN","message":"[CoreLogic] Failed to create genesis snapshot (non-fatal):","details":"[DIContainer] Service not found: GenesisSnapshot\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-25T22:21:05.702Z] [INFO] {"timestamp":"2025-10-25T22:21:05.702Z","level":"INFO","message":"[CoreLogic] Starting agent with initial goal:","details":"Analyze your own inefficiency patterns and improve yourself"}
[2025-10-25T22:21:05.702Z] [INFO] {"timestamp":"2025-10-25T22:21:05.702Z","level":"INFO","message":"[UI] updateGoal called with:","details":{"text":"Analyze your own inefficiency patterns and improve yourself","hasGoalTextRef":true}}
[2025-10-25T22:21:05.702Z] [INFO] {"timestamp":"2025-10-25T22:21:05.702Z","level":"INFO","message":"[UI] Goal text element updated successfully","details":{}}
[2025-10-25T22:21:05.724Z] [INFO] {"timestamp":"2025-10-25T22:21:05.724Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761430865704_3a90122449db482e/session.json (SHA: 9d1503b)","details":{}}
[2025-10-25T22:21:05.724Z] [INFO] {"timestamp":"2025-10-25T22:21:05.724Z","level":"INFO","message":"[SessionManager] Created new session: session_1761430865704_3a90122449db482e","details":{}}
[2025-10-25T22:21:05.732Z] [INFO] {"timestamp":"2025-10-25T22:21:05.732Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.checkpoints/checkpoint_1761430865725.json (SHA: 30d633f)","details":{}}
[2025-10-25T22:21:05.732Z] [INFO] {"timestamp":"2025-10-25T22:21:05.732Z","level":"INFO","message":"[StateManager] Created checkpoint: checkpoint_1761430865725 - Session session_1761430865704_3a90122449db482e - Turn 0 start","details":{}}
[2025-10-25T22:21:05.732Z] [INFO] {"timestamp":"2025-10-25T22:21:05.732Z","level":"INFO","message":"[SessionManager] Created turn checkpoint: checkpoint_1761430865725","details":{}}
[2025-10-25T22:21:05.739Z] [INFO] {"timestamp":"2025-10-25T22:21:05.739Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761430865704_3a90122449db482e/session.json (SHA: 9a103f1)","details":{}}
[2025-10-25T22:21:05.739Z] [INFO] {"timestamp":"2025-10-25T22:21:05.739Z","level":"INFO","message":"[SessionManager] Created turn 0 for session session_1761430865704_3a90122449db482e","details":{}}
[2025-10-25T22:21:05.739Z] [INFO] {"timestamp":"2025-10-25T22:21:05.739Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"","progress":null}}
[2025-10-25T22:21:05.739Z] [INFO] {"timestamp":"2025-10-25T22:21:05.739Z","level":"INFO","message":"[SentinelFSM] State transition: IDLE -> CURATING_CONTEXT","details":{}}
[2025-10-25T22:21:05.739Z] [INFO] {"timestamp":"2025-10-25T22:21:05.739Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"IDLE","newState":"CURATING_CONTEXT","context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761430865704_3a90122449db482e","turn":{"turn":0,"context_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.context.md","proposal_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761430865725","createdAt":"2025-10-25T22:21:05.732Z"},"startTime":1761430865739,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:21:05.740Z] [INFO] {"timestamp":"2025-10-25T22:21:05.740Z","level":"INFO","message":"[SentinelFSM] Executing state: CURATING_CONTEXT","details":{}}
[2025-10-25T22:21:05.740Z] [INFO] {"timestamp":"2025-10-25T22:21:05.740Z","level":"INFO","message":"[EventBus] Emitting event: agent:curating","details":{"goal":"Analyze your own inefficiency patterns and improve yourself"}}
[2025-10-25T22:21:05.740Z] [INFO] {"timestamp":"2025-10-25T22:21:05.740Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Analyzing project files...","progress":null}}
[2025-10-25T22:21:05.740Z] [WARN] {"timestamp":"2025-10-25T22:21:05.740Z","level":"WARN","message":"[SentinelTools] No files available for curation","details":{}}
[2025-10-25T22:21:05.740Z] [INFO] {"timestamp":"2025-10-25T22:21:05.740Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Found 0 relevant files","progress":50}}
[2025-10-25T22:21:05.740Z] [ERROR] {"timestamp":"2025-10-25T22:21:05.740Z","level":"ERROR","message":"[SentinelFSM] Error in state CURATING_CONTEXT:","details":{}}
[2025-10-25T22:21:05.740Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:21:05.740Z] [INFO] {"timestamp":"2025-10-25T22:21:05.740Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"ERROR","detail":"","progress":null}}
[2025-10-25T22:21:05.740Z] [INFO] {"timestamp":"2025-10-25T22:21:05.740Z","level":"INFO","message":"[SentinelFSM] State transition: CURATING_CONTEXT -> ERROR","details":{}}
[2025-10-25T22:21:05.740Z] [INFO] {"timestamp":"2025-10-25T22:21:05.740Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"CURATING_CONTEXT","newState":"ERROR","context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761430865704_3a90122449db482e","turn":{"turn":0,"context_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.context.md","proposal_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761430865725","createdAt":"2025-10-25T22:21:05.732Z"},"startTime":1761430865739,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:21:05.740Z] [INFO] {"timestamp":"2025-10-25T22:21:05.740Z","level":"INFO","message":"[SentinelFSM] Executing state: ERROR","details":{}}
[2025-10-25T22:21:05.741Z] [INFO] {"timestamp":"2025-10-25T22:21:05.740Z","level":"INFO","message":"[EventBus] Emitting event: agent:error","details":{"state":{"from":"CURATING_CONTEXT","to":"ERROR","timestamp":1761430865740,"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761430865704_3a90122449db482e","turn":{"turn":0,"context_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.context.md","proposal_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761430865725","createdAt":"2025-10-25T22:21:05.732Z"},"startTime":1761430865739,"iterations":0,"maxIterations":10}},"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761430865704_3a90122449db482e","turn":{"turn":0,"context_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.context.md","proposal_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761430865725","createdAt":"2025-10-25T22:21:05.732Z"},"startTime":1761430865739,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:21:05.741Z] [ERROR] {"timestamp":"2025-10-25T22:21:05.741Z","level":"ERROR","message":"[SentinelFSM] Error state reached","details":{"previous_state":{"from":"IDLE","to":"CURATING_CONTEXT","timestamp":1761430865739,"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761430865704_3a90122449db482e","turn":{"turn":0,"context_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.context.md","proposal_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761430865725","createdAt":"2025-10-25T22:21:05.732Z"},"startTime":1761430865739,"iterations":0,"maxIterations":10}},"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761430865704_3a90122449db482e","turn":{"turn":0,"context_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.context.md","proposal_path":"/sessions/session_1761430865704_3a90122449db482e/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761430865725","createdAt":"2025-10-25T22:21:05.732Z"},"startTime":1761430865739,"iterations":0,"maxIterations":10}}}
[2025-10-25T22:21:05.741Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-25T22:21:05.741Z] [INFO] {"timestamp":"2025-10-25T22:21:05.741Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"IDLE","detail":"","progress":null}}
[2025-10-25T22:21:05.741Z] [INFO] {"timestamp":"2025-10-25T22:21:05.741Z","level":"INFO","message":"[SentinelFSM] State transition: ERROR -> IDLE","details":{}}
[2025-10-25T22:21:05.741Z] [INFO] {"timestamp":"2025-10-25T22:21:05.741Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"ERROR","newState":"IDLE","context":null}}
[2025-10-25T22:21:05.741Z] [INFO] {"timestamp":"2025-10-25T22:21:05.741Z","level":"INFO","message":"[CoreLogic] Agent cycle started successfully","details":{}}
[2025-10-25T22:21:05.741Z] [LOG] [Boot] Agent system awakened successfully
[2025-10-25T22:21:09.013Z] [INFO] {"timestamp":"2025-10-25T22:21:09.013Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-25T22:21:09.014Z] [INFO] {"timestamp":"2025-10-25T22:21:09.013Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-25T22:21:09.017Z] [INFO] {"timestamp":"2025-10-25T22:21:09.017Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-25T22:21:09.018Z] [INFO] {"timestamp":"2025-10-25T22:21:09.018Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-25T22:21:38.971Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:21:38.971Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:21:38.979Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:21:38.979Z] [LOG] [API] Checking server status...
[2025-10-25T22:21:38.986Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:21:38.982Z"
}
[2025-10-25T22:21:38.986Z] [LOG] [API] WebGPU available
[2025-10-25T22:21:38.993Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:21:38.993Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:21:38.993Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:21:39.016Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:22:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:22:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:22:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:22:50.377Z] [LOG] [API] Checking server status...
[2025-10-25T22:22:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:22:50.378Z"
}
[2025-10-25T22:22:50.386Z] [LOG] [API] WebGPU available
[2025-10-25T22:22:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:22:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:22:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:22:50.401Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:23:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:23:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:23:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:23:50.396Z] [LOG] [API] Checking server status...
[2025-10-25T22:23:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:23:50.397Z"
}
[2025-10-25T22:23:50.406Z] [LOG] [API] WebGPU available
[2025-10-25T22:23:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:23:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:23:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:23:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:24:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:24:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:24:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:24:50.361Z] [LOG] [API] Checking server status...
[2025-10-25T22:24:50.371Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:24:50.362Z"
}
[2025-10-25T22:24:50.371Z] [LOG] [API] WebGPU available
[2025-10-25T22:24:50.376Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:24:50.376Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:24:50.376Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:24:50.389Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:25:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:25:50.389Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:25:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:25:50.392Z] [LOG] [API] Checking server status...
[2025-10-25T22:25:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:25:50.394Z"
}
[2025-10-25T22:25:50.398Z] [LOG] [API] WebGPU available
[2025-10-25T22:25:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:25:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:25:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:25:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:26:50.278Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:26:50.278Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:26:50.284Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:26:50.284Z] [LOG] [API] Checking server status...
[2025-10-25T22:26:50.292Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:26:50.285Z"
}
[2025-10-25T22:26:50.292Z] [LOG] [API] WebGPU available
[2025-10-25T22:26:50.299Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:26:50.299Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:26:50.299Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:26:50.313Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:27:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:27:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:27:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:27:50.365Z] [LOG] [API] Checking server status...
[2025-10-25T22:27:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:27:50.367Z"
}
[2025-10-25T22:27:50.377Z] [LOG] [API] WebGPU available
[2025-10-25T22:27:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:27:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:27:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:27:50.396Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:28:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:28:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:28:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:28:50.385Z] [LOG] [API] Checking server status...
[2025-10-25T22:28:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:28:50.387Z"
}
[2025-10-25T22:28:50.394Z] [LOG] [API] WebGPU available
[2025-10-25T22:28:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:28:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:28:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:28:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:29:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:29:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:29:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:29:50.364Z] [LOG] [API] Checking server status...
[2025-10-25T22:29:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:29:50.366Z"
}
[2025-10-25T22:29:50.375Z] [LOG] [API] WebGPU available
[2025-10-25T22:29:50.379Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:29:50.379Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:29:50.379Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:29:50.395Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:30:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:30:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:30:50.601Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:30:50.601Z] [LOG] [API] Checking server status...
[2025-10-25T22:30:50.609Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:30:50.602Z"
}
[2025-10-25T22:30:50.609Z] [LOG] [API] WebGPU available
[2025-10-25T22:30:50.612Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:30:50.613Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:30:50.613Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:30:50.621Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:31:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:31:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:31:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:31:50.372Z] [LOG] [API] Checking server status...
[2025-10-25T22:31:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:31:50.374Z"
}
[2025-10-25T22:31:50.384Z] [LOG] [API] WebGPU available
[2025-10-25T22:31:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:31:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:31:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:31:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:32:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:32:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:32:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:32:50.395Z] [LOG] [API] Checking server status...
[2025-10-25T22:32:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:32:50.399Z"
}
[2025-10-25T22:32:50.405Z] [LOG] [API] WebGPU available
[2025-10-25T22:32:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:32:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:32:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:32:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:33:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:33:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:33:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:33:50.367Z] [LOG] [API] Checking server status...
[2025-10-25T22:33:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:33:50.368Z"
}
[2025-10-25T22:33:50.377Z] [LOG] [API] WebGPU available
[2025-10-25T22:33:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:33:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:33:50.384Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:33:50.393Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:34:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:34:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:34:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:34:50.379Z] [LOG] [API] Checking server status...
[2025-10-25T22:34:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:34:50.381Z"
}
[2025-10-25T22:34:50.388Z] [LOG] [API] WebGPU available
[2025-10-25T22:34:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:34:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:34:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:34:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:35:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:35:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:35:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:35:50.373Z] [LOG] [API] Checking server status...
[2025-10-25T22:35:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:35:50.374Z"
}
[2025-10-25T22:35:50.384Z] [LOG] [API] WebGPU available
[2025-10-25T22:35:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:35:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:35:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:35:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:36:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:36:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:36:50.423Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:36:50.423Z] [LOG] [API] Checking server status...
[2025-10-25T22:36:50.433Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:36:50.424Z"
}
[2025-10-25T22:36:50.433Z] [LOG] [API] WebGPU available
[2025-10-25T22:36:50.437Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:36:50.438Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:36:50.438Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:36:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:37:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:37:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:37:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:37:50.360Z] [LOG] [API] Checking server status...
[2025-10-25T22:37:50.371Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:37:50.361Z"
}
[2025-10-25T22:37:50.371Z] [LOG] [API] WebGPU available
[2025-10-25T22:37:50.377Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:37:50.378Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:37:50.378Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:37:50.386Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:38:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:38:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:38:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:38:50.370Z] [LOG] [API] Checking server status...
[2025-10-25T22:38:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:38:50.372Z"
}
[2025-10-25T22:38:50.382Z] [LOG] [API] WebGPU available
[2025-10-25T22:38:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:38:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:38:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:38:50.397Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:39:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:39:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:39:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:39:50.386Z] [LOG] [API] Checking server status...
[2025-10-25T22:39:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:39:50.388Z"
}
[2025-10-25T22:39:50.398Z] [LOG] [API] WebGPU available
[2025-10-25T22:39:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:39:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:39:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:39:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:40:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:40:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:40:50.439Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:40:50.439Z] [LOG] [API] Checking server status...
[2025-10-25T22:40:50.462Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:40:50.441Z"
}
[2025-10-25T22:40:50.463Z] [LOG] [API] WebGPU available
[2025-10-25T22:40:50.469Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:40:50.469Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:40:50.470Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:40:50.476Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:41:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:41:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:41:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:41:50.369Z] [LOG] [API] Checking server status...
[2025-10-25T22:41:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:41:50.372Z"
}
[2025-10-25T22:41:50.380Z] [LOG] [API] WebGPU available
[2025-10-25T22:41:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:41:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:41:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:41:50.401Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:42:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:42:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:42:50.417Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:42:50.418Z] [LOG] [API] Checking server status...
[2025-10-25T22:42:50.429Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:42:50.421Z"
}
[2025-10-25T22:42:50.429Z] [LOG] [API] WebGPU available
[2025-10-25T22:42:50.444Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:42:50.444Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:42:50.444Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:42:50.465Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:43:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:43:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:43:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:43:50.370Z] [LOG] [API] Checking server status...
[2025-10-25T22:43:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:43:50.372Z"
}
[2025-10-25T22:43:50.383Z] [LOG] [API] WebGPU available
[2025-10-25T22:43:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:43:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:43:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:43:50.398Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:44:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:44:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:44:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:44:50.381Z] [LOG] [API] Checking server status...
[2025-10-25T22:44:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:44:50.383Z"
}
[2025-10-25T22:44:50.391Z] [LOG] [API] WebGPU available
[2025-10-25T22:44:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:44:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:44:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:44:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:45:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:45:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:45:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:45:50.371Z] [LOG] [API] Checking server status...
[2025-10-25T22:45:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:45:50.372Z"
}
[2025-10-25T22:45:50.381Z] [LOG] [API] WebGPU available
[2025-10-25T22:45:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:45:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:45:50.384Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:45:50.394Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:46:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:46:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:46:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:46:50.387Z] [LOG] [API] Checking server status...
[2025-10-25T22:46:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:46:50.390Z"
}
[2025-10-25T22:46:50.396Z] [LOG] [API] WebGPU available
[2025-10-25T22:46:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:46:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:46:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:46:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:47:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:47:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:47:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:47:50.371Z] [LOG] [API] Checking server status...
[2025-10-25T22:47:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:47:50.372Z"
}
[2025-10-25T22:47:50.381Z] [LOG] [API] WebGPU available
[2025-10-25T22:47:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:47:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:47:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:47:50.397Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:48:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:48:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:48:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:48:50.386Z] [LOG] [API] Checking server status...
[2025-10-25T22:48:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:48:50.388Z"
}
[2025-10-25T22:48:50.395Z] [LOG] [API] WebGPU available
[2025-10-25T22:48:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:48:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:48:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:48:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:49:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:49:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:49:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:49:50.382Z] [LOG] [API] Checking server status...
[2025-10-25T22:49:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:49:50.384Z"
}
[2025-10-25T22:49:50.393Z] [LOG] [API] WebGPU available
[2025-10-25T22:49:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:49:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:49:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:49:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:50:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:50:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:50:50.418Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:50:50.418Z] [LOG] [API] Checking server status...
[2025-10-25T22:50:50.428Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:50:50.420Z"
}
[2025-10-25T22:50:50.428Z] [LOG] [API] WebGPU available
[2025-10-25T22:50:50.440Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:50:50.440Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:50:50.440Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:50:50.466Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:51:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:51:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:51:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:51:50.377Z] [LOG] [API] Checking server status...
[2025-10-25T22:51:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:51:50.380Z"
}
[2025-10-25T22:51:50.390Z] [LOG] [API] WebGPU available
[2025-10-25T22:51:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:51:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:51:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:51:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:52:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:52:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:52:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:52:50.389Z] [LOG] [API] Checking server status...
[2025-10-25T22:52:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:52:50.390Z"
}
[2025-10-25T22:52:50.398Z] [LOG] [API] WebGPU available
[2025-10-25T22:52:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:52:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:52:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:52:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:53:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:53:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:53:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:53:50.385Z] [LOG] [API] Checking server status...
[2025-10-25T22:53:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:53:50.387Z"
}
[2025-10-25T22:53:50.395Z] [LOG] [API] WebGPU available
[2025-10-25T22:53:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:53:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:53:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:53:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:54:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:54:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:54:50.418Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:54:50.418Z] [LOG] [API] Checking server status...
[2025-10-25T22:54:50.429Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:54:50.422Z"
}
[2025-10-25T22:54:50.430Z] [LOG] [API] WebGPU available
[2025-10-25T22:54:50.443Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:54:50.443Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:54:50.443Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:54:50.468Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:55:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:55:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:55:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:55:50.375Z] [LOG] [API] Checking server status...
[2025-10-25T22:55:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:55:50.377Z"
}
[2025-10-25T22:55:50.386Z] [LOG] [API] WebGPU available
[2025-10-25T22:55:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:55:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:55:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:55:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:56:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:56:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:56:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:56:50.386Z] [LOG] [API] Checking server status...
[2025-10-25T22:56:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:56:50.387Z"
}
[2025-10-25T22:56:50.396Z] [LOG] [API] WebGPU available
[2025-10-25T22:56:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:56:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:56:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:56:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:57:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:57:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:57:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:57:50.378Z] [LOG] [API] Checking server status...
[2025-10-25T22:57:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:57:50.380Z"
}
[2025-10-25T22:57:50.388Z] [LOG] [API] WebGPU available
[2025-10-25T22:57:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:57:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:57:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:57:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:58:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:58:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:58:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:58:50.366Z] [LOG] [API] Checking server status...
[2025-10-25T22:58:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:58:50.367Z"
}
[2025-10-25T22:58:50.376Z] [LOG] [API] WebGPU available
[2025-10-25T22:58:50.380Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:58:50.380Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:58:50.380Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:58:50.395Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T22:59:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T22:59:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T22:59:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T22:59:50.374Z] [LOG] [API] Checking server status...
[2025-10-25T22:59:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T22:59:50.376Z"
}
[2025-10-25T22:59:50.386Z] [LOG] [API] WebGPU available
[2025-10-25T22:59:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T22:59:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T22:59:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T22:59:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:00:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:00:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:00:50.476Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:00:50.477Z] [LOG] [API] Checking server status...
[2025-10-25T23:00:50.491Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:00:50.478Z"
}
[2025-10-25T23:00:50.491Z] [LOG] [API] WebGPU available
[2025-10-25T23:00:50.497Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:00:50.498Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:00:50.498Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:00:50.505Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:01:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:01:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:01:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:01:50.384Z] [LOG] [API] Checking server status...
[2025-10-25T23:01:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:01:50.385Z"
}
[2025-10-25T23:01:50.395Z] [LOG] [API] WebGPU available
[2025-10-25T23:01:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:01:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:01:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:01:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:02:50.550Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:02:50.550Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:02:50.552Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:02:50.552Z] [LOG] [API] Checking server status...
[2025-10-25T23:02:50.561Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:02:50.553Z"
}
[2025-10-25T23:02:50.561Z] [LOG] [API] WebGPU available
[2025-10-25T23:02:50.564Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:02:50.564Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:02:50.564Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:02:50.583Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:03:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:03:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:03:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:03:50.376Z] [LOG] [API] Checking server status...
[2025-10-25T23:03:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:03:50.379Z"
}
[2025-10-25T23:03:50.388Z] [LOG] [API] WebGPU available
[2025-10-25T23:03:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:03:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:03:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:03:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:04:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:04:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:04:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:04:50.381Z] [LOG] [API] Checking server status...
[2025-10-25T23:04:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:04:50.384Z"
}
[2025-10-25T23:04:50.390Z] [LOG] [API] WebGPU available
[2025-10-25T23:04:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:04:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:04:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:04:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:05:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:05:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:05:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:05:50.388Z] [LOG] [API] Checking server status...
[2025-10-25T23:05:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:05:50.389Z"
}
[2025-10-25T23:05:50.397Z] [LOG] [API] WebGPU available
[2025-10-25T23:05:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:05:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:05:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:05:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:06:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:06:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:06:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:06:50.387Z] [LOG] [API] Checking server status...
[2025-10-25T23:06:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:06:50.388Z"
}
[2025-10-25T23:06:50.397Z] [LOG] [API] WebGPU available
[2025-10-25T23:06:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:06:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:06:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:06:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:07:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:07:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:07:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:07:50.364Z] [LOG] [API] Checking server status...
[2025-10-25T23:07:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:07:50.366Z"
}
[2025-10-25T23:07:50.374Z] [LOG] [API] WebGPU available
[2025-10-25T23:07:50.378Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:07:50.378Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:07:50.378Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:07:50.394Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:08:50.583Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:08:50.583Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:08:50.585Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:08:50.585Z] [LOG] [API] Checking server status...
[2025-10-25T23:08:50.595Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:08:50.587Z"
}
[2025-10-25T23:08:50.595Z] [LOG] [API] WebGPU available
[2025-10-25T23:08:50.597Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:08:50.597Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:08:50.597Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:08:50.615Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:09:50.394Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:09:50.394Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:09:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:09:50.398Z] [LOG] [API] Checking server status...
[2025-10-25T23:09:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:09:50.400Z"
}
[2025-10-25T23:09:50.409Z] [LOG] [API] WebGPU available
[2025-10-25T23:09:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:09:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:09:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:09:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:10:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:10:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:10:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:10:50.397Z] [LOG] [API] Checking server status...
[2025-10-25T23:10:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:10:50.398Z"
}
[2025-10-25T23:10:50.406Z] [LOG] [API] WebGPU available
[2025-10-25T23:10:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:10:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:10:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:10:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:11:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:11:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:11:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:11:50.384Z] [LOG] [API] Checking server status...
[2025-10-25T23:11:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:11:50.386Z"
}
[2025-10-25T23:11:50.394Z] [LOG] [API] WebGPU available
[2025-10-25T23:11:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:11:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:11:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:11:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:12:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:12:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:12:50.436Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:12:50.436Z] [LOG] [API] Checking server status...
[2025-10-25T23:12:50.447Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:12:50.437Z"
}
[2025-10-25T23:12:50.447Z] [LOG] [API] WebGPU available
[2025-10-25T23:12:50.455Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:12:50.455Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:12:50.455Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:12:50.463Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:13:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:13:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:13:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:13:50.378Z] [LOG] [API] Checking server status...
[2025-10-25T23:13:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:13:50.380Z"
}
[2025-10-25T23:13:50.388Z] [LOG] [API] WebGPU available
[2025-10-25T23:13:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:13:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:13:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:13:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:14:50.497Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:14:50.497Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:14:50.499Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:14:50.499Z] [LOG] [API] Checking server status...
[2025-10-25T23:14:50.511Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:14:50.500Z"
}
[2025-10-25T23:14:50.511Z] [LOG] [API] WebGPU available
[2025-10-25T23:14:50.519Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:14:50.519Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:14:50.519Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:14:50.528Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:15:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:15:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:15:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:15:50.395Z] [LOG] [API] Checking server status...
[2025-10-25T23:15:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:15:50.397Z"
}
[2025-10-25T23:15:50.405Z] [LOG] [API] WebGPU available
[2025-10-25T23:15:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:15:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:15:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:15:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:16:50.414Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:16:50.414Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:16:50.430Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:16:50.430Z] [LOG] [API] Checking server status...
[2025-10-25T23:16:50.438Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:16:50.433Z"
}
[2025-10-25T23:16:50.438Z] [LOG] [API] WebGPU available
[2025-10-25T23:16:50.442Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:16:50.442Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:16:50.442Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:16:50.458Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:17:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:17:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:17:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:17:50.367Z] [LOG] [API] Checking server status...
[2025-10-25T23:17:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:17:50.369Z"
}
[2025-10-25T23:17:50.378Z] [LOG] [API] WebGPU available
[2025-10-25T23:17:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:17:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:17:50.384Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:17:50.395Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:18:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:18:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:18:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:18:50.397Z] [LOG] [API] Checking server status...
[2025-10-25T23:18:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:18:50.398Z"
}
[2025-10-25T23:18:50.408Z] [LOG] [API] WebGPU available
[2025-10-25T23:18:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:18:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:18:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:18:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:19:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:19:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:19:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:19:50.376Z] [LOG] [API] Checking server status...
[2025-10-25T23:19:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:19:50.378Z"
}
[2025-10-25T23:19:50.388Z] [LOG] [API] WebGPU available
[2025-10-25T23:19:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:19:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:19:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:19:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:20:50.471Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:20:50.471Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:20:50.474Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:20:50.474Z] [LOG] [API] Checking server status...
[2025-10-25T23:20:50.485Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:20:50.475Z"
}
[2025-10-25T23:20:50.485Z] [LOG] [API] WebGPU available
[2025-10-25T23:20:50.491Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:20:50.491Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:20:50.491Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:20:50.498Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:21:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:21:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:21:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:21:50.394Z] [LOG] [API] Checking server status...
[2025-10-25T23:21:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:21:50.395Z"
}
[2025-10-25T23:21:50.403Z] [LOG] [API] WebGPU available
[2025-10-25T23:21:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:21:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:21:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:21:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:22:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:22:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:22:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:22:50.375Z] [LOG] [API] Checking server status...
[2025-10-25T23:22:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:22:50.376Z"
}
[2025-10-25T23:22:50.384Z] [LOG] [API] WebGPU available
[2025-10-25T23:22:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:22:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:22:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:22:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:23:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:23:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:23:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:23:50.377Z] [LOG] [API] Checking server status...
[2025-10-25T23:23:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:23:50.380Z"
}
[2025-10-25T23:23:50.389Z] [LOG] [API] WebGPU available
[2025-10-25T23:23:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:23:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:23:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:23:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:24:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:24:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:24:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:24:50.414Z] [LOG] [API] Checking server status...
[2025-10-25T23:24:50.424Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:24:50.416Z"
}
[2025-10-25T23:24:50.425Z] [LOG] [API] WebGPU available
[2025-10-25T23:24:50.439Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:24:50.439Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:24:50.439Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:24:50.465Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:25:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:25:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:25:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:25:50.372Z] [LOG] [API] Checking server status...
[2025-10-25T23:25:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:25:50.374Z"
}
[2025-10-25T23:25:50.382Z] [LOG] [API] WebGPU available
[2025-10-25T23:25:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:25:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:25:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:25:50.394Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:26:50.503Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:26:50.503Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:26:50.508Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:26:50.508Z] [LOG] [API] Checking server status...
[2025-10-25T23:26:50.516Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:26:50.509Z"
}
[2025-10-25T23:26:50.516Z] [LOG] [API] WebGPU available
[2025-10-25T23:26:50.524Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:26:50.524Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:26:50.524Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:26:50.536Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:27:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:27:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:27:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:27:50.395Z] [LOG] [API] Checking server status...
[2025-10-25T23:27:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:27:50.397Z"
}
[2025-10-25T23:27:50.406Z] [LOG] [API] WebGPU available
[2025-10-25T23:27:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:27:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:27:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:27:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:28:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:28:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:28:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:28:50.384Z] [LOG] [API] Checking server status...
[2025-10-25T23:28:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:28:50.385Z"
}
[2025-10-25T23:28:50.392Z] [LOG] [API] WebGPU available
[2025-10-25T23:28:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:28:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:28:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:28:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:29:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:29:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:29:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:29:50.383Z] [LOG] [API] Checking server status...
[2025-10-25T23:29:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:29:50.385Z"
}
[2025-10-25T23:29:50.395Z] [LOG] [API] WebGPU available
[2025-10-25T23:29:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:29:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:29:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:29:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:30:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:30:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:30:50.437Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:30:50.437Z] [LOG] [API] Checking server status...
[2025-10-25T23:30:50.445Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:30:50.438Z"
}
[2025-10-25T23:30:50.445Z] [LOG] [API] WebGPU available
[2025-10-25T23:30:50.452Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:30:50.452Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:30:50.452Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:30:50.461Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:31:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:31:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:31:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:31:50.383Z] [LOG] [API] Checking server status...
[2025-10-25T23:31:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:31:50.386Z"
}
[2025-10-25T23:31:50.393Z] [LOG] [API] WebGPU available
[2025-10-25T23:31:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:31:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:31:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:31:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:32:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:32:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:32:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:32:50.390Z] [LOG] [API] Checking server status...
[2025-10-25T23:32:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:32:50.391Z"
}
[2025-10-25T23:32:50.399Z] [LOG] [API] WebGPU available
[2025-10-25T23:32:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:32:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:32:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:32:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:33:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:33:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:33:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:33:50.375Z] [LOG] [API] Checking server status...
[2025-10-25T23:33:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:33:50.376Z"
}
[2025-10-25T23:33:50.385Z] [LOG] [API] WebGPU available
[2025-10-25T23:33:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:33:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:33:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:33:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:34:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:34:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:34:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:34:50.381Z] [LOG] [API] Checking server status...
[2025-10-25T23:34:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:34:50.384Z"
}
[2025-10-25T23:34:50.391Z] [LOG] [API] WebGPU available
[2025-10-25T23:34:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:34:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:34:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:34:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:35:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:35:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:35:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:35:50.378Z] [LOG] [API] Checking server status...
[2025-10-25T23:35:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:35:50.380Z"
}
[2025-10-25T23:35:50.388Z] [LOG] [API] WebGPU available
[2025-10-25T23:35:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:35:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:35:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:35:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:36:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:36:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:36:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:36:50.387Z] [LOG] [API] Checking server status...
[2025-10-25T23:36:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:36:50.388Z"
}
[2025-10-25T23:36:50.396Z] [LOG] [API] WebGPU available
[2025-10-25T23:36:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:36:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:36:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:36:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:37:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:37:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:37:50.363Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:37:50.363Z] [LOG] [API] Checking server status...
[2025-10-25T23:37:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:37:50.364Z"
}
[2025-10-25T23:37:50.373Z] [LOG] [API] WebGPU available
[2025-10-25T23:37:50.377Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:37:50.378Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:37:50.378Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:37:50.385Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:38:50.533Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:38:50.533Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:38:50.537Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:38:50.537Z] [LOG] [API] Checking server status...
[2025-10-25T23:38:50.546Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:38:50.538Z"
}
[2025-10-25T23:38:50.546Z] [LOG] [API] WebGPU available
[2025-10-25T23:38:50.549Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:38:50.549Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:38:50.549Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:38:50.563Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:39:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:39:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:39:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:39:50.369Z] [LOG] [API] Checking server status...
[2025-10-25T23:39:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:39:50.372Z"
}
[2025-10-25T23:39:50.381Z] [LOG] [API] WebGPU available
[2025-10-25T23:39:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:39:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:39:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:39:50.392Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:40:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:40:50.389Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:40:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:40:50.399Z] [LOG] [API] Checking server status...
[2025-10-25T23:40:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:40:50.402Z"
}
[2025-10-25T23:40:50.408Z] [LOG] [API] WebGPU available
[2025-10-25T23:40:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:40:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:40:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:40:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:41:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:41:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:41:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:41:50.375Z] [LOG] [API] Checking server status...
[2025-10-25T23:41:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:41:50.378Z"
}
[2025-10-25T23:41:50.386Z] [LOG] [API] WebGPU available
[2025-10-25T23:41:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:41:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:41:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:41:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:42:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:42:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:42:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:42:50.380Z] [LOG] [API] Checking server status...
[2025-10-25T23:42:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:42:50.381Z"
}
[2025-10-25T23:42:50.388Z] [LOG] [API] WebGPU available
[2025-10-25T23:42:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:42:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:42:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:42:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:43:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:43:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:43:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:43:50.371Z] [LOG] [API] Checking server status...
[2025-10-25T23:43:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:43:50.373Z"
}
[2025-10-25T23:43:50.382Z] [LOG] [API] WebGPU available
[2025-10-25T23:43:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:43:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:43:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:43:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:44:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:44:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:44:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:44:50.387Z] [LOG] [API] Checking server status...
[2025-10-25T23:44:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:44:50.388Z"
}
[2025-10-25T23:44:50.396Z] [LOG] [API] WebGPU available
[2025-10-25T23:44:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:44:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:44:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:44:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:45:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:45:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:45:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:45:50.387Z] [LOG] [API] Checking server status...
[2025-10-25T23:45:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:45:50.389Z"
}
[2025-10-25T23:45:50.399Z] [LOG] [API] WebGPU available
[2025-10-25T23:45:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:45:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:45:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:45:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:46:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:46:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:46:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:46:50.383Z] [LOG] [API] Checking server status...
[2025-10-25T23:46:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:46:50.385Z"
}
[2025-10-25T23:46:50.395Z] [LOG] [API] WebGPU available
[2025-10-25T23:46:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:46:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:46:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:46:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:47:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:47:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:47:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:47:50.372Z] [LOG] [API] Checking server status...
[2025-10-25T23:47:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:47:50.373Z"
}
[2025-10-25T23:47:50.382Z] [LOG] [API] WebGPU available
[2025-10-25T23:47:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:47:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:47:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:47:50.393Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:48:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:48:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:48:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:48:50.386Z] [LOG] [API] Checking server status...
[2025-10-25T23:48:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:48:50.387Z"
}
[2025-10-25T23:48:50.395Z] [LOG] [API] WebGPU available
[2025-10-25T23:48:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:48:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:48:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:48:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:49:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:49:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:49:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:49:50.383Z] [LOG] [API] Checking server status...
[2025-10-25T23:49:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:49:50.385Z"
}
[2025-10-25T23:49:50.394Z] [LOG] [API] WebGPU available
[2025-10-25T23:49:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:49:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:49:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:49:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:50:50.476Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:50:50.476Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:50:50.478Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:50:50.478Z] [LOG] [API] Checking server status...
[2025-10-25T23:50:50.488Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:50:50.480Z"
}
[2025-10-25T23:50:50.488Z] [LOG] [API] WebGPU available
[2025-10-25T23:50:50.495Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:50:50.495Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:50:50.495Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:50:50.502Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:51:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:51:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:51:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:51:50.374Z] [LOG] [API] Checking server status...
[2025-10-25T23:51:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:51:50.375Z"
}
[2025-10-25T23:51:50.384Z] [LOG] [API] WebGPU available
[2025-10-25T23:51:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:51:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:51:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:51:50.396Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:52:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:52:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:52:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:52:50.399Z] [LOG] [API] Checking server status...
[2025-10-25T23:52:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:52:50.400Z"
}
[2025-10-25T23:52:50.407Z] [LOG] [API] WebGPU available
[2025-10-25T23:52:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:52:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:52:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:52:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:53:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:53:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:53:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:53:50.383Z] [LOG] [API] Checking server status...
[2025-10-25T23:53:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:53:50.385Z"
}
[2025-10-25T23:53:50.394Z] [LOG] [API] WebGPU available
[2025-10-25T23:53:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:53:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:53:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:53:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:54:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:54:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:54:50.418Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:54:50.418Z] [LOG] [API] Checking server status...
[2025-10-25T23:54:50.429Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:54:50.419Z"
}
[2025-10-25T23:54:50.429Z] [LOG] [API] WebGPU available
[2025-10-25T23:54:50.438Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:54:50.438Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:54:50.438Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:54:50.461Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:55:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:55:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:55:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:55:50.385Z] [LOG] [API] Checking server status...
[2025-10-25T23:55:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:55:50.388Z"
}
[2025-10-25T23:55:50.396Z] [LOG] [API] WebGPU available
[2025-10-25T23:55:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:55:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:55:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:55:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:56:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:56:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:56:50.502Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:56:50.502Z] [LOG] [API] Checking server status...
[2025-10-25T23:56:50.510Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:56:50.503Z"
}
[2025-10-25T23:56:50.510Z] [LOG] [API] WebGPU available
[2025-10-25T23:56:50.513Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:56:50.513Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:56:50.513Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:56:50.523Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:57:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:57:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:57:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:57:50.371Z] [LOG] [API] Checking server status...
[2025-10-25T23:57:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:57:50.373Z"
}
[2025-10-25T23:57:50.381Z] [LOG] [API] WebGPU available
[2025-10-25T23:57:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:57:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:57:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:57:50.392Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:58:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:58:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:58:50.418Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:58:50.418Z] [LOG] [API] Checking server status...
[2025-10-25T23:58:50.427Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:58:50.422Z"
}
[2025-10-25T23:58:50.428Z] [LOG] [API] WebGPU available
[2025-10-25T23:58:50.439Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:58:50.439Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:58:50.439Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:58:50.464Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-25T23:59:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-25T23:59:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-25T23:59:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-25T23:59:50.389Z] [LOG] [API] Checking server status...
[2025-10-25T23:59:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-25T23:59:50.392Z"
}
[2025-10-25T23:59:50.401Z] [LOG] [API] WebGPU available
[2025-10-25T23:59:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-25T23:59:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-25T23:59:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-25T23:59:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:00:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:00:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:00:50.483Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:00:50.483Z] [LOG] [API] Checking server status...
[2025-10-26T00:00:50.493Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:00:50.484Z"
}
[2025-10-26T00:00:50.493Z] [LOG] [API] WebGPU available
[2025-10-26T00:00:50.495Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:00:50.495Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:00:50.495Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:00:50.503Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:01:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:01:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:01:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:01:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T00:01:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:01:50.382Z"
}
[2025-10-26T00:01:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T00:01:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:01:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:01:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:01:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:02:50.513Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:02:50.513Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:02:50.515Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:02:50.515Z] [LOG] [API] Checking server status...
[2025-10-26T00:02:50.527Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:02:50.516Z"
}
[2025-10-26T00:02:50.527Z] [LOG] [API] WebGPU available
[2025-10-26T00:02:50.532Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:02:50.532Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:02:50.532Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:02:50.540Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:03:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:03:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:03:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:03:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T00:03:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:03:50.373Z"
}
[2025-10-26T00:03:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T00:03:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:03:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:03:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:03:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:04:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:04:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:04:50.443Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:04:50.443Z] [LOG] [API] Checking server status...
[2025-10-26T00:04:50.451Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:04:50.444Z"
}
[2025-10-26T00:04:50.451Z] [LOG] [API] WebGPU available
[2025-10-26T00:04:50.455Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:04:50.455Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:04:50.455Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:04:50.466Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:05:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:05:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:05:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:05:50.385Z] [LOG] [API] Checking server status...
[2025-10-26T00:05:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:05:50.387Z"
}
[2025-10-26T00:05:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T00:05:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:05:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:05:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:05:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:06:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:06:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:06:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:06:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T00:06:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:06:50.399Z"
}
[2025-10-26T00:06:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T00:06:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:06:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:06:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:06:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:07:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:07:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:07:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:07:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T00:07:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:07:50.393Z"
}
[2025-10-26T00:07:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T00:07:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:07:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:07:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:07:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:08:50.512Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:08:50.512Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:08:50.514Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:08:50.514Z] [LOG] [API] Checking server status...
[2025-10-26T00:08:50.525Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:08:50.516Z"
}
[2025-10-26T00:08:50.525Z] [LOG] [API] WebGPU available
[2025-10-26T00:08:50.532Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:08:50.532Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:08:50.532Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:08:50.538Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:09:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:09:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:09:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:09:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T00:09:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:09:50.370Z"
}
[2025-10-26T00:09:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T00:09:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:09:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:09:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:09:50.394Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:10:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:10:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:10:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:10:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T00:10:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:10:50.393Z"
}
[2025-10-26T00:10:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T00:10:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:10:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:10:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:10:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:11:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:11:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:11:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:11:50.385Z] [LOG] [API] Checking server status...
[2025-10-26T00:11:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:11:50.387Z"
}
[2025-10-26T00:11:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T00:11:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:11:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:11:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:11:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:12:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:12:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:12:50.418Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:12:50.418Z] [LOG] [API] Checking server status...
[2025-10-26T00:12:50.444Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:12:50.419Z"
}
[2025-10-26T00:12:50.444Z] [LOG] [API] WebGPU available
[2025-10-26T00:12:50.451Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:12:50.451Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:12:50.451Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:12:50.458Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:13:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:13:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:13:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:13:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T00:13:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:13:50.379Z"
}
[2025-10-26T00:13:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T00:13:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:13:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:13:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:13:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:14:50.492Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:14:50.492Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:14:50.494Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:14:50.494Z] [LOG] [API] Checking server status...
[2025-10-26T00:14:50.506Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:14:50.495Z"
}
[2025-10-26T00:14:50.506Z] [LOG] [API] WebGPU available
[2025-10-26T00:14:50.510Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:14:50.510Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:14:50.510Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:14:50.518Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:15:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:15:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:15:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:15:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T00:15:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:15:50.372Z"
}
[2025-10-26T00:15:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T00:15:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:15:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:15:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:15:50.396Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:16:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:16:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:16:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:16:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T00:16:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:16:50.372Z"
}
[2025-10-26T00:16:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T00:16:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:16:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:16:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:16:50.391Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:17:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:17:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:17:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:17:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T00:17:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:17:50.370Z"
}
[2025-10-26T00:17:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T00:17:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:17:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:17:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:17:50.397Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:18:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:18:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:18:50.430Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:18:50.430Z] [LOG] [API] Checking server status...
[2025-10-26T00:18:50.441Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:18:50.432Z"
}
[2025-10-26T00:18:50.441Z] [LOG] [API] WebGPU available
[2025-10-26T00:18:50.444Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:18:50.444Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:18:50.444Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:18:50.461Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:19:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:19:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:19:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:19:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T00:19:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:19:50.385Z"
}
[2025-10-26T00:19:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T00:19:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:19:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:19:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:19:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:20:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:20:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:20:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:20:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T00:20:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:20:50.406Z"
}
[2025-10-26T00:20:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T00:20:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:20:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:20:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:20:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:21:50.335Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:21:50.335Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:21:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:21:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T00:21:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:21:50.395Z"
}
[2025-10-26T00:21:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T00:21:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:21:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:21:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:21:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:22:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:22:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:22:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:22:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T00:22:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:22:50.389Z"
}
[2025-10-26T00:22:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T00:22:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:22:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:22:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:22:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:23:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:23:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:23:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:23:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T00:23:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:23:50.378Z"
}
[2025-10-26T00:23:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T00:23:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:23:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:23:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:23:50.401Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:24:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:24:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:24:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:24:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T00:24:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:24:50.388Z"
}
[2025-10-26T00:24:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T00:24:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:24:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:24:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:24:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:25:50.409Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:25:50.410Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:25:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:25:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T00:25:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:25:50.414Z"
}
[2025-10-26T00:25:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T00:25:50.428Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:25:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:25:50.429Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:25:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:26:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:26:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:26:50.418Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:26:50.418Z] [LOG] [API] Checking server status...
[2025-10-26T00:26:50.430Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:26:50.419Z"
}
[2025-10-26T00:26:50.430Z] [LOG] [API] WebGPU available
[2025-10-26T00:26:50.434Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:26:50.434Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:26:50.434Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:26:50.449Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:27:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:27:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:27:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:27:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T00:27:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:27:50.404Z"
}
[2025-10-26T00:27:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T00:27:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:27:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:27:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:27:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:28:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:28:50.389Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:28:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:28:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T00:28:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:28:50.395Z"
}
[2025-10-26T00:28:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T00:28:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:28:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:28:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:28:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:29:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:29:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:29:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:29:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T00:29:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:29:50.391Z"
}
[2025-10-26T00:29:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T00:29:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:29:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:29:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:29:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:30:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:30:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:30:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:30:50.415Z] [LOG] [API] Checking server status...
[2025-10-26T00:30:50.427Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:30:50.418Z"
}
[2025-10-26T00:30:50.427Z] [LOG] [API] WebGPU available
[2025-10-26T00:30:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:30:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:30:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:30:50.457Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:31:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:31:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:31:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:31:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T00:31:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:31:50.391Z"
}
[2025-10-26T00:31:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T00:31:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:31:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:31:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:31:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:32:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:32:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:32:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:32:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T00:32:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:32:50.387Z"
}
[2025-10-26T00:32:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T00:32:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:32:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:32:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:32:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:33:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:33:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:33:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:33:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T00:33:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:33:50.393Z"
}
[2025-10-26T00:33:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T00:33:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:33:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:33:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:33:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:34:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:34:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:34:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:34:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T00:34:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:34:50.403Z"
}
[2025-10-26T00:34:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T00:34:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:34:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:34:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:34:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:35:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:35:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:35:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:35:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T00:35:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:35:50.389Z"
}
[2025-10-26T00:35:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T00:35:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:35:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:35:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:35:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:36:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:36:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:36:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:36:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T00:36:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:36:50.399Z"
}
[2025-10-26T00:36:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T00:36:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:36:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:36:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:36:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:37:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:37:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:37:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:37:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T00:37:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:37:50.383Z"
}
[2025-10-26T00:37:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T00:37:50.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:37:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:37:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:37:50.494Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:38:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:38:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:38:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:38:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T00:38:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:38:50.401Z"
}
[2025-10-26T00:38:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T00:38:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:38:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:38:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:38:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:39:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:39:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:39:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:39:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T00:39:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:39:50.384Z"
}
[2025-10-26T00:39:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T00:39:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:39:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:39:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:39:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:40:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:40:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:40:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:40:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T00:40:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:40:50.396Z"
}
[2025-10-26T00:40:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T00:40:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:40:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:40:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:40:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:41:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:41:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:41:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:41:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T00:41:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:41:50.390Z"
}
[2025-10-26T00:41:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T00:41:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:41:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:41:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:41:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:42:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:42:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:42:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:42:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T00:42:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:42:50.398Z"
}
[2025-10-26T00:42:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T00:42:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:42:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:42:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:42:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:43:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:43:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:43:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:43:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T00:43:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:43:50.383Z"
}
[2025-10-26T00:43:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T00:43:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:43:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:43:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:43:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:44:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:44:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:44:50.416Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:44:50.416Z] [LOG] [API] Checking server status...
[2025-10-26T00:44:50.427Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:44:50.419Z"
}
[2025-10-26T00:44:50.427Z] [LOG] [API] WebGPU available
[2025-10-26T00:44:50.434Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:44:50.434Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:44:50.434Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:44:50.465Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:45:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:45:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:45:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:45:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T00:45:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:45:50.382Z"
}
[2025-10-26T00:45:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T00:45:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:45:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:45:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:45:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:46:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:46:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:46:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:46:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T00:46:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:46:50.409Z"
}
[2025-10-26T00:46:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T00:46:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:46:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:46:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:46:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:47:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:47:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:47:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:47:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T00:47:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:47:50.392Z"
}
[2025-10-26T00:47:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T00:47:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:47:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:47:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:47:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:48:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:48:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:48:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:48:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T00:48:50.431Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:48:50.423Z"
}
[2025-10-26T00:48:50.432Z] [LOG] [API] WebGPU available
[2025-10-26T00:48:50.437Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:48:50.438Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:48:50.438Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:48:50.471Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:49:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:49:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:49:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:49:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T00:49:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:49:50.379Z"
}
[2025-10-26T00:49:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T00:49:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:49:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:49:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:49:50.401Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:50:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:50:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:50:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:50:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T00:50:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:50:50.398Z"
}
[2025-10-26T00:50:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T00:50:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:50:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:50:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:50:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:51:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:51:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:51:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:51:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T00:51:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:51:50.389Z"
}
[2025-10-26T00:51:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T00:51:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:51:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:51:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:51:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:52:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:52:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:52:50.463Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:52:50.463Z] [LOG] [API] Checking server status...
[2025-10-26T00:52:50.473Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:52:50.465Z"
}
[2025-10-26T00:52:50.473Z] [LOG] [API] WebGPU available
[2025-10-26T00:52:50.476Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:52:50.476Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:52:50.476Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:52:50.502Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:53:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:53:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:53:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:53:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T00:53:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:53:50.378Z"
}
[2025-10-26T00:53:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T00:53:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:53:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:53:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:53:50.398Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:54:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:54:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:54:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:54:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T00:54:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:54:50.405Z"
}
[2025-10-26T00:54:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T00:54:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:54:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:54:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:54:50.431Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:55:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:55:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:55:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:55:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T00:55:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:55:50.392Z"
}
[2025-10-26T00:55:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T00:55:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:55:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:55:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:55:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:56:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:56:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:56:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:56:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T00:56:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:56:50.393Z"
}
[2025-10-26T00:56:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T00:56:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:56:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:56:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:56:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:57:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:57:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:57:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:57:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T00:57:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:57:50.391Z"
}
[2025-10-26T00:57:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T00:57:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:57:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:57:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:57:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:58:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:58:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:58:50.416Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:58:50.416Z] [LOG] [API] Checking server status...
[2025-10-26T00:58:50.425Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:58:50.417Z"
}
[2025-10-26T00:58:50.425Z] [LOG] [API] WebGPU available
[2025-10-26T00:58:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:58:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:58:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:58:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T00:59:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T00:59:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T00:59:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T00:59:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T00:59:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T00:59:50.388Z"
}
[2025-10-26T00:59:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T00:59:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T00:59:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T00:59:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T00:59:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:00:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:00:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:00:50.446Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:00:50.446Z] [LOG] [API] Checking server status...
[2025-10-26T01:00:50.478Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:00:50.448Z"
}
[2025-10-26T01:00:50.478Z] [LOG] [API] WebGPU available
[2025-10-26T01:00:50.483Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:00:50.483Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:00:50.483Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:00:50.500Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:01:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:01:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:01:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:01:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T01:01:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:01:50.402Z"
}
[2025-10-26T01:01:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T01:01:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:01:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:01:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:01:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:02:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:02:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:02:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:02:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T01:02:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:02:50.404Z"
}
[2025-10-26T01:02:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T01:02:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:02:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:02:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:02:50.452Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:03:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:03:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:03:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:03:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T01:03:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:03:50.404Z"
}
[2025-10-26T01:03:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T01:03:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:03:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:03:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:03:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:04:50.403Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:04:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:04:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:04:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T01:04:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:04:50.409Z"
}
[2025-10-26T01:04:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T01:04:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:04:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:04:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:04:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:05:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:05:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:05:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:05:50.385Z] [LOG] [API] Checking server status...
[2025-10-26T01:05:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:05:50.386Z"
}
[2025-10-26T01:05:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T01:05:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:05:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:05:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:05:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:06:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:06:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:06:50.456Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:06:50.456Z] [LOG] [API] Checking server status...
[2025-10-26T01:06:50.466Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:06:50.457Z"
}
[2025-10-26T01:06:50.466Z] [LOG] [API] WebGPU available
[2025-10-26T01:06:50.468Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:06:50.468Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:06:50.468Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:06:50.476Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:07:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:07:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:07:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:07:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T01:07:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:07:50.404Z"
}
[2025-10-26T01:07:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T01:07:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:07:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:07:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:07:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:08:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:08:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:08:50.424Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:08:50.424Z] [LOG] [API] Checking server status...
[2025-10-26T01:08:50.433Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:08:50.425Z"
}
[2025-10-26T01:08:50.434Z] [LOG] [API] WebGPU available
[2025-10-26T01:08:50.441Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:08:50.441Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:08:50.441Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:08:50.466Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:09:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:09:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:09:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:09:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T01:09:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:09:50.383Z"
}
[2025-10-26T01:09:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T01:09:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:09:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:09:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:09:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:10:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:10:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:10:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:10:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T01:10:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:10:50.416Z"
}
[2025-10-26T01:10:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T01:10:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:10:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:10:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:10:50.478Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:11:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:11:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:11:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:11:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T01:11:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:11:50.381Z"
}
[2025-10-26T01:11:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T01:11:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:11:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:11:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:11:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:12:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:12:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:12:50.450Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:12:50.450Z] [LOG] [API] Checking server status...
[2025-10-26T01:12:50.475Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:12:50.451Z"
}
[2025-10-26T01:12:50.476Z] [LOG] [API] WebGPU available
[2025-10-26T01:12:50.482Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:12:50.482Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:12:50.482Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:12:50.487Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:13:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:13:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:13:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:13:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T01:13:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:13:50.390Z"
}
[2025-10-26T01:13:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T01:13:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:13:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:13:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:13:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:14:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:14:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:14:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:14:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T01:14:50.424Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:14:50.413Z"
}
[2025-10-26T01:14:50.424Z] [LOG] [API] WebGPU available
[2025-10-26T01:14:50.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:14:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:14:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:14:50.476Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:15:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:15:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:15:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:15:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T01:15:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:15:50.391Z"
}
[2025-10-26T01:15:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T01:15:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:15:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:15:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:15:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:16:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:16:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:16:50.443Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:16:50.444Z] [LOG] [API] Checking server status...
[2025-10-26T01:16:50.456Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:16:50.445Z"
}
[2025-10-26T01:16:50.456Z] [LOG] [API] WebGPU available
[2025-10-26T01:16:50.487Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:16:50.487Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:16:50.487Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:16:50.509Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:17:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:17:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:17:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:17:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T01:17:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:17:50.385Z"
}
[2025-10-26T01:17:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T01:17:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:17:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:17:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:17:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:18:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:18:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:18:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:18:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T01:18:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:18:50.412Z"
}
[2025-10-26T01:18:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T01:18:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:18:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:18:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:18:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:19:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:19:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:19:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:19:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T01:19:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:19:50.392Z"
}
[2025-10-26T01:19:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T01:19:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:19:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:19:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:19:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:20:50.586Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:20:50.586Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:20:50.589Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:20:50.589Z] [LOG] [API] Checking server status...
[2025-10-26T01:20:50.602Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:20:50.590Z"
}
[2025-10-26T01:20:50.602Z] [LOG] [API] WebGPU available
[2025-10-26T01:20:50.610Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:20:50.610Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:20:50.611Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:20:50.626Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:21:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:21:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:21:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:21:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T01:21:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:21:50.391Z"
}
[2025-10-26T01:21:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T01:21:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:21:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:21:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:21:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:22:50.394Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:22:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:22:50.416Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:22:50.417Z] [LOG] [API] Checking server status...
[2025-10-26T01:22:50.428Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:22:50.420Z"
}
[2025-10-26T01:22:50.428Z] [LOG] [API] WebGPU available
[2025-10-26T01:22:50.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:22:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:22:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:22:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:23:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:23:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:23:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:23:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T01:23:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:23:50.391Z"
}
[2025-10-26T01:23:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T01:23:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:23:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:23:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:23:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:24:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:24:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:24:50.420Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:24:50.420Z] [LOG] [API] Checking server status...
[2025-10-26T01:24:50.429Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:24:50.421Z"
}
[2025-10-26T01:24:50.430Z] [LOG] [API] WebGPU available
[2025-10-26T01:24:50.434Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:24:50.434Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:24:50.434Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:24:50.470Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:25:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:25:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:25:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:25:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T01:25:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:25:50.383Z"
}
[2025-10-26T01:25:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T01:25:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:25:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:25:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:25:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:26:50.537Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:26:50.537Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:26:50.539Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:26:50.539Z] [LOG] [API] Checking server status...
[2025-10-26T01:26:50.548Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:26:50.540Z"
}
[2025-10-26T01:26:50.548Z] [LOG] [API] WebGPU available
[2025-10-26T01:26:50.555Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:26:50.555Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:26:50.555Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:26:50.562Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:27:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:27:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:27:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:27:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T01:27:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:27:50.398Z"
}
[2025-10-26T01:27:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T01:27:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:27:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:27:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:27:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:28:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:28:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:28:50.420Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:28:50.420Z] [LOG] [API] Checking server status...
[2025-10-26T01:28:50.431Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:28:50.421Z"
}
[2025-10-26T01:28:50.431Z] [LOG] [API] WebGPU available
[2025-10-26T01:28:50.442Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:28:50.442Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:28:50.442Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:28:50.469Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:29:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:29:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:29:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:29:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T01:29:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:29:50.400Z"
}
[2025-10-26T01:29:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T01:29:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:29:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:29:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:29:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:30:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:30:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:30:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:30:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T01:30:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:30:50.412Z"
}
[2025-10-26T01:30:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T01:30:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:30:50.429Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:30:50.429Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:30:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:31:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:31:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:31:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:31:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T01:31:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:31:50.395Z"
}
[2025-10-26T01:31:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T01:31:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:31:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:31:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:31:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:32:50.532Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:32:50.532Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:32:50.534Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:32:50.534Z] [LOG] [API] Checking server status...
[2025-10-26T01:32:50.543Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:32:50.535Z"
}
[2025-10-26T01:32:50.544Z] [LOG] [API] WebGPU available
[2025-10-26T01:32:50.547Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:32:50.547Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:32:50.547Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:32:50.563Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:33:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:33:50.389Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:33:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:33:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T01:33:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:33:50.394Z"
}
[2025-10-26T01:33:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T01:33:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:33:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:33:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:33:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:34:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:34:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:34:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:34:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T01:34:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:34:50.400Z"
}
[2025-10-26T01:34:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T01:34:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:34:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:34:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:34:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:35:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:35:50.397Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:35:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:35:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T01:35:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:35:50.403Z"
}
[2025-10-26T01:35:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T01:35:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:35:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:35:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:35:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:36:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:36:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:36:50.477Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:36:50.477Z] [LOG] [API] Checking server status...
[2025-10-26T01:36:50.484Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:36:50.478Z"
}
[2025-10-26T01:36:50.485Z] [LOG] [API] WebGPU available
[2025-10-26T01:36:50.487Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:36:50.487Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:36:50.487Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:36:50.491Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:37:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:37:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:37:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:37:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T01:37:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:37:50.400Z"
}
[2025-10-26T01:37:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T01:37:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:37:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:37:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:37:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:38:50.394Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:38:50.394Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:38:50.432Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:38:50.432Z] [LOG] [API] Checking server status...
[2025-10-26T01:38:50.442Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:38:50.433Z"
}
[2025-10-26T01:38:50.442Z] [LOG] [API] WebGPU available
[2025-10-26T01:38:50.451Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:38:50.451Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:38:50.451Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:38:50.479Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:39:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:39:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:39:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:39:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T01:39:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:39:50.399Z"
}
[2025-10-26T01:39:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T01:39:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:39:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:39:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:39:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:40:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:40:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:40:50.432Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:40:50.432Z] [LOG] [API] Checking server status...
[2025-10-26T01:40:50.442Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:40:50.434Z"
}
[2025-10-26T01:40:50.442Z] [LOG] [API] WebGPU available
[2025-10-26T01:40:50.452Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:40:50.452Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:40:50.452Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:40:50.483Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:41:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:41:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:41:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:41:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T01:41:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:41:50.386Z"
}
[2025-10-26T01:41:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T01:41:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:41:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:41:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:41:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:42:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:42:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:42:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:42:50.416Z] [LOG] [API] Checking server status...
[2025-10-26T01:42:50.427Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:42:50.420Z"
}
[2025-10-26T01:42:50.427Z] [LOG] [API] WebGPU available
[2025-10-26T01:42:50.431Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:42:50.431Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:42:50.431Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:42:50.449Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:43:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:43:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:43:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:43:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T01:43:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:43:50.390Z"
}
[2025-10-26T01:43:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T01:43:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:43:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:43:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:43:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:44:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:44:50.389Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:44:50.459Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:44:50.459Z] [LOG] [API] Checking server status...
[2025-10-26T01:44:50.471Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:44:50.461Z"
}
[2025-10-26T01:44:50.471Z] [LOG] [API] WebGPU available
[2025-10-26T01:44:50.480Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:44:50.480Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:44:50.480Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:44:50.499Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:45:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:45:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:45:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:45:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T01:45:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:45:50.386Z"
}
[2025-10-26T01:45:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T01:45:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:45:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:45:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:45:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:46:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:46:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:46:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:46:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T01:46:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:46:50.407Z"
}
[2025-10-26T01:46:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T01:46:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:46:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:46:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:46:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:47:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:47:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:47:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:47:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T01:47:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:47:50.386Z"
}
[2025-10-26T01:47:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T01:47:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:47:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:47:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:47:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:48:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:48:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:48:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:48:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T01:48:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:48:50.399Z"
}
[2025-10-26T01:48:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T01:48:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:48:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:48:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:48:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:49:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:49:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:49:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:49:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T01:49:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:49:50.389Z"
}
[2025-10-26T01:49:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T01:49:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:49:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:49:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:49:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:50:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:50:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:50:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:50:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T01:50:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:50:50.404Z"
}
[2025-10-26T01:50:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T01:50:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:50:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:50:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:50:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:51:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:51:50.389Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:51:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:51:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T01:51:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:51:50.394Z"
}
[2025-10-26T01:51:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T01:51:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:51:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:51:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:51:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:52:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:52:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:52:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:52:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T01:52:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:52:50.399Z"
}
[2025-10-26T01:52:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T01:52:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:52:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:52:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:52:50.431Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:53:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:53:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:53:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:53:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T01:53:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:53:50.398Z"
}
[2025-10-26T01:53:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T01:53:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:53:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:53:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:53:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:54:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:54:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:54:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:54:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T01:54:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:54:50.389Z"
}
[2025-10-26T01:54:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T01:54:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:54:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:54:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:54:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:55:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:55:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:55:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:55:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T01:55:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:55:50.390Z"
}
[2025-10-26T01:55:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T01:55:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:55:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:55:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:55:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:56:50.551Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:56:50.551Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:56:50.553Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:56:50.553Z] [LOG] [API] Checking server status...
[2025-10-26T01:56:50.562Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:56:50.554Z"
}
[2025-10-26T01:56:50.562Z] [LOG] [API] WebGPU available
[2025-10-26T01:56:50.565Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:56:50.565Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:56:50.565Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:56:50.572Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:57:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:57:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:57:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:57:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T01:57:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:57:50.388Z"
}
[2025-10-26T01:57:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T01:57:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:57:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:57:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:57:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:58:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:58:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:58:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:58:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T01:58:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:58:50.395Z"
}
[2025-10-26T01:58:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T01:58:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:58:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:58:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:58:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T01:59:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T01:59:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T01:59:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T01:59:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T01:59:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T01:59:50.393Z"
}
[2025-10-26T01:59:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T01:59:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T01:59:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T01:59:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T01:59:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:00:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:00:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:00:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:00:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T02:00:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:00:50.391Z"
}
[2025-10-26T02:00:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T02:00:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:00:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:00:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:00:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:01:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:01:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:01:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:01:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T02:01:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:01:50.398Z"
}
[2025-10-26T02:01:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T02:01:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:01:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:01:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:01:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:02:50.499Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:02:50.499Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:02:50.501Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:02:50.501Z] [LOG] [API] Checking server status...
[2025-10-26T02:02:50.511Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:02:50.503Z"
}
[2025-10-26T02:02:50.511Z] [LOG] [API] WebGPU available
[2025-10-26T02:02:50.513Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:02:50.513Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:02:50.513Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:02:50.523Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:03:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:03:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:03:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:03:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T02:03:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:03:50.390Z"
}
[2025-10-26T02:03:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T02:03:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:03:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:03:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:03:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:04:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:04:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:04:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:04:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T02:04:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:04:50.391Z"
}
[2025-10-26T02:04:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T02:04:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:04:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:04:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:04:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:05:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:05:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:05:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:05:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T02:05:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:05:50.392Z"
}
[2025-10-26T02:05:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T02:05:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:05:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:05:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:05:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:06:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:06:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:06:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:06:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T02:06:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:06:50.389Z"
}
[2025-10-26T02:06:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T02:06:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:06:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:06:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:06:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:07:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:07:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:07:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:07:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T02:07:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:07:50.393Z"
}
[2025-10-26T02:07:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T02:07:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:07:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:07:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:07:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:08:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:08:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:08:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:08:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T02:08:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:08:50.398Z"
}
[2025-10-26T02:08:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T02:08:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:08:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:08:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:08:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:09:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:09:50.389Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:09:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:09:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T02:09:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:09:50.394Z"
}
[2025-10-26T02:09:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T02:09:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:09:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:09:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:09:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:10:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:10:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:10:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:10:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T02:10:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:10:50.392Z"
}
[2025-10-26T02:10:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T02:10:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:10:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:10:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:10:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:11:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:11:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:11:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:11:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T02:11:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:11:50.386Z"
}
[2025-10-26T02:11:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T02:11:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:11:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:11:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:11:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:12:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:12:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:12:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:12:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T02:12:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:12:50.385Z"
}
[2025-10-26T02:12:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T02:12:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:12:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:12:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:12:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:13:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:13:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:13:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:13:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T02:13:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:13:50.389Z"
}
[2025-10-26T02:13:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T02:13:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:13:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:13:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:13:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:14:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:14:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:14:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:14:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T02:14:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:14:50.409Z"
}
[2025-10-26T02:14:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T02:14:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:14:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:14:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:14:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:15:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:15:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:15:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:15:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T02:15:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:15:50.408Z"
}
[2025-10-26T02:15:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T02:15:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:15:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:15:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:15:50.455Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:16:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:16:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:16:50.419Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:16:50.419Z] [LOG] [API] Checking server status...
[2025-10-26T02:16:50.428Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:16:50.420Z"
}
[2025-10-26T02:16:50.428Z] [LOG] [API] WebGPU available
[2025-10-26T02:16:50.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:16:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:16:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:16:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:17:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:17:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:17:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:17:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T02:17:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:17:50.397Z"
}
[2025-10-26T02:17:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T02:17:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:17:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:17:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:17:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:18:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:18:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:18:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:18:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T02:18:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:18:50.394Z"
}
[2025-10-26T02:18:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T02:18:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:18:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:18:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:18:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:19:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:19:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:19:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:19:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T02:19:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:19:50.385Z"
}
[2025-10-26T02:19:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T02:19:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:19:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:19:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:19:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:20:50.518Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:20:50.518Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:20:50.520Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:20:50.520Z] [LOG] [API] Checking server status...
[2025-10-26T02:20:50.529Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:20:50.522Z"
}
[2025-10-26T02:20:50.530Z] [LOG] [API] WebGPU available
[2025-10-26T02:20:50.535Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:20:50.535Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:20:50.536Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:20:50.540Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:21:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:21:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:21:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:21:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T02:21:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:21:50.389Z"
}
[2025-10-26T02:21:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T02:21:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:21:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:21:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:21:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:22:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:22:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:22:50.443Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:22:50.444Z] [LOG] [API] Checking server status...
[2025-10-26T02:22:50.454Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:22:50.445Z"
}
[2025-10-26T02:22:50.454Z] [LOG] [API] WebGPU available
[2025-10-26T02:22:50.480Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:22:50.480Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:22:50.480Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:22:50.508Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:23:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:23:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:23:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:23:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T02:23:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:23:50.397Z"
}
[2025-10-26T02:23:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T02:23:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:23:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:23:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:23:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:24:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:24:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:24:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:24:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T02:24:50.424Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:24:50.414Z"
}
[2025-10-26T02:24:50.424Z] [LOG] [API] WebGPU available
[2025-10-26T02:24:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:24:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:24:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:24:50.460Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:25:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:25:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:25:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:25:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T02:25:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:25:50.391Z"
}
[2025-10-26T02:25:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T02:25:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:25:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:25:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:25:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:26:50.512Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:26:50.513Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:26:50.515Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:26:50.515Z] [LOG] [API] Checking server status...
[2025-10-26T02:26:50.527Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:26:50.516Z"
}
[2025-10-26T02:26:50.527Z] [LOG] [API] WebGPU available
[2025-10-26T02:26:50.530Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:26:50.530Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:26:50.530Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:26:50.537Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:27:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:27:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:27:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:27:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T02:27:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:27:50.384Z"
}
[2025-10-26T02:27:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T02:27:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:27:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:27:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:27:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:28:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:28:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:28:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:28:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T02:28:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:28:50.409Z"
}
[2025-10-26T02:28:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T02:28:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:28:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:28:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:28:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:29:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:29:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:29:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:29:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T02:29:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:29:50.389Z"
}
[2025-10-26T02:29:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T02:29:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:29:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:29:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:29:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:30:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:30:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:30:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:30:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T02:30:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:30:50.384Z"
}
[2025-10-26T02:30:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T02:30:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:30:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:30:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:30:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:31:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:31:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:31:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:31:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T02:31:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:31:50.383Z"
}
[2025-10-26T02:31:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T02:31:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:31:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:31:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:31:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:32:50.522Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:32:50.522Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:32:50.524Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:32:50.524Z] [LOG] [API] Checking server status...
[2025-10-26T02:32:50.532Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:32:50.525Z"
}
[2025-10-26T02:32:50.533Z] [LOG] [API] WebGPU available
[2025-10-26T02:32:50.536Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:32:50.536Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:32:50.536Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:32:50.544Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:33:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:33:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:33:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:33:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T02:33:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:33:50.397Z"
}
[2025-10-26T02:33:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T02:33:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:33:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:33:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:33:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:34:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:34:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:34:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:34:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T02:34:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:34:50.384Z"
}
[2025-10-26T02:34:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T02:34:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:34:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:34:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:34:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:35:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:35:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:35:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:35:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T02:35:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:35:50.389Z"
}
[2025-10-26T02:35:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T02:35:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:35:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:35:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:35:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:36:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:36:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:36:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:36:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T02:36:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:36:50.384Z"
}
[2025-10-26T02:36:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T02:36:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:36:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:36:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:36:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:37:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:37:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:37:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:37:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T02:37:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:37:50.407Z"
}
[2025-10-26T02:37:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T02:37:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:37:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:37:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:37:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:38:50.537Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:38:50.537Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:38:50.539Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:38:50.539Z] [LOG] [API] Checking server status...
[2025-10-26T02:38:50.549Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:38:50.541Z"
}
[2025-10-26T02:38:50.550Z] [LOG] [API] WebGPU available
[2025-10-26T02:38:50.553Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:38:50.553Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:38:50.553Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:38:50.561Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:39:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:39:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:39:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:39:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T02:39:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:39:50.390Z"
}
[2025-10-26T02:39:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T02:39:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:39:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:39:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:39:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:40:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:40:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:40:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:40:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T02:40:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:40:50.388Z"
}
[2025-10-26T02:40:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T02:40:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:40:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:40:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:40:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:41:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:41:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:41:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:41:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T02:41:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:41:50.390Z"
}
[2025-10-26T02:41:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T02:41:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:41:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:41:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:41:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:42:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:42:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:42:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:42:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T02:42:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:42:50.391Z"
}
[2025-10-26T02:42:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T02:42:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:42:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:42:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:42:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:43:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:43:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:43:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:43:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T02:43:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:43:50.384Z"
}
[2025-10-26T02:43:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T02:43:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:43:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:43:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:43:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:44:50.578Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:44:50.578Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:44:50.584Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:44:50.584Z] [LOG] [API] Checking server status...
[2025-10-26T02:44:50.592Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:44:50.585Z"
}
[2025-10-26T02:44:50.593Z] [LOG] [API] WebGPU available
[2025-10-26T02:44:50.599Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:44:50.599Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:44:50.599Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:44:50.610Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:45:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:45:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:45:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:45:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T02:45:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:45:50.382Z"
}
[2025-10-26T02:45:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T02:45:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:45:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:45:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:45:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:46:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:46:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:46:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:46:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T02:46:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:46:50.413Z"
}
[2025-10-26T02:46:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T02:46:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:46:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:46:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:46:50.435Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:47:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:47:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:47:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:47:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T02:47:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:47:50.391Z"
}
[2025-10-26T02:47:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T02:47:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:47:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:47:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:47:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:48:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:48:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:48:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:48:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T02:48:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:48:50.387Z"
}
[2025-10-26T02:48:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T02:48:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:48:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:48:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:48:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:49:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:49:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:49:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:49:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T02:49:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:49:50.383Z"
}
[2025-10-26T02:49:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T02:49:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:49:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:49:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:49:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:50:50.496Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:50:50.496Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:50:50.499Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:50:50.499Z] [LOG] [API] Checking server status...
[2025-10-26T02:50:50.508Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:50:50.500Z"
}
[2025-10-26T02:50:50.508Z] [LOG] [API] WebGPU available
[2025-10-26T02:50:50.512Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:50:50.512Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:50:50.512Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:50:50.526Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:51:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:51:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:51:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:51:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T02:51:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:51:50.398Z"
}
[2025-10-26T02:51:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T02:51:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:51:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:51:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:51:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:52:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:52:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:52:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:52:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T02:52:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:52:50.385Z"
}
[2025-10-26T02:52:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T02:52:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:52:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:52:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:52:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:53:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:53:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:53:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:53:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T02:53:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:53:50.395Z"
}
[2025-10-26T02:53:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T02:53:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:53:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:53:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:53:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:54:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:54:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:54:50.475Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:54:50.475Z] [LOG] [API] Checking server status...
[2025-10-26T02:54:50.485Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:54:50.476Z"
}
[2025-10-26T02:54:50.485Z] [LOG] [API] WebGPU available
[2025-10-26T02:54:50.492Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:54:50.492Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:54:50.492Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:54:50.499Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:55:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:55:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:55:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:55:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T02:55:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:55:50.397Z"
}
[2025-10-26T02:55:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T02:55:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:55:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:55:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:55:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:56:50.478Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:56:50.479Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:56:50.481Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:56:50.481Z] [LOG] [API] Checking server status...
[2025-10-26T02:56:50.490Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:56:50.482Z"
}
[2025-10-26T02:56:50.490Z] [LOG] [API] WebGPU available
[2025-10-26T02:56:50.497Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:56:50.497Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:56:50.497Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:56:50.503Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:57:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:57:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:57:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:57:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T02:57:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:57:50.392Z"
}
[2025-10-26T02:57:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T02:57:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:57:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:57:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:57:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:58:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:58:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:58:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:58:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T02:58:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:58:50.391Z"
}
[2025-10-26T02:58:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T02:58:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:58:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:58:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:58:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T02:59:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T02:59:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T02:59:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T02:59:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T02:59:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T02:59:50.383Z"
}
[2025-10-26T02:59:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T02:59:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T02:59:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T02:59:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T02:59:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:00:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:00:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:00:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:00:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T03:00:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:00:50.391Z"
}
[2025-10-26T03:00:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T03:00:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:00:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:00:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:00:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:01:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:01:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:01:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:01:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T03:01:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:01:50.383Z"
}
[2025-10-26T03:01:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T03:01:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:01:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:01:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:01:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:02:50.589Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:02:50.590Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:02:50.592Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:02:50.592Z] [LOG] [API] Checking server status...
[2025-10-26T03:02:50.601Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:02:50.593Z"
}
[2025-10-26T03:02:50.601Z] [LOG] [API] WebGPU available
[2025-10-26T03:02:50.607Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:02:50.607Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:02:50.607Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:02:50.613Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:03:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:03:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:03:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:03:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T03:03:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:03:50.398Z"
}
[2025-10-26T03:03:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T03:03:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:03:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:03:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:03:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:04:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:04:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:04:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:04:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T03:04:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:04:50.402Z"
}
[2025-10-26T03:04:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T03:04:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:04:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:04:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:04:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:05:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:05:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:05:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:05:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T03:05:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:05:50.403Z"
}
[2025-10-26T03:05:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T03:05:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:05:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:05:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:05:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:06:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:06:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:06:50.435Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:06:50.436Z] [LOG] [API] Checking server status...
[2025-10-26T03:06:50.445Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:06:50.437Z"
}
[2025-10-26T03:06:50.445Z] [LOG] [API] WebGPU available
[2025-10-26T03:06:50.451Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:06:50.451Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:06:50.451Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:06:50.457Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:07:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:07:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:07:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:07:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T03:07:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:07:50.389Z"
}
[2025-10-26T03:07:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T03:07:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:07:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:07:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:07:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:08:50.518Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:08:50.518Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:08:50.521Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:08:50.521Z] [LOG] [API] Checking server status...
[2025-10-26T03:08:50.532Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:08:50.523Z"
}
[2025-10-26T03:08:50.532Z] [LOG] [API] WebGPU available
[2025-10-26T03:08:50.539Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:08:50.539Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:08:50.539Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:08:50.551Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:09:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:09:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:09:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:09:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T03:09:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:09:50.405Z"
}
[2025-10-26T03:09:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T03:09:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:09:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:09:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:09:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:10:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:10:50.389Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:10:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:10:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T03:10:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:10:50.394Z"
}
[2025-10-26T03:10:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T03:10:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:10:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:10:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:10:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:11:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:11:50.389Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:11:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:11:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T03:11:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:11:50.395Z"
}
[2025-10-26T03:11:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T03:11:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:11:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:11:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:11:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:12:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:12:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:12:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:12:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T03:12:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:12:50.385Z"
}
[2025-10-26T03:12:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T03:12:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:12:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:12:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:12:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:13:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:13:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:13:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:13:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T03:13:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:13:50.382Z"
}
[2025-10-26T03:13:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T03:13:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:13:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:13:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:13:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:14:50.497Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:14:50.497Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:14:50.499Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:14:50.500Z] [LOG] [API] Checking server status...
[2025-10-26T03:14:50.510Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:14:50.501Z"
}
[2025-10-26T03:14:50.510Z] [LOG] [API] WebGPU available
[2025-10-26T03:14:50.516Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:14:50.516Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:14:50.516Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:14:50.526Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:15:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:15:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:15:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:15:50.385Z] [LOG] [API] Checking server status...
[2025-10-26T03:15:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:15:50.386Z"
}
[2025-10-26T03:15:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T03:15:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:15:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:15:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:15:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:16:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:16:50.389Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:16:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:16:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T03:16:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:16:50.403Z"
}
[2025-10-26T03:16:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T03:16:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:16:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:16:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:16:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:17:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:17:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:17:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:17:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T03:17:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:17:50.391Z"
}
[2025-10-26T03:17:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T03:17:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:17:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:17:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:17:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:18:50.400Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:18:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:18:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:18:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T03:18:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:18:50.414Z"
}
[2025-10-26T03:18:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T03:18:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:18:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:18:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:18:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:19:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:19:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:19:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:19:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T03:19:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:19:50.393Z"
}
[2025-10-26T03:19:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T03:19:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:19:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:19:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:19:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:20:50.514Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:20:50.514Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:20:50.516Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:20:50.516Z] [LOG] [API] Checking server status...
[2025-10-26T03:20:50.524Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:20:50.517Z"
}
[2025-10-26T03:20:50.524Z] [LOG] [API] WebGPU available
[2025-10-26T03:20:50.527Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:20:50.527Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:20:50.527Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:20:50.535Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:21:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:21:50.397Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:21:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:21:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T03:21:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:21:50.401Z"
}
[2025-10-26T03:21:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T03:21:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:21:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:21:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:21:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:22:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:22:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:22:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:22:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T03:22:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:22:50.397Z"
}
[2025-10-26T03:22:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T03:22:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:22:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:22:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:22:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:23:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:23:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:23:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:23:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T03:23:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:23:50.408Z"
}
[2025-10-26T03:23:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T03:23:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:23:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:23:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:23:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:24:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:24:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:24:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:24:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T03:24:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:24:50.397Z"
}
[2025-10-26T03:24:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T03:24:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:24:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:24:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:24:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:25:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:25:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:25:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:25:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T03:25:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:25:50.396Z"
}
[2025-10-26T03:25:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T03:25:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:25:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:25:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:25:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:26:50.564Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:26:50.564Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:26:50.567Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:26:50.567Z] [LOG] [API] Checking server status...
[2025-10-26T03:26:50.575Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:26:50.568Z"
}
[2025-10-26T03:26:50.575Z] [LOG] [API] WebGPU available
[2025-10-26T03:26:50.578Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:26:50.579Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:26:50.579Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:26:50.586Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:27:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:27:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:27:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:27:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T03:27:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:27:50.391Z"
}
[2025-10-26T03:27:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T03:27:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:27:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:27:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:27:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:28:50.389Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:28:50.389Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:28:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:28:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T03:28:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:28:50.395Z"
}
[2025-10-26T03:28:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T03:28:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:28:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:28:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:28:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:29:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:29:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:29:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:29:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T03:29:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:29:50.407Z"
}
[2025-10-26T03:29:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T03:29:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:29:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:29:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:29:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:30:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:30:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:30:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:30:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T03:30:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:30:50.410Z"
}
[2025-10-26T03:30:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T03:30:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:30:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:30:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:30:50.468Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:31:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:31:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:31:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:31:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T03:31:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:31:50.403Z"
}
[2025-10-26T03:31:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T03:31:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:31:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:31:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:31:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:32:50.471Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:32:50.471Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:32:50.474Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:32:50.474Z] [LOG] [API] Checking server status...
[2025-10-26T03:32:50.483Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:32:50.475Z"
}
[2025-10-26T03:32:50.483Z] [LOG] [API] WebGPU available
[2025-10-26T03:32:50.489Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:32:50.489Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:32:50.489Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:32:50.494Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:33:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:33:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:33:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:33:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T03:33:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:33:50.391Z"
}
[2025-10-26T03:33:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T03:33:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:33:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:33:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:33:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:34:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:34:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:34:50.424Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:34:50.424Z] [LOG] [API] Checking server status...
[2025-10-26T03:34:50.435Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:34:50.427Z"
}
[2025-10-26T03:34:50.435Z] [LOG] [API] WebGPU available
[2025-10-26T03:34:50.446Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:34:50.446Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:34:50.446Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:34:50.471Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:35:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:35:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:35:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:35:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T03:35:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:35:50.389Z"
}
[2025-10-26T03:35:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T03:35:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:35:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:35:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:35:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:36:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:36:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:36:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:36:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T03:36:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:36:50.401Z"
}
[2025-10-26T03:36:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T03:36:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:36:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:36:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:36:50.431Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:37:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:37:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:37:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:37:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T03:37:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:37:50.390Z"
}
[2025-10-26T03:37:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T03:37:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:37:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:37:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:37:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:38:50.518Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:38:50.518Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:38:50.520Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:38:50.520Z] [LOG] [API] Checking server status...
[2025-10-26T03:38:50.530Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:38:50.521Z"
}
[2025-10-26T03:38:50.530Z] [LOG] [API] WebGPU available
[2025-10-26T03:38:50.537Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:38:50.537Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:38:50.537Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:38:50.545Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:39:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:39:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:39:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:39:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T03:39:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:39:50.392Z"
}
[2025-10-26T03:39:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T03:39:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:39:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:39:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:39:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:40:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:40:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:40:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:40:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T03:40:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:40:50.399Z"
}
[2025-10-26T03:40:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T03:40:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:40:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:40:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:40:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:41:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:41:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:41:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:41:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T03:41:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:41:50.393Z"
}
[2025-10-26T03:41:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T03:41:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:41:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:41:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:41:50.431Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:42:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:42:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:42:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:42:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T03:42:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:42:50.391Z"
}
[2025-10-26T03:42:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T03:42:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:42:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:42:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:42:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:43:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:43:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:43:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:43:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T03:43:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:43:50.395Z"
}
[2025-10-26T03:43:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T03:43:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:43:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:43:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:43:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:44:50.520Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:44:50.521Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:44:50.523Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:44:50.523Z] [LOG] [API] Checking server status...
[2025-10-26T03:44:50.532Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:44:50.524Z"
}
[2025-10-26T03:44:50.532Z] [LOG] [API] WebGPU available
[2025-10-26T03:44:50.538Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:44:50.538Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:44:50.538Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:44:50.543Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:45:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:45:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:45:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:45:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T03:45:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:45:50.395Z"
}
[2025-10-26T03:45:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T03:45:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:45:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:45:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:45:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:46:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:46:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:46:50.439Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:46:50.439Z] [LOG] [API] Checking server status...
[2025-10-26T03:46:50.449Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:46:50.440Z"
}
[2025-10-26T03:46:50.449Z] [LOG] [API] WebGPU available
[2025-10-26T03:46:50.459Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:46:50.459Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:46:50.459Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:46:50.486Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:47:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:47:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:47:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:47:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T03:47:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:47:50.392Z"
}
[2025-10-26T03:47:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T03:47:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:47:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:47:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:47:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:48:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:48:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:48:50.430Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:48:50.430Z] [LOG] [API] Checking server status...
[2025-10-26T03:48:50.440Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:48:50.432Z"
}
[2025-10-26T03:48:50.441Z] [LOG] [API] WebGPU available
[2025-10-26T03:48:50.453Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:48:50.453Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:48:50.453Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:48:50.473Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:49:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:49:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:49:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:49:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T03:49:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:49:50.392Z"
}
[2025-10-26T03:49:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T03:49:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:49:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:49:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:49:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:50:50.487Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:50:50.488Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:50:50.490Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:50:50.490Z] [LOG] [API] Checking server status...
[2025-10-26T03:50:50.499Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:50:50.491Z"
}
[2025-10-26T03:50:50.499Z] [LOG] [API] WebGPU available
[2025-10-26T03:50:50.502Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:50:50.502Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:50:50.502Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:50:50.518Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:51:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:51:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:51:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:51:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T03:51:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:51:50.393Z"
}
[2025-10-26T03:51:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T03:51:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:51:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:51:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:51:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:52:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:52:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:52:50.429Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:52:50.429Z] [LOG] [API] Checking server status...
[2025-10-26T03:52:50.438Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:52:50.430Z"
}
[2025-10-26T03:52:50.438Z] [LOG] [API] WebGPU available
[2025-10-26T03:52:50.463Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:52:50.463Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:52:50.463Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:52:50.487Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:53:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:53:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:53:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:53:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T03:53:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:53:50.399Z"
}
[2025-10-26T03:53:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T03:53:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:53:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:53:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:53:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:54:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:54:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:54:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:54:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T03:54:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:54:50.409Z"
}
[2025-10-26T03:54:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T03:54:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:54:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:54:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:54:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:55:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:55:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:55:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:55:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T03:55:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:55:50.395Z"
}
[2025-10-26T03:55:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T03:55:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:55:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:55:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:55:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:56:50.609Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:56:50.609Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:56:50.611Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:56:50.611Z] [LOG] [API] Checking server status...
[2025-10-26T03:56:50.620Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:56:50.612Z"
}
[2025-10-26T03:56:50.620Z] [LOG] [API] WebGPU available
[2025-10-26T03:56:50.627Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:56:50.627Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:56:50.627Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:56:50.634Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:57:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:57:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:57:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:57:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T03:57:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:57:50.396Z"
}
[2025-10-26T03:57:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T03:57:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:57:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:57:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:57:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:58:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:58:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:58:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:58:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T03:58:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:58:50.415Z"
}
[2025-10-26T03:58:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T03:58:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:58:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:58:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:58:50.459Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T03:59:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T03:59:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T03:59:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T03:59:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T03:59:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T03:59:50.391Z"
}
[2025-10-26T03:59:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T03:59:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T03:59:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T03:59:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T03:59:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:00:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:00:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:00:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:00:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T04:00:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:00:50.398Z"
}
[2025-10-26T04:00:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T04:00:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:00:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:00:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:00:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:01:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:01:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:01:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:01:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T04:01:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:01:50.396Z"
}
[2025-10-26T04:01:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T04:01:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:01:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:01:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:01:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:02:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:02:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:02:50.426Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:02:50.426Z] [LOG] [API] Checking server status...
[2025-10-26T04:02:50.438Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:02:50.430Z"
}
[2025-10-26T04:02:50.438Z] [LOG] [API] WebGPU available
[2025-10-26T04:02:50.453Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:02:50.454Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:02:50.454Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:02:50.473Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:03:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:03:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:03:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:03:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T04:03:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:03:50.393Z"
}
[2025-10-26T04:03:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T04:03:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:03:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:03:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:03:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:04:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:04:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:04:50.416Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:04:50.416Z] [LOG] [API] Checking server status...
[2025-10-26T04:04:50.424Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:04:50.419Z"
}
[2025-10-26T04:04:50.424Z] [LOG] [API] WebGPU available
[2025-10-26T04:04:50.428Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:04:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:04:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:04:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:05:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:05:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:05:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:05:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T04:05:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:05:50.397Z"
}
[2025-10-26T04:05:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T04:05:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:05:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:05:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:05:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:06:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:06:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:06:50.420Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:06:50.420Z] [LOG] [API] Checking server status...
[2025-10-26T04:06:50.432Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:06:50.423Z"
}
[2025-10-26T04:06:50.432Z] [LOG] [API] WebGPU available
[2025-10-26T04:06:50.438Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:06:50.438Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:06:50.438Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:06:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:07:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:07:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:07:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:07:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T04:07:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:07:50.388Z"
}
[2025-10-26T04:07:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T04:07:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:07:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:07:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:07:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:08:50.527Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:08:50.527Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:08:50.529Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:08:50.529Z] [LOG] [API] Checking server status...
[2025-10-26T04:08:50.538Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:08:50.530Z"
}
[2025-10-26T04:08:50.538Z] [LOG] [API] WebGPU available
[2025-10-26T04:08:50.546Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:08:50.546Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:08:50.546Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:08:50.551Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:09:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:09:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:09:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:09:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T04:09:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:09:50.398Z"
}
[2025-10-26T04:09:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T04:09:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:09:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:09:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:09:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:10:50.396Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:10:50.396Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:10:50.436Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:10:50.436Z] [LOG] [API] Checking server status...
[2025-10-26T04:10:50.445Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:10:50.438Z"
}
[2025-10-26T04:10:50.446Z] [LOG] [API] WebGPU available
[2025-10-26T04:10:50.460Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:10:50.460Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:10:50.460Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:10:50.480Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:11:50.394Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:11:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:11:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:11:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T04:11:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:11:50.400Z"
}
[2025-10-26T04:11:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T04:11:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:11:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:11:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:11:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:12:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:12:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:12:50.436Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:12:50.436Z] [LOG] [API] Checking server status...
[2025-10-26T04:12:50.446Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:12:50.437Z"
}
[2025-10-26T04:12:50.446Z] [LOG] [API] WebGPU available
[2025-10-26T04:12:50.460Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:12:50.460Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:12:50.460Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:12:50.481Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:13:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:13:50.397Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:13:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:13:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T04:13:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:13:50.401Z"
}
[2025-10-26T04:13:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T04:13:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:13:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:13:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:13:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:14:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:14:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:14:50.419Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:14:50.419Z] [LOG] [API] Checking server status...
[2025-10-26T04:14:50.429Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:14:50.420Z"
}
[2025-10-26T04:14:50.429Z] [LOG] [API] WebGPU available
[2025-10-26T04:14:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:14:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:14:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:14:50.465Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:15:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:15:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:15:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:15:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T04:15:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:15:50.399Z"
}
[2025-10-26T04:15:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T04:15:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:15:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:15:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:15:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:16:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:16:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:16:50.419Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:16:50.419Z] [LOG] [API] Checking server status...
[2025-10-26T04:16:50.432Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:16:50.420Z"
}
[2025-10-26T04:16:50.432Z] [LOG] [API] WebGPU available
[2025-10-26T04:16:50.438Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:16:50.438Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:16:50.438Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:16:50.463Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:17:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:17:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:17:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:17:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T04:17:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:17:50.396Z"
}
[2025-10-26T04:17:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T04:17:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:17:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:17:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:17:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:18:50.396Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:18:50.396Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:18:50.430Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:18:50.430Z] [LOG] [API] Checking server status...
[2025-10-26T04:18:50.440Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:18:50.431Z"
}
[2025-10-26T04:18:50.440Z] [LOG] [API] WebGPU available
[2025-10-26T04:18:50.448Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:18:50.448Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:18:50.448Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:18:50.475Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:19:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:19:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:19:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:19:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T04:19:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:19:50.407Z"
}
[2025-10-26T04:19:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T04:19:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:19:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:19:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:19:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:20:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:20:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:20:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:20:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T04:20:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:20:50.408Z"
}
[2025-10-26T04:20:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T04:20:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:20:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:20:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:20:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:21:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:21:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:21:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:21:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T04:21:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:21:50.403Z"
}
[2025-10-26T04:21:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T04:21:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:21:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:21:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:21:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:22:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:22:50.394Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:22:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:22:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T04:22:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:22:50.405Z"
}
[2025-10-26T04:22:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T04:22:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:22:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:22:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:22:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:23:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:23:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:23:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:23:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T04:23:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:23:50.396Z"
}
[2025-10-26T04:23:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T04:23:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:23:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:23:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:23:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:24:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:24:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:24:50.423Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:24:50.423Z] [LOG] [API] Checking server status...
[2025-10-26T04:24:50.433Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:24:50.424Z"
}
[2025-10-26T04:24:50.433Z] [LOG] [API] WebGPU available
[2025-10-26T04:24:50.439Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:24:50.439Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:24:50.439Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:24:50.452Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:25:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:25:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:25:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:25:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T04:25:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:25:50.404Z"
}
[2025-10-26T04:25:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T04:25:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:25:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:25:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:25:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:26:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:26:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:26:50.422Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:26:50.422Z] [LOG] [API] Checking server status...
[2025-10-26T04:26:50.433Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:26:50.423Z"
}
[2025-10-26T04:26:50.433Z] [LOG] [API] WebGPU available
[2025-10-26T04:26:50.439Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:26:50.439Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:26:50.439Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:26:50.463Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:27:50.394Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:27:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:27:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:27:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T04:27:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:27:50.400Z"
}
[2025-10-26T04:27:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T04:27:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:27:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:27:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:27:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:28:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:28:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:28:50.431Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:28:50.431Z] [LOG] [API] Checking server status...
[2025-10-26T04:28:50.441Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:28:50.432Z"
}
[2025-10-26T04:28:50.441Z] [LOG] [API] WebGPU available
[2025-10-26T04:28:50.457Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:28:50.457Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:28:50.457Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:28:50.477Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:29:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:29:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:29:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:29:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T04:29:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:29:50.392Z"
}
[2025-10-26T04:29:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T04:29:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:29:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:29:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:29:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:30:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:30:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:30:50.435Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:30:50.435Z] [LOG] [API] Checking server status...
[2025-10-26T04:30:50.444Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:30:50.436Z"
}
[2025-10-26T04:30:50.444Z] [LOG] [API] WebGPU available
[2025-10-26T04:30:50.458Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:30:50.458Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:30:50.458Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:30:50.479Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:31:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:31:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:31:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:31:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T04:31:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:31:50.397Z"
}
[2025-10-26T04:31:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T04:31:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:31:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:31:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:31:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:32:50.500Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:32:50.500Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:32:50.503Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:32:50.503Z] [LOG] [API] Checking server status...
[2025-10-26T04:32:50.512Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:32:50.504Z"
}
[2025-10-26T04:32:50.512Z] [LOG] [API] WebGPU available
[2025-10-26T04:32:50.518Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:32:50.518Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:32:50.518Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:32:50.525Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:33:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:33:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:33:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:33:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T04:33:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:33:50.400Z"
}
[2025-10-26T04:33:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T04:33:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:33:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:33:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:33:50.435Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:34:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:34:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:34:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:34:50.422Z] [LOG] [API] Checking server status...
[2025-10-26T04:34:50.433Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:34:50.422Z"
}
[2025-10-26T04:34:50.434Z] [LOG] [API] WebGPU available
[2025-10-26T04:34:50.464Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:34:50.464Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:34:50.464Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:34:50.489Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:35:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:35:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:35:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:35:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T04:35:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:35:50.400Z"
}
[2025-10-26T04:35:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T04:35:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:35:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:35:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:35:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:36:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:36:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:36:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:36:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T04:36:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:36:50.408Z"
}
[2025-10-26T04:36:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T04:36:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:36:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:36:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:36:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:37:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:37:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:37:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:37:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T04:37:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:37:50.397Z"
}
[2025-10-26T04:37:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T04:37:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:37:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:37:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:37:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:38:50.439Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:38:50.439Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:38:50.442Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:38:50.442Z] [LOG] [API] Checking server status...
[2025-10-26T04:38:50.451Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:38:50.443Z"
}
[2025-10-26T04:38:50.451Z] [LOG] [API] WebGPU available
[2025-10-26T04:38:50.458Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:38:50.458Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:38:50.458Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:38:50.463Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:39:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:39:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:39:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:39:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T04:39:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:39:50.396Z"
}
[2025-10-26T04:39:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T04:39:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:39:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:39:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:39:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:40:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:40:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:40:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:40:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T04:40:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:40:50.399Z"
}
[2025-10-26T04:40:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T04:40:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:40:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:40:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:40:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:41:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:41:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:41:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:41:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T04:41:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:41:50.398Z"
}
[2025-10-26T04:41:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T04:41:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:41:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:41:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:41:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:42:50.394Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:42:50.394Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:42:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:42:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T04:42:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:42:50.400Z"
}
[2025-10-26T04:42:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T04:42:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:42:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:42:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:42:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:43:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:43:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:43:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:43:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T04:43:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:43:50.394Z"
}
[2025-10-26T04:43:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T04:43:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:43:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:43:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:43:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:44:50.572Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:44:50.572Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:44:50.574Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:44:50.574Z] [LOG] [API] Checking server status...
[2025-10-26T04:44:50.584Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:44:50.576Z"
}
[2025-10-26T04:44:50.584Z] [LOG] [API] WebGPU available
[2025-10-26T04:44:50.586Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:44:50.586Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:44:50.586Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:44:50.594Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:45:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:45:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:45:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:45:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T04:45:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:45:50.396Z"
}
[2025-10-26T04:45:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T04:45:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:45:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:45:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:45:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:46:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:46:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:46:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:46:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T04:46:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:46:50.405Z"
}
[2025-10-26T04:46:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T04:46:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:46:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:46:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:46:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:47:50.396Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:47:50.396Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:47:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:47:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T04:47:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:47:50.401Z"
}
[2025-10-26T04:47:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T04:47:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:47:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:47:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:47:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:48:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:48:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:48:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:48:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T04:48:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:48:50.404Z"
}
[2025-10-26T04:48:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T04:48:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:48:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:48:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:48:50.435Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:49:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:49:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:49:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:49:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T04:49:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:49:50.396Z"
}
[2025-10-26T04:49:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T04:49:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:49:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:49:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:49:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:50:50.516Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:50:50.516Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:50:50.518Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:50:50.518Z] [LOG] [API] Checking server status...
[2025-10-26T04:50:50.527Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:50:50.519Z"
}
[2025-10-26T04:50:50.527Z] [LOG] [API] WebGPU available
[2025-10-26T04:50:50.535Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:50:50.535Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:50:50.535Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:50:50.545Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:51:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:51:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:51:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:51:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T04:51:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:51:50.407Z"
}
[2025-10-26T04:51:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T04:51:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:51:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:51:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:51:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:52:50.396Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:52:50.396Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:52:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:52:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T04:52:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:52:50.401Z"
}
[2025-10-26T04:52:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T04:52:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:52:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:52:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:52:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:53:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:53:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:53:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:53:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T04:53:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:53:50.395Z"
}
[2025-10-26T04:53:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T04:53:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:53:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:53:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:53:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:54:50.405Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:54:50.405Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:54:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:54:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T04:54:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:54:50.412Z"
}
[2025-10-26T04:54:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T04:54:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:54:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:54:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:54:50.452Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:55:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:55:50.397Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:55:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:55:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T04:55:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:55:50.402Z"
}
[2025-10-26T04:55:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T04:55:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:55:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:55:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:55:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:56:50.494Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:56:50.494Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:56:50.496Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:56:50.496Z] [LOG] [API] Checking server status...
[2025-10-26T04:56:50.505Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:56:50.498Z"
}
[2025-10-26T04:56:50.506Z] [LOG] [API] WebGPU available
[2025-10-26T04:56:50.512Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:56:50.512Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:56:50.512Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:56:50.518Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:57:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:57:50.397Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:57:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:57:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T04:57:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:57:50.403Z"
}
[2025-10-26T04:57:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T04:57:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:57:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:57:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:57:50.435Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:58:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:58:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:58:50.420Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:58:50.420Z] [LOG] [API] Checking server status...
[2025-10-26T04:58:50.433Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:58:50.424Z"
}
[2025-10-26T04:58:50.433Z] [LOG] [API] WebGPU available
[2025-10-26T04:58:50.439Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:58:50.439Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:58:50.439Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:58:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T04:59:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T04:59:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T04:59:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T04:59:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T04:59:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T04:59:50.409Z"
}
[2025-10-26T04:59:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T04:59:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T04:59:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T04:59:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T04:59:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:00:50.394Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:00:50.394Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:00:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:00:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T05:00:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:00:50.417Z"
}
[2025-10-26T05:00:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T05:00:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:00:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:00:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:00:50.456Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:01:50.394Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:01:50.394Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:01:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:01:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T05:01:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:01:50.399Z"
}
[2025-10-26T05:01:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T05:01:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:01:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:01:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:01:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:02:50.441Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:02:50.441Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:02:50.443Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:02:50.443Z] [LOG] [API] Checking server status...
[2025-10-26T05:02:50.452Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:02:50.445Z"
}
[2025-10-26T05:02:50.452Z] [LOG] [API] WebGPU available
[2025-10-26T05:02:50.459Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:02:50.459Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:02:50.459Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:02:50.464Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:03:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:03:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:03:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:03:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T05:03:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:03:50.413Z"
}
[2025-10-26T05:03:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T05:03:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:03:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:03:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:03:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:04:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:04:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:04:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:04:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T05:04:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:04:50.412Z"
}
[2025-10-26T05:04:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T05:04:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:04:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:04:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:04:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:05:50.403Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:05:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:05:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:05:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T05:05:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:05:50.408Z"
}
[2025-10-26T05:05:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T05:05:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:05:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:05:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:05:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:06:50.396Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:06:50.396Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:06:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:06:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T05:06:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:06:50.401Z"
}
[2025-10-26T05:06:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T05:06:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:06:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:06:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:06:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:07:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:07:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:07:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:07:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T05:07:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:07:50.398Z"
}
[2025-10-26T05:07:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T05:07:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:07:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:07:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:07:50.431Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:08:50.536Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:08:50.536Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:08:50.538Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:08:50.538Z] [LOG] [API] Checking server status...
[2025-10-26T05:08:50.547Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:08:50.540Z"
}
[2025-10-26T05:08:50.547Z] [LOG] [API] WebGPU available
[2025-10-26T05:08:50.550Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:08:50.550Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:08:50.550Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:08:50.567Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:09:50.401Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:09:50.401Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:09:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:09:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T05:09:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:09:50.406Z"
}
[2025-10-26T05:09:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T05:09:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:09:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:09:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:09:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:10:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:10:50.396Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:10:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:10:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T05:10:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:10:50.402Z"
}
[2025-10-26T05:10:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T05:10:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:10:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:10:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:10:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:11:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:11:50.397Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:11:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:11:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T05:11:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:11:50.402Z"
}
[2025-10-26T05:11:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T05:11:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:11:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:11:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:11:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:12:50.414Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:12:50.414Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:12:50.418Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:12:50.418Z] [LOG] [API] Checking server status...
[2025-10-26T05:12:50.428Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:12:50.419Z"
}
[2025-10-26T05:12:50.428Z] [LOG] [API] WebGPU available
[2025-10-26T05:12:50.435Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:12:50.435Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:12:50.435Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:12:50.458Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:13:50.401Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:13:50.401Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:13:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:13:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T05:13:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:13:50.408Z"
}
[2025-10-26T05:13:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T05:13:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:13:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:13:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:13:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:14:50.581Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:14:50.581Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:14:50.583Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:14:50.583Z] [LOG] [API] Checking server status...
[2025-10-26T05:14:50.592Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:14:50.584Z"
}
[2025-10-26T05:14:50.593Z] [LOG] [API] WebGPU available
[2025-10-26T05:14:50.595Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:14:50.595Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:14:50.595Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:14:50.603Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:15:50.394Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:15:50.394Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:15:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:15:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T05:15:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:15:50.399Z"
}
[2025-10-26T05:15:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T05:15:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:15:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:15:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:15:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:16:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:16:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:16:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:16:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T05:16:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:16:50.405Z"
}
[2025-10-26T05:16:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T05:16:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:16:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:16:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:16:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:17:50.418Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:17:50.418Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:17:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:17:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T05:17:50.432Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:17:50.423Z"
}
[2025-10-26T05:17:50.432Z] [LOG] [API] WebGPU available
[2025-10-26T05:17:50.435Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:17:50.435Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:17:50.435Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:17:50.456Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:18:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:18:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:18:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:18:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T05:18:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:18:50.410Z"
}
[2025-10-26T05:18:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T05:18:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:18:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:18:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:18:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:19:50.409Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:19:50.409Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:19:50.413Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:19:50.413Z] [LOG] [API] Checking server status...
[2025-10-26T05:19:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:19:50.414Z"
}
[2025-10-26T05:19:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T05:19:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:19:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:19:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:19:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:20:50.561Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:20:50.562Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:20:50.564Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:20:50.564Z] [LOG] [API] Checking server status...
[2025-10-26T05:20:50.573Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:20:50.565Z"
}
[2025-10-26T05:20:50.573Z] [LOG] [API] WebGPU available
[2025-10-26T05:20:50.579Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:20:50.579Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:20:50.579Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:20:50.584Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:21:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:21:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:21:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:21:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T05:21:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:21:50.413Z"
}
[2025-10-26T05:21:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T05:21:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:21:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:21:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:21:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:22:50.405Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:22:50.405Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:22:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:22:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T05:22:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:22:50.410Z"
}
[2025-10-26T05:22:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T05:22:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:22:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:22:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:22:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:23:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:23:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:23:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:23:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T05:23:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:23:50.397Z"
}
[2025-10-26T05:23:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T05:23:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:23:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:23:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:23:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:24:50.401Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:24:50.401Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:24:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:24:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T05:24:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:24:50.406Z"
}
[2025-10-26T05:24:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T05:24:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:24:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:24:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:24:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:25:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:25:50.397Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:25:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:25:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T05:25:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:25:50.402Z"
}
[2025-10-26T05:25:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T05:25:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:25:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:25:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:25:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:26:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:26:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:26:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:26:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T05:26:50.424Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:26:50.415Z"
}
[2025-10-26T05:26:50.424Z] [LOG] [API] WebGPU available
[2025-10-26T05:26:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:26:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:26:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:26:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:27:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:27:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:27:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:27:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T05:27:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:27:50.400Z"
}
[2025-10-26T05:27:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T05:27:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:27:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:27:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:27:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:28:50.396Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:28:50.396Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:28:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:28:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T05:28:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:28:50.403Z"
}
[2025-10-26T05:28:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T05:28:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:28:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:28:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:28:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:29:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:29:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:29:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:29:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T05:29:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:29:50.402Z"
}
[2025-10-26T05:29:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T05:29:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:29:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:29:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:29:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:30:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:30:50.397Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:30:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:30:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T05:30:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:30:50.402Z"
}
[2025-10-26T05:30:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T05:30:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:30:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:30:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:30:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:31:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:31:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:31:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:31:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T05:31:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:31:50.412Z"
}
[2025-10-26T05:31:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T05:31:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:31:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:31:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:31:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:32:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:32:50.394Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:32:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:32:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T05:32:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:32:50.400Z"
}
[2025-10-26T05:32:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T05:32:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:32:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:32:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:32:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:33:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:33:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:33:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:33:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T05:33:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:33:50.409Z"
}
[2025-10-26T05:33:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T05:33:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:33:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:33:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:33:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:34:50.411Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:34:50.411Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:34:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:34:50.415Z] [LOG] [API] Checking server status...
[2025-10-26T05:34:50.425Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:34:50.416Z"
}
[2025-10-26T05:34:50.425Z] [LOG] [API] WebGPU available
[2025-10-26T05:34:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:34:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:34:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:34:50.452Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:35:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:35:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:35:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:35:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T05:35:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:35:50.404Z"
}
[2025-10-26T05:35:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T05:35:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:35:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:35:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:35:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:36:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:36:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:36:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:36:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T05:36:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:36:50.413Z"
}
[2025-10-26T05:36:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T05:36:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:36:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:36:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:36:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:37:50.403Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:37:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:37:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:37:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T05:37:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:37:50.409Z"
}
[2025-10-26T05:37:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T05:37:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:37:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:37:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:37:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:38:50.488Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:38:50.489Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:38:50.491Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:38:50.491Z] [LOG] [API] Checking server status...
[2025-10-26T05:38:50.500Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:38:50.492Z"
}
[2025-10-26T05:38:50.500Z] [LOG] [API] WebGPU available
[2025-10-26T05:38:50.503Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:38:50.503Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:38:50.503Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:38:50.510Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:39:50.396Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:39:50.396Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:39:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:39:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T05:39:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:39:50.400Z"
}
[2025-10-26T05:39:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T05:39:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:39:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:39:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:39:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:40:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:40:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:40:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:40:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T05:40:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:40:50.413Z"
}
[2025-10-26T05:40:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T05:40:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:40:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:40:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:40:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:41:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:41:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:41:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:41:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T05:41:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:41:50.409Z"
}
[2025-10-26T05:41:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T05:41:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:41:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:41:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:41:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:42:50.405Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:42:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:42:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:42:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T05:42:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:42:50.411Z"
}
[2025-10-26T05:42:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T05:42:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:42:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:42:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:42:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:43:50.405Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:43:50.405Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:43:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:43:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T05:43:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:43:50.410Z"
}
[2025-10-26T05:43:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T05:43:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:43:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:43:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:43:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:44:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:44:50.405Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:44:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:44:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T05:44:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:44:50.412Z"
}
[2025-10-26T05:44:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T05:44:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:44:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:44:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:44:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:45:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:45:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:45:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:45:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T05:45:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:45:50.408Z"
}
[2025-10-26T05:45:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T05:45:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:45:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:45:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:45:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:46:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:46:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:46:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:46:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T05:46:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:46:50.407Z"
}
[2025-10-26T05:46:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T05:46:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:46:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:46:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:46:50.435Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:47:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:47:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:47:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:47:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T05:47:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:47:50.411Z"
}
[2025-10-26T05:47:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T05:47:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:47:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:47:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:47:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:48:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:48:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:48:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:48:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T05:48:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:48:50.408Z"
}
[2025-10-26T05:48:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T05:48:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:48:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:48:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:48:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:49:50.409Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:49:50.409Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:49:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:49:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T05:49:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:49:50.414Z"
}
[2025-10-26T05:49:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T05:49:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:49:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:49:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:49:50.451Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:50:50.458Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:50:50.458Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:50:50.460Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:50:50.460Z] [LOG] [API] Checking server status...
[2025-10-26T05:50:50.470Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:50:50.462Z"
}
[2025-10-26T05:50:50.470Z] [LOG] [API] WebGPU available
[2025-10-26T05:50:50.472Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:50:50.472Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:50:50.472Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:50:50.480Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:51:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:51:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:51:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:51:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T05:51:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:51:50.404Z"
}
[2025-10-26T05:51:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T05:51:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:51:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:51:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:51:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:52:50.400Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:52:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:52:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:52:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T05:52:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:52:50.410Z"
}
[2025-10-26T05:52:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T05:52:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:52:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:52:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:52:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:53:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:53:50.405Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:53:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:53:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T05:53:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:53:50.409Z"
}
[2025-10-26T05:53:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T05:53:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:53:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:53:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:53:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:54:50.401Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:54:50.401Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:54:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:54:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T05:54:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:54:50.408Z"
}
[2025-10-26T05:54:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T05:54:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:54:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:54:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:54:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:55:50.411Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:55:50.411Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:55:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:55:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T05:55:50.425Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:55:50.416Z"
}
[2025-10-26T05:55:50.425Z] [LOG] [API] WebGPU available
[2025-10-26T05:55:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:55:50.429Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:55:50.429Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:55:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:56:50.538Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:56:50.538Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:56:50.540Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:56:50.540Z] [LOG] [API] Checking server status...
[2025-10-26T05:56:50.549Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:56:50.541Z"
}
[2025-10-26T05:56:50.549Z] [LOG] [API] WebGPU available
[2025-10-26T05:56:50.552Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:56:50.552Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:56:50.552Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:56:50.559Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:57:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:57:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:57:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:57:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T05:57:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:57:50.413Z"
}
[2025-10-26T05:57:50.422Z] [LOG] [API] WebGPU available
[2025-10-26T05:57:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:57:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:57:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:57:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:58:50.400Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:58:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:58:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:58:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T05:58:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:58:50.406Z"
}
[2025-10-26T05:58:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T05:58:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:58:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:58:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:58:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T05:59:50.403Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T05:59:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T05:59:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T05:59:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T05:59:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T05:59:50.409Z"
}
[2025-10-26T05:59:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T05:59:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T05:59:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T05:59:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T05:59:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:00:50.400Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:00:50.401Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:00:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:00:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T06:00:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:00:50.407Z"
}
[2025-10-26T06:00:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T06:00:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:00:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:00:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:00:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:01:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:01:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:01:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:01:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T06:01:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:01:50.410Z"
}
[2025-10-26T06:01:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T06:01:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:01:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:01:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:01:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:02:50.539Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:02:50.539Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:02:50.541Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:02:50.541Z] [LOG] [API] Checking server status...
[2025-10-26T06:02:50.550Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:02:50.543Z"
}
[2025-10-26T06:02:50.550Z] [LOG] [API] WebGPU available
[2025-10-26T06:02:50.557Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:02:50.557Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:02:50.557Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:02:50.561Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:03:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:03:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:03:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:03:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T06:03:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:03:50.400Z"
}
[2025-10-26T06:03:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T06:03:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:03:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:03:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:03:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:04:50.412Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:04:50.412Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:04:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:04:50.415Z] [LOG] [API] Checking server status...
[2025-10-26T06:04:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:04:50.417Z"
}
[2025-10-26T06:04:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T06:04:50.431Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:04:50.431Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:04:50.431Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:04:50.452Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:05:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:05:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:05:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:05:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T06:05:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:05:50.405Z"
}
[2025-10-26T06:05:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T06:05:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:05:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:05:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:05:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:06:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:06:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:06:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:06:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T06:06:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:06:50.412Z"
}
[2025-10-26T06:06:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T06:06:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:06:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:06:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:06:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:07:50.403Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:07:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:07:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:07:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T06:07:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:07:50.408Z"
}
[2025-10-26T06:07:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T06:07:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:07:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:07:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:07:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:08:50.415Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:08:50.415Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:08:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:08:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T06:08:50.431Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:08:50.423Z"
}
[2025-10-26T06:08:50.431Z] [LOG] [API] WebGPU available
[2025-10-26T06:08:50.436Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:08:50.436Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:08:50.436Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:08:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:09:50.401Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:09:50.401Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:09:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:09:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T06:09:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:09:50.407Z"
}
[2025-10-26T06:09:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T06:09:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:09:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:09:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:09:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:10:50.419Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:10:50.420Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:10:50.423Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:10:50.423Z] [LOG] [API] Checking server status...
[2025-10-26T06:10:50.433Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:10:50.424Z"
}
[2025-10-26T06:10:50.433Z] [LOG] [API] WebGPU available
[2025-10-26T06:10:50.436Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:10:50.436Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:10:50.436Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:10:50.453Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:11:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:11:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:11:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:11:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T06:11:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:11:50.413Z"
}
[2025-10-26T06:11:50.422Z] [LOG] [API] WebGPU available
[2025-10-26T06:11:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:11:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:11:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:11:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:12:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:12:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:12:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:12:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T06:12:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:12:50.406Z"
}
[2025-10-26T06:12:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T06:12:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:12:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:12:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:12:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:13:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:13:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:13:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:13:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T06:13:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:13:50.411Z"
}
[2025-10-26T06:13:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T06:13:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:13:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:13:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:13:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:14:50.493Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:14:50.493Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:14:50.496Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:14:50.496Z] [LOG] [API] Checking server status...
[2025-10-26T06:14:50.505Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:14:50.497Z"
}
[2025-10-26T06:14:50.505Z] [LOG] [API] WebGPU available
[2025-10-26T06:14:50.512Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:14:50.512Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:14:50.512Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:14:50.521Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:15:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:15:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:15:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:15:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T06:15:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:15:50.409Z"
}
[2025-10-26T06:15:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T06:15:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:15:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:15:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:15:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:16:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:16:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:16:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:16:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T06:16:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:16:50.411Z"
}
[2025-10-26T06:16:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T06:16:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:16:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:16:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:16:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:17:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:17:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:17:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:17:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T06:17:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:17:50.404Z"
}
[2025-10-26T06:17:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T06:17:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:17:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:17:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:17:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:18:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:18:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:18:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:18:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T06:18:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:18:50.407Z"
}
[2025-10-26T06:18:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T06:18:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:18:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:18:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:18:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:19:50.400Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:19:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:19:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:19:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T06:19:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:19:50.405Z"
}
[2025-10-26T06:19:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T06:19:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:19:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:19:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:19:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:20:50.426Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:20:50.426Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:20:50.429Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:20:50.429Z] [LOG] [API] Checking server status...
[2025-10-26T06:20:50.440Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:20:50.430Z"
}
[2025-10-26T06:20:50.440Z] [LOG] [API] WebGPU available
[2025-10-26T06:20:50.446Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:20:50.446Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:20:50.446Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:20:50.464Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:21:50.409Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:21:50.409Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:21:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:21:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T06:21:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:21:50.414Z"
}
[2025-10-26T06:21:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T06:21:50.428Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:21:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:21:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:21:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:22:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:22:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:22:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:22:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T06:22:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:22:50.401Z"
}
[2025-10-26T06:22:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T06:22:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:22:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:22:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:22:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:23:50.405Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:23:50.405Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:23:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:23:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T06:23:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:23:50.412Z"
}
[2025-10-26T06:23:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T06:23:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:23:50.429Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:23:50.429Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:23:50.435Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:24:50.403Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:24:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:24:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:24:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T06:24:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:24:50.408Z"
}
[2025-10-26T06:24:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T06:24:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:24:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:24:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:24:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:25:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:25:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:25:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:25:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T06:25:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:25:50.402Z"
}
[2025-10-26T06:25:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T06:25:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:25:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:25:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:25:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:26:50.494Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:26:50.494Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:26:50.496Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:26:50.496Z] [LOG] [API] Checking server status...
[2025-10-26T06:26:50.506Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:26:50.497Z"
}
[2025-10-26T06:26:50.506Z] [LOG] [API] WebGPU available
[2025-10-26T06:26:50.513Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:26:50.514Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:26:50.514Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:26:50.519Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:27:50.413Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:27:50.413Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:27:50.416Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:27:50.416Z] [LOG] [API] Checking server status...
[2025-10-26T06:27:50.427Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:27:50.418Z"
}
[2025-10-26T06:27:50.427Z] [LOG] [API] WebGPU available
[2025-10-26T06:27:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:27:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:27:50.431Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:27:50.454Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:28:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:28:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:28:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:28:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T06:28:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:28:50.410Z"
}
[2025-10-26T06:28:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T06:28:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:28:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:28:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:28:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:29:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:29:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:29:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:29:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T06:29:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:29:50.409Z"
}
[2025-10-26T06:29:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T06:29:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:29:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:29:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:29:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:30:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:30:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:30:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:30:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T06:30:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:30:50.399Z"
}
[2025-10-26T06:30:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T06:30:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:30:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:30:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:30:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:31:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:31:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:31:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:31:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T06:31:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:31:50.409Z"
}
[2025-10-26T06:31:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T06:31:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:31:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:31:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:31:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:32:50.472Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:32:50.472Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:32:50.474Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:32:50.474Z] [LOG] [API] Checking server status...
[2025-10-26T06:32:50.484Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:32:50.475Z"
}
[2025-10-26T06:32:50.484Z] [LOG] [API] WebGPU available
[2025-10-26T06:32:50.490Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:32:50.490Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:32:50.490Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:32:50.499Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:33:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:33:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:33:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:33:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T06:33:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:33:50.413Z"
}
[2025-10-26T06:33:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T06:33:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:33:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:33:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:33:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:34:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:34:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:34:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:34:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T06:34:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:34:50.414Z"
}
[2025-10-26T06:34:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T06:34:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:34:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:34:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:34:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:35:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:35:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:35:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:35:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T06:35:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:35:50.410Z"
}
[2025-10-26T06:35:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T06:35:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:35:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:35:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:35:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:36:50.417Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:36:50.417Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:36:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:36:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T06:36:50.431Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:36:50.424Z"
}
[2025-10-26T06:36:50.431Z] [LOG] [API] WebGPU available
[2025-10-26T06:36:50.435Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:36:50.436Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:36:50.436Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:36:50.458Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:37:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:37:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:37:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:37:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T06:37:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:37:50.412Z"
}
[2025-10-26T06:37:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T06:37:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:37:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:37:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:37:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:38:50.529Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:38:50.529Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:38:50.531Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:38:50.531Z] [LOG] [API] Checking server status...
[2025-10-26T06:38:50.540Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:38:50.533Z"
}
[2025-10-26T06:38:50.540Z] [LOG] [API] WebGPU available
[2025-10-26T06:38:50.546Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:38:50.546Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:38:50.546Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:38:50.551Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:39:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:39:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:39:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:39:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T06:39:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:39:50.409Z"
}
[2025-10-26T06:39:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T06:39:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:39:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:39:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:39:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:40:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:40:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:40:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:40:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T06:40:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:40:50.414Z"
}
[2025-10-26T06:40:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T06:40:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:40:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:40:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:40:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:41:50.403Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:41:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:41:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:41:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T06:41:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:41:50.408Z"
}
[2025-10-26T06:41:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T06:41:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:41:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:41:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:41:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:42:50.414Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:42:50.414Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:42:50.417Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:42:50.417Z] [LOG] [API] Checking server status...
[2025-10-26T06:42:50.428Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:42:50.418Z"
}
[2025-10-26T06:42:50.428Z] [LOG] [API] WebGPU available
[2025-10-26T06:42:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:42:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:42:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:42:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:43:50.409Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:43:50.409Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:43:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:43:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T06:43:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:43:50.414Z"
}
[2025-10-26T06:43:50.422Z] [LOG] [API] WebGPU available
[2025-10-26T06:43:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:43:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:43:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:43:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:44:50.507Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:44:50.507Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:44:50.509Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:44:50.510Z] [LOG] [API] Checking server status...
[2025-10-26T06:44:50.519Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:44:50.511Z"
}
[2025-10-26T06:44:50.519Z] [LOG] [API] WebGPU available
[2025-10-26T06:44:50.524Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:44:50.525Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:44:50.525Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:44:50.529Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:45:50.403Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:45:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:45:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:45:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T06:45:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:45:50.408Z"
}
[2025-10-26T06:45:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T06:45:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:45:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:45:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:45:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:46:50.410Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:46:50.410Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:46:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:46:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T06:46:50.431Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:46:50.425Z"
}
[2025-10-26T06:46:50.431Z] [LOG] [API] WebGPU available
[2025-10-26T06:46:50.435Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:46:50.435Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:46:50.435Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:46:50.457Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:47:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:47:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:47:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:47:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T06:47:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:47:50.413Z"
}
[2025-10-26T06:47:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T06:47:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:47:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:47:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:47:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:48:50.423Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:48:50.424Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:48:50.428Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:48:50.428Z] [LOG] [API] Checking server status...
[2025-10-26T06:48:50.438Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:48:50.429Z"
}
[2025-10-26T06:48:50.438Z] [LOG] [API] WebGPU available
[2025-10-26T06:48:50.441Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:48:50.441Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:48:50.441Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:48:50.465Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:49:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:49:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:49:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:49:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T06:49:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:49:50.408Z"
}
[2025-10-26T06:49:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T06:49:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:49:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:49:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:49:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:50:50.454Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:50:50.454Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:50:50.457Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:50:50.457Z] [LOG] [API] Checking server status...
[2025-10-26T06:50:50.466Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:50:50.458Z"
}
[2025-10-26T06:50:50.466Z] [LOG] [API] WebGPU available
[2025-10-26T06:50:50.468Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:50:50.468Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:50:50.468Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:50:50.478Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:51:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:51:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:51:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:51:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T06:51:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:51:50.412Z"
}
[2025-10-26T06:51:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T06:51:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:51:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:51:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:51:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:52:50.415Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:52:50.415Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:52:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:52:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T06:52:50.431Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:52:50.422Z"
}
[2025-10-26T06:52:50.431Z] [LOG] [API] WebGPU available
[2025-10-26T06:52:50.436Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:52:50.436Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:52:50.436Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:52:50.453Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:53:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:53:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:53:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:53:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T06:53:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:53:50.411Z"
}
[2025-10-26T06:53:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T06:53:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:53:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:53:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:53:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:54:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:54:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:54:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:54:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T06:54:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:54:50.413Z"
}
[2025-10-26T06:54:50.422Z] [LOG] [API] WebGPU available
[2025-10-26T06:54:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:54:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:54:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:54:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:55:50.410Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:55:50.410Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:55:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:55:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T06:55:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:55:50.416Z"
}
[2025-10-26T06:55:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T06:55:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:55:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:55:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:55:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:56:50.443Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:56:50.443Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:56:50.445Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:56:50.445Z] [LOG] [API] Checking server status...
[2025-10-26T06:56:50.454Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:56:50.446Z"
}
[2025-10-26T06:56:50.454Z] [LOG] [API] WebGPU available
[2025-10-26T06:56:50.459Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:56:50.459Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:56:50.459Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:56:50.469Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:57:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:57:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:57:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:57:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T06:57:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:57:50.401Z"
}
[2025-10-26T06:57:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T06:57:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:57:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:57:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:57:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:58:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:58:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:58:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:58:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T06:58:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:58:50.409Z"
}
[2025-10-26T06:58:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T06:58:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:58:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:58:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:58:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T06:59:50.416Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T06:59:50.416Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T06:59:50.419Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T06:59:50.420Z] [LOG] [API] Checking server status...
[2025-10-26T06:59:50.430Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T06:59:50.421Z"
}
[2025-10-26T06:59:50.430Z] [LOG] [API] WebGPU available
[2025-10-26T06:59:50.439Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T06:59:50.439Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T06:59:50.439Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T06:59:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:00:50.419Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:00:50.419Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:00:50.429Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:00:50.429Z] [LOG] [API] Checking server status...
[2025-10-26T07:00:50.438Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:00:50.430Z"
}
[2025-10-26T07:00:50.439Z] [LOG] [API] WebGPU available
[2025-10-26T07:00:50.442Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:00:50.442Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:00:50.442Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:00:50.460Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:01:50.396Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:01:50.396Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:01:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:01:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T07:01:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:01:50.400Z"
}
[2025-10-26T07:01:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T07:01:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:01:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:01:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:01:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:02:50.485Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:02:50.485Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:02:50.488Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:02:50.488Z] [LOG] [API] Checking server status...
[2025-10-26T07:02:50.498Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:02:50.489Z"
}
[2025-10-26T07:02:50.498Z] [LOG] [API] WebGPU available
[2025-10-26T07:02:50.501Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:02:50.501Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:02:50.501Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:02:50.510Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:03:50.409Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:03:50.409Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:03:50.413Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:03:50.413Z] [LOG] [API] Checking server status...
[2025-10-26T07:03:50.424Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:03:50.414Z"
}
[2025-10-26T07:03:50.424Z] [LOG] [API] WebGPU available
[2025-10-26T07:03:50.431Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:03:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:03:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:03:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:04:50.410Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:04:50.410Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:04:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:04:50.415Z] [LOG] [API] Checking server status...
[2025-10-26T07:04:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:04:50.416Z"
}
[2025-10-26T07:04:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T07:04:50.428Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:04:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:04:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:04:50.452Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:05:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:05:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:05:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:05:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T07:05:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:05:50.413Z"
}
[2025-10-26T07:05:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T07:05:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:05:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:05:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:05:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:06:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:06:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:06:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:06:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T07:06:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:06:50.410Z"
}
[2025-10-26T07:06:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T07:06:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:06:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:06:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:06:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:07:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:07:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:07:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:07:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T07:07:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:07:50.404Z"
}
[2025-10-26T07:07:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T07:07:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:07:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:07:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:07:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:08:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:08:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:08:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:08:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T07:08:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:08:50.413Z"
}
[2025-10-26T07:08:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T07:08:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:08:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:08:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:08:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:09:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:09:50.397Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:09:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:09:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T07:09:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:09:50.402Z"
}
[2025-10-26T07:09:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T07:09:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:09:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:09:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:09:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:10:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:10:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:10:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:10:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T07:10:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:10:50.412Z"
}
[2025-10-26T07:10:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T07:10:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:10:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:10:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:10:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:11:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:11:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:11:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:11:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T07:11:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:11:50.411Z"
}
[2025-10-26T07:11:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T07:11:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:11:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:11:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:11:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:12:50.409Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:12:50.409Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:12:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:12:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T07:12:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:12:50.414Z"
}
[2025-10-26T07:12:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T07:12:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:12:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:12:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:12:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:13:50.413Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:13:50.413Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:13:50.417Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:13:50.417Z] [LOG] [API] Checking server status...
[2025-10-26T07:13:50.427Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:13:50.419Z"
}
[2025-10-26T07:13:50.428Z] [LOG] [API] WebGPU available
[2025-10-26T07:13:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:13:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:13:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:13:50.453Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:14:50.472Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:14:50.473Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:14:50.475Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:14:50.475Z] [LOG] [API] Checking server status...
[2025-10-26T07:14:50.484Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:14:50.476Z"
}
[2025-10-26T07:14:50.484Z] [LOG] [API] WebGPU available
[2025-10-26T07:14:50.491Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:14:50.491Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:14:50.491Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:14:50.496Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:15:50.418Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:15:50.418Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:15:50.422Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:15:50.422Z] [LOG] [API] Checking server status...
[2025-10-26T07:15:50.433Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:15:50.425Z"
}
[2025-10-26T07:15:50.433Z] [LOG] [API] WebGPU available
[2025-10-26T07:15:50.441Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:15:50.441Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:15:50.441Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:15:50.458Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:16:50.411Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:16:50.411Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:16:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:16:50.415Z] [LOG] [API] Checking server status...
[2025-10-26T07:16:50.425Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:16:50.417Z"
}
[2025-10-26T07:16:50.425Z] [LOG] [API] WebGPU available
[2025-10-26T07:16:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:16:50.429Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:16:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:16:50.453Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:17:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:17:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:17:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:17:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T07:17:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:17:50.408Z"
}
[2025-10-26T07:17:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T07:17:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:17:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:17:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:17:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:18:50.400Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:18:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:18:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:18:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T07:18:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:18:50.412Z"
}
[2025-10-26T07:18:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T07:18:50.428Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:18:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:18:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:18:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:19:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:19:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:19:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:19:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T07:19:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:19:50.411Z"
}
[2025-10-26T07:19:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T07:19:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:19:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:19:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:19:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:20:50.491Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:20:50.491Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:20:50.510Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:20:50.510Z] [LOG] [API] Checking server status...
[2025-10-26T07:20:50.518Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:20:50.511Z"
}
[2025-10-26T07:20:50.518Z] [LOG] [API] WebGPU available
[2025-10-26T07:20:50.521Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:20:50.522Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:20:50.522Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:20:50.537Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:21:50.414Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:21:50.414Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:21:50.417Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:21:50.417Z] [LOG] [API] Checking server status...
[2025-10-26T07:21:50.430Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:21:50.418Z"
}
[2025-10-26T07:21:50.430Z] [LOG] [API] WebGPU available
[2025-10-26T07:21:50.435Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:21:50.435Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:21:50.436Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:21:50.455Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:22:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:22:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:22:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:22:50.415Z] [LOG] [API] Checking server status...
[2025-10-26T07:22:50.424Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:22:50.416Z"
}
[2025-10-26T07:22:50.424Z] [LOG] [API] WebGPU available
[2025-10-26T07:22:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:22:50.429Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:22:50.429Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:22:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:23:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:23:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:23:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:23:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T07:23:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:23:50.412Z"
}
[2025-10-26T07:23:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T07:23:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:23:50.429Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:23:50.429Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:23:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:24:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:24:50.409Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:24:50.413Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:24:50.413Z] [LOG] [API] Checking server status...
[2025-10-26T07:24:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:24:50.414Z"
}
[2025-10-26T07:24:50.422Z] [LOG] [API] WebGPU available
[2025-10-26T07:24:50.428Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:24:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:24:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:24:50.454Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:25:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:25:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:25:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:25:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T07:25:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:25:50.403Z"
}
[2025-10-26T07:25:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T07:25:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:25:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:25:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:25:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:26:50.562Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:26:50.563Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:26:50.565Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:26:50.565Z] [LOG] [API] Checking server status...
[2025-10-26T07:26:50.574Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:26:50.566Z"
}
[2025-10-26T07:26:50.574Z] [LOG] [API] WebGPU available
[2025-10-26T07:26:50.577Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:26:50.577Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:26:50.577Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:26:50.594Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:27:50.419Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:27:50.419Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:27:50.422Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:27:50.422Z] [LOG] [API] Checking server status...
[2025-10-26T07:27:50.432Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:27:50.425Z"
}
[2025-10-26T07:27:50.432Z] [LOG] [API] WebGPU available
[2025-10-26T07:27:50.438Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:27:50.438Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:27:50.438Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:27:50.458Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:28:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:28:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:28:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:28:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T07:28:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:28:50.401Z"
}
[2025-10-26T07:28:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T07:28:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:28:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:28:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:28:50.435Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:29:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:29:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:29:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:29:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T07:29:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:29:50.399Z"
}
[2025-10-26T07:29:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T07:29:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:29:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:29:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:29:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:30:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:30:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:30:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:30:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T07:30:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:30:50.410Z"
}
[2025-10-26T07:30:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T07:30:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:30:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:30:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:30:50.450Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:31:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:31:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:31:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:31:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T07:31:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:31:50.398Z"
}
[2025-10-26T07:31:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T07:31:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:31:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:31:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:31:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:32:50.540Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:32:50.540Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:32:50.542Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:32:50.543Z] [LOG] [API] Checking server status...
[2025-10-26T07:32:50.551Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:32:50.544Z"
}
[2025-10-26T07:32:50.551Z] [LOG] [API] WebGPU available
[2025-10-26T07:32:50.554Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:32:50.554Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:32:50.554Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:32:50.561Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:33:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:33:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:33:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:33:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T07:33:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:33:50.413Z"
}
[2025-10-26T07:33:50.422Z] [LOG] [API] WebGPU available
[2025-10-26T07:33:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:33:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:33:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:33:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:34:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:34:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:34:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:34:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T07:34:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:34:50.413Z"
}
[2025-10-26T07:34:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T07:34:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:34:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:34:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:34:50.449Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:35:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:35:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:35:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:35:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T07:35:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:35:50.405Z"
}
[2025-10-26T07:35:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T07:35:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:35:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:35:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:35:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:36:50.401Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:36:50.401Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:36:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:36:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T07:36:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:36:50.410Z"
}
[2025-10-26T07:36:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T07:36:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:36:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:36:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:36:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:37:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:37:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:37:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:37:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T07:37:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:37:50.404Z"
}
[2025-10-26T07:37:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T07:37:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:37:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:37:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:37:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:38:50.539Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:38:50.539Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:38:50.541Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:38:50.541Z] [LOG] [API] Checking server status...
[2025-10-26T07:38:50.550Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:38:50.542Z"
}
[2025-10-26T07:38:50.550Z] [LOG] [API] WebGPU available
[2025-10-26T07:38:50.557Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:38:50.557Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:38:50.557Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:38:50.561Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:39:50.409Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:39:50.409Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:39:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:39:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T07:39:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:39:50.413Z"
}
[2025-10-26T07:39:50.422Z] [LOG] [API] WebGPU available
[2025-10-26T07:39:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:39:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:39:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:39:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:40:50.394Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:40:50.394Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:40:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:40:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T07:40:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:40:50.398Z"
}
[2025-10-26T07:40:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T07:40:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:40:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:40:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:40:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:41:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:41:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:41:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:41:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T07:41:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:41:50.397Z"
}
[2025-10-26T07:41:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T07:41:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:41:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:41:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:41:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:42:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:42:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:42:50.413Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:42:50.413Z] [LOG] [API] Checking server status...
[2025-10-26T07:42:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:42:50.416Z"
}
[2025-10-26T07:42:50.422Z] [LOG] [API] WebGPU available
[2025-10-26T07:42:50.428Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:42:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:42:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:42:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:43:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:43:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:43:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:43:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T07:43:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:43:50.411Z"
}
[2025-10-26T07:43:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T07:43:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:43:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:43:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:43:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:44:50.540Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:44:50.540Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:44:50.542Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:44:50.542Z] [LOG] [API] Checking server status...
[2025-10-26T07:44:50.551Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:44:50.543Z"
}
[2025-10-26T07:44:50.551Z] [LOG] [API] WebGPU available
[2025-10-26T07:44:50.557Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:44:50.557Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:44:50.558Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:44:50.562Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:45:50.392Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:45:50.392Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:45:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:45:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T07:45:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:45:50.398Z"
}
[2025-10-26T07:45:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T07:45:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:45:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:45:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:45:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:46:50.403Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:46:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:46:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:46:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T07:46:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:46:50.407Z"
}
[2025-10-26T07:46:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T07:46:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:46:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:46:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:46:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:47:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:47:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:47:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:47:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T07:47:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:47:50.411Z"
}
[2025-10-26T07:47:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T07:47:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:47:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:47:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:47:50.450Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:48:50.403Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:48:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:48:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:48:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T07:48:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:48:50.408Z"
}
[2025-10-26T07:48:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T07:48:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:48:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:48:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:48:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:49:50.400Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:49:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:49:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:49:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T07:49:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:49:50.406Z"
}
[2025-10-26T07:49:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T07:49:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:49:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:49:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:49:50.435Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:50:50.506Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:50:50.506Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:50:50.509Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:50:50.509Z] [LOG] [API] Checking server status...
[2025-10-26T07:50:50.519Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:50:50.510Z"
}
[2025-10-26T07:50:50.519Z] [LOG] [API] WebGPU available
[2025-10-26T07:50:50.525Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:50:50.525Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:50:50.525Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:50:50.536Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:51:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:51:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:51:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:51:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T07:51:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:51:50.404Z"
}
[2025-10-26T07:51:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T07:51:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:51:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:51:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:51:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:52:50.405Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:52:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:52:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:52:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T07:52:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:52:50.411Z"
}
[2025-10-26T07:52:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T07:52:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:52:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:52:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:52:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:53:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:53:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:53:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:53:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T07:53:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:53:50.412Z"
}
[2025-10-26T07:53:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T07:53:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:53:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:53:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:53:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:54:50.415Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:54:50.416Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:54:50.448Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:54:50.448Z] [LOG] [API] Checking server status...
[2025-10-26T07:54:50.460Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:54:50.451Z"
}
[2025-10-26T07:54:50.460Z] [LOG] [API] WebGPU available
[2025-10-26T07:54:50.472Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:54:50.472Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:54:50.472Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:54:50.489Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:55:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:55:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:55:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:55:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T07:55:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:55:50.412Z"
}
[2025-10-26T07:55:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T07:55:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:55:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:55:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:55:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:56:50.518Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:56:50.518Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:56:50.520Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:56:50.520Z] [LOG] [API] Checking server status...
[2025-10-26T07:56:50.528Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:56:50.521Z"
}
[2025-10-26T07:56:50.528Z] [LOG] [API] WebGPU available
[2025-10-26T07:56:50.531Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:56:50.531Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:56:50.531Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:56:50.538Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:57:50.390Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:57:50.390Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:57:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:57:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T07:57:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:57:50.395Z"
}
[2025-10-26T07:57:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T07:57:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:57:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:57:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:57:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:58:50.405Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:58:50.405Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:58:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:58:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T07:58:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:58:50.411Z"
}
[2025-10-26T07:58:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T07:58:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:58:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:58:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:58:50.449Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T07:59:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T07:59:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T07:59:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T07:59:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T07:59:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T07:59:50.408Z"
}
[2025-10-26T07:59:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T07:59:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T07:59:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T07:59:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T07:59:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:00:50.400Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:00:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:00:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:00:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T08:00:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:00:50.406Z"
}
[2025-10-26T08:00:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T08:00:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:00:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:00:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:00:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:01:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:01:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:01:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:01:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T08:01:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:01:50.409Z"
}
[2025-10-26T08:01:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T08:01:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:01:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:01:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:01:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:02:50.414Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:02:50.414Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:02:50.442Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:02:50.442Z] [LOG] [API] Checking server status...
[2025-10-26T08:02:50.451Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:02:50.443Z"
}
[2025-10-26T08:02:50.451Z] [LOG] [API] WebGPU available
[2025-10-26T08:02:50.458Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:02:50.458Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:02:50.458Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:02:50.463Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:03:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:03:50.396Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:03:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:03:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T08:03:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:03:50.400Z"
}
[2025-10-26T08:03:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T08:03:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:03:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:03:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:03:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:04:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:04:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:04:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:04:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T08:04:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:04:50.414Z"
}
[2025-10-26T08:04:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T08:04:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:04:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:04:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:04:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:05:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:05:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:05:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:05:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T08:05:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:05:50.397Z"
}
[2025-10-26T08:05:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T08:05:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:05:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:05:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:05:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:06:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:06:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:06:50.418Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:06:50.418Z] [LOG] [API] Checking server status...
[2025-10-26T08:06:50.427Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:06:50.421Z"
}
[2025-10-26T08:06:50.427Z] [LOG] [API] WebGPU available
[2025-10-26T08:06:50.431Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:06:50.431Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:06:50.431Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:06:50.451Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:07:50.411Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:07:50.411Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:07:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:07:50.415Z] [LOG] [API] Checking server status...
[2025-10-26T08:07:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:07:50.417Z"
}
[2025-10-26T08:07:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T08:07:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:07:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:07:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:07:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:08:50.619Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:08:50.619Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:08:50.621Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:08:50.621Z] [LOG] [API] Checking server status...
[2025-10-26T08:08:50.630Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:08:50.622Z"
}
[2025-10-26T08:08:50.630Z] [LOG] [API] WebGPU available
[2025-10-26T08:08:50.637Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:08:50.637Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:08:50.637Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:08:50.641Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:09:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:09:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:09:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:09:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T08:09:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:09:50.409Z"
}
[2025-10-26T08:09:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T08:09:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:09:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:09:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:09:50.431Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:10:50.414Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:10:50.414Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:10:50.418Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:10:50.418Z] [LOG] [API] Checking server status...
[2025-10-26T08:10:50.428Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:10:50.419Z"
}
[2025-10-26T08:10:50.428Z] [LOG] [API] WebGPU available
[2025-10-26T08:10:50.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:10:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:10:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:10:50.454Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:11:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:11:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:11:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:11:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T08:11:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:11:50.411Z"
}
[2025-10-26T08:11:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T08:11:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:11:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:11:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:11:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:12:50.411Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:12:50.411Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:12:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:12:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T08:12:50.424Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:12:50.416Z"
}
[2025-10-26T08:12:50.424Z] [LOG] [API] WebGPU available
[2025-10-26T08:12:50.428Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:12:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:12:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:12:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:13:50.422Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:13:50.422Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:13:50.425Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:13:50.425Z] [LOG] [API] Checking server status...
[2025-10-26T08:13:50.435Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:13:50.426Z"
}
[2025-10-26T08:13:50.435Z] [LOG] [API] WebGPU available
[2025-10-26T08:13:50.439Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:13:50.439Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:13:50.439Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:13:50.457Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:14:50.493Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:14:50.493Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:14:50.495Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:14:50.495Z] [LOG] [API] Checking server status...
[2025-10-26T08:14:50.504Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:14:50.497Z"
}
[2025-10-26T08:14:50.504Z] [LOG] [API] WebGPU available
[2025-10-26T08:14:50.507Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:14:50.507Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:14:50.507Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:14:50.523Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:15:50.411Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:15:50.411Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:15:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:15:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T08:15:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:15:50.416Z"
}
[2025-10-26T08:15:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T08:15:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:15:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:15:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:15:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:16:50.413Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:16:50.413Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:16:50.416Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:16:50.416Z] [LOG] [API] Checking server status...
[2025-10-26T08:16:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:16:50.418Z"
}
[2025-10-26T08:16:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T08:16:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:16:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:16:50.431Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:16:50.451Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:17:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:17:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:17:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:17:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T08:17:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:17:50.409Z"
}
[2025-10-26T08:17:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T08:17:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:17:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:17:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:17:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:18:50.420Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:18:50.420Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:18:50.424Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:18:50.424Z] [LOG] [API] Checking server status...
[2025-10-26T08:18:50.433Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:18:50.425Z"
}
[2025-10-26T08:18:50.433Z] [LOG] [API] WebGPU available
[2025-10-26T08:18:50.438Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:18:50.438Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:18:50.438Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:18:50.458Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:19:50.421Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:19:50.421Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:19:50.424Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:19:50.424Z] [LOG] [API] Checking server status...
[2025-10-26T08:19:50.436Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:19:50.425Z"
}
[2025-10-26T08:19:50.436Z] [LOG] [API] WebGPU available
[2025-10-26T08:19:50.444Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:19:50.444Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:19:50.444Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:19:50.451Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:20:50.502Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:20:50.502Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:20:50.524Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:20:50.524Z] [LOG] [API] Checking server status...
[2025-10-26T08:20:50.533Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:20:50.525Z"
}
[2025-10-26T08:20:50.533Z] [LOG] [API] WebGPU available
[2025-10-26T08:20:50.540Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:20:50.540Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:20:50.540Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:20:50.547Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:21:50.411Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:21:50.412Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:21:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:21:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T08:21:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:21:50.416Z"
}
[2025-10-26T08:21:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T08:21:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:21:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:21:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:21:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:22:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:22:50.397Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:22:50.423Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:22:50.423Z] [LOG] [API] Checking server status...
[2025-10-26T08:22:50.433Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:22:50.424Z"
}
[2025-10-26T08:22:50.433Z] [LOG] [API] WebGPU available
[2025-10-26T08:22:50.441Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:22:50.441Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:22:50.441Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:22:50.485Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:23:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:23:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:23:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:23:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T08:23:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:23:50.413Z"
}
[2025-10-26T08:23:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T08:23:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:23:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:23:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:23:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:24:50.396Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:24:50.396Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:24:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:24:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T08:24:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:24:50.400Z"
}
[2025-10-26T08:24:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T08:24:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:24:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:24:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:24:50.435Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:25:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:25:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:25:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:25:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T08:25:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:25:50.408Z"
}
[2025-10-26T08:25:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T08:25:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:25:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:25:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:25:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:26:50.475Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:26:50.475Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:26:50.478Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:26:50.478Z] [LOG] [API] Checking server status...
[2025-10-26T08:26:50.488Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:26:50.479Z"
}
[2025-10-26T08:26:50.488Z] [LOG] [API] WebGPU available
[2025-10-26T08:26:50.494Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:26:50.494Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:26:50.494Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:26:50.505Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:27:50.401Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:27:50.401Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:27:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:27:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T08:27:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:27:50.405Z"
}
[2025-10-26T08:27:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T08:27:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:27:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:27:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:27:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:28:50.409Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:28:50.409Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:28:50.417Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:28:50.417Z] [LOG] [API] Checking server status...
[2025-10-26T08:28:50.425Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:28:50.420Z"
}
[2025-10-26T08:28:50.425Z] [LOG] [API] WebGPU available
[2025-10-26T08:28:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:28:50.429Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:28:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:28:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:29:50.409Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:29:50.409Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:29:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:29:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T08:29:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:29:50.414Z"
}
[2025-10-26T08:29:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T08:29:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:29:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:29:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:29:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:30:50.415Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:30:50.416Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:30:50.419Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:30:50.419Z] [LOG] [API] Checking server status...
[2025-10-26T08:30:50.430Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:30:50.420Z"
}
[2025-10-26T08:30:50.430Z] [LOG] [API] WebGPU available
[2025-10-26T08:30:50.435Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:30:50.435Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:30:50.435Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:30:50.458Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:31:50.411Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:31:50.411Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:31:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:31:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T08:31:50.424Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:31:50.416Z"
}
[2025-10-26T08:31:50.424Z] [LOG] [API] WebGPU available
[2025-10-26T08:31:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:31:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:31:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:31:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:32:50.472Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:32:50.472Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:32:50.475Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:32:50.475Z] [LOG] [API] Checking server status...
[2025-10-26T08:32:50.485Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:32:50.477Z"
}
[2025-10-26T08:32:50.485Z] [LOG] [API] WebGPU available
[2025-10-26T08:32:50.488Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:32:50.488Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:32:50.489Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:32:50.498Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:33:50.412Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:33:50.412Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:33:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:33:50.415Z] [LOG] [API] Checking server status...
[2025-10-26T08:33:50.427Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:33:50.419Z"
}
[2025-10-26T08:33:50.427Z] [LOG] [API] WebGPU available
[2025-10-26T08:33:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:33:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:33:50.434Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:33:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:34:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:34:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:34:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:34:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T08:34:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:34:50.407Z"
}
[2025-10-26T08:34:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T08:34:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:34:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:34:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:34:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:35:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:35:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:35:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:35:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T08:35:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:35:50.412Z"
}
[2025-10-26T08:35:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T08:35:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:35:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:35:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:35:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:36:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:36:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:36:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:36:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T08:36:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:36:50.403Z"
}
[2025-10-26T08:36:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T08:36:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:36:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:36:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:36:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:37:50.401Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:37:50.401Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:37:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:37:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T08:37:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:37:50.406Z"
}
[2025-10-26T08:37:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T08:37:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:37:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:37:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:37:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:38:50.478Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:38:50.478Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:38:50.480Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:38:50.480Z] [LOG] [API] Checking server status...
[2025-10-26T08:38:50.490Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:38:50.482Z"
}
[2025-10-26T08:38:50.490Z] [LOG] [API] WebGPU available
[2025-10-26T08:38:50.497Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:38:50.497Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:38:50.497Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:38:50.502Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:39:50.422Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:39:50.422Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:39:50.424Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:39:50.424Z] [LOG] [API] Checking server status...
[2025-10-26T08:39:50.435Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:39:50.426Z"
}
[2025-10-26T08:39:50.435Z] [LOG] [API] WebGPU available
[2025-10-26T08:39:50.439Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:39:50.439Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:39:50.439Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:39:50.456Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:40:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:40:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:40:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:40:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T08:40:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:40:50.407Z"
}
[2025-10-26T08:40:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T08:40:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:40:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:40:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:40:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:41:50.400Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:41:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:41:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:41:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T08:41:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:41:50.405Z"
}
[2025-10-26T08:41:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T08:41:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:41:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:41:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:41:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:42:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:42:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:42:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:42:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T08:42:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:42:50.407Z"
}
[2025-10-26T08:42:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T08:42:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:42:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:42:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:42:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:43:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:43:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:43:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:43:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T08:43:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:43:50.379Z"
}
[2025-10-26T08:43:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T08:43:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:43:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:43:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:43:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:44:50.535Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:44:50.535Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:44:50.537Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:44:50.537Z] [LOG] [API] Checking server status...
[2025-10-26T08:44:50.546Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:44:50.539Z"
}
[2025-10-26T08:44:50.546Z] [LOG] [API] WebGPU available
[2025-10-26T08:44:50.552Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:44:50.552Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:44:50.552Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:44:50.557Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:45:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:45:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:45:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:45:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T08:45:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:45:50.410Z"
}
[2025-10-26T08:45:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T08:45:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:45:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:45:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:45:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:46:50.417Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:46:50.418Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:46:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:46:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T08:46:50.431Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:46:50.423Z"
}
[2025-10-26T08:46:50.431Z] [LOG] [API] WebGPU available
[2025-10-26T08:46:50.439Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:46:50.439Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:46:50.439Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:46:50.451Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:47:50.405Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:47:50.405Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:47:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:47:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T08:47:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:47:50.409Z"
}
[2025-10-26T08:47:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T08:47:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:47:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:47:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:47:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:48:50.415Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:48:50.415Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:48:50.419Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:48:50.419Z] [LOG] [API] Checking server status...
[2025-10-26T08:48:50.428Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:48:50.421Z"
}
[2025-10-26T08:48:50.428Z] [LOG] [API] WebGPU available
[2025-10-26T08:48:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:48:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:48:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:48:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:49:50.416Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:49:50.416Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:49:50.420Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:49:50.420Z] [LOG] [API] Checking server status...
[2025-10-26T08:49:50.430Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:49:50.422Z"
}
[2025-10-26T08:49:50.431Z] [LOG] [API] WebGPU available
[2025-10-26T08:49:50.439Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:49:50.439Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:49:50.439Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:49:50.450Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:50:50.558Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:50:50.559Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:50:50.561Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:50:50.561Z] [LOG] [API] Checking server status...
[2025-10-26T08:50:50.569Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:50:50.562Z"
}
[2025-10-26T08:50:50.570Z] [LOG] [API] WebGPU available
[2025-10-26T08:50:50.576Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:50:50.577Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:50:50.577Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:50:50.582Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:51:50.420Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:51:50.420Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:51:50.423Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:51:50.423Z] [LOG] [API] Checking server status...
[2025-10-26T08:51:50.436Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:51:50.425Z"
}
[2025-10-26T08:51:50.436Z] [LOG] [API] WebGPU available
[2025-10-26T08:51:50.442Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:51:50.442Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:51:50.442Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:51:50.456Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:52:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:52:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:52:50.437Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:52:50.437Z] [LOG] [API] Checking server status...
[2025-10-26T08:52:50.447Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:52:50.439Z"
}
[2025-10-26T08:52:50.447Z] [LOG] [API] WebGPU available
[2025-10-26T08:52:50.454Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:52:50.454Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:52:50.454Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:52:50.495Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:53:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:53:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:53:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:53:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T08:53:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:53:50.386Z"
}
[2025-10-26T08:53:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T08:53:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:53:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:53:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:53:50.459Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:54:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:54:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:54:50.432Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:54:50.432Z] [LOG] [API] Checking server status...
[2025-10-26T08:54:50.443Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:54:50.433Z"
}
[2025-10-26T08:54:50.443Z] [LOG] [API] WebGPU available
[2025-10-26T08:54:50.489Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:54:50.490Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:54:50.490Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:54:50.509Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:55:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:55:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:55:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:55:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T08:55:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:55:50.385Z"
}
[2025-10-26T08:55:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T08:55:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:55:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:55:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:55:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:56:50.490Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:56:50.490Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:56:50.492Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:56:50.492Z] [LOG] [API] Checking server status...
[2025-10-26T08:56:50.500Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:56:50.494Z"
}
[2025-10-26T08:56:50.500Z] [LOG] [API] WebGPU available
[2025-10-26T08:56:50.504Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:56:50.504Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:56:50.504Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:56:50.520Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:57:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:57:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:57:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:57:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T08:57:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:57:50.382Z"
}
[2025-10-26T08:57:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T08:57:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:57:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:57:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:57:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:58:50.405Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:58:50.405Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:58:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:58:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T08:58:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:58:50.410Z"
}
[2025-10-26T08:58:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T08:58:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:58:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:58:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:58:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T08:59:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T08:59:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T08:59:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T08:59:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T08:59:50.435Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T08:59:50.386Z"
}
[2025-10-26T08:59:50.435Z] [LOG] [API] WebGPU available
[2025-10-26T08:59:50.440Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T08:59:50.440Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T08:59:50.440Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T08:59:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:00:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:00:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:00:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:00:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T09:00:50.431Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:00:50.422Z"
}
[2025-10-26T09:00:50.431Z] [LOG] [API] WebGPU available
[2025-10-26T09:00:50.437Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:00:50.437Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:00:50.437Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:00:50.462Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:01:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:01:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:01:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:01:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T09:01:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:01:50.382Z"
}
[2025-10-26T09:01:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T09:01:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:01:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:01:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:01:50.462Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:02:50.420Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:02:50.420Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:02:50.423Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:02:50.423Z] [LOG] [API] Checking server status...
[2025-10-26T09:02:50.434Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:02:50.424Z"
}
[2025-10-26T09:02:50.434Z] [LOG] [API] WebGPU available
[2025-10-26T09:02:50.441Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:02:50.441Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:02:50.441Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:02:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:03:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:03:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:03:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:03:50.385Z] [LOG] [API] Checking server status...
[2025-10-26T09:03:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:03:50.387Z"
}
[2025-10-26T09:03:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T09:03:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:03:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:03:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:03:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:04:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:04:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:04:50.442Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:04:50.442Z] [LOG] [API] Checking server status...
[2025-10-26T09:04:50.470Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:04:50.443Z"
}
[2025-10-26T09:04:50.470Z] [LOG] [API] WebGPU available
[2025-10-26T09:04:50.479Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:04:50.479Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:04:50.479Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:04:50.491Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:05:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:05:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:05:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:05:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T09:05:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:05:50.387Z"
}
[2025-10-26T09:05:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T09:05:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:05:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:05:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:05:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:06:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:06:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:06:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:06:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T09:06:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:06:50.396Z"
}
[2025-10-26T09:06:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T09:06:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:06:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:06:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:06:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:07:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:07:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:07:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:07:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T09:07:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:07:50.384Z"
}
[2025-10-26T09:07:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T09:07:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:07:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:07:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:07:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:08:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:08:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:08:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:08:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T09:08:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:08:50.383Z"
}
[2025-10-26T09:08:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T09:08:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:08:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:08:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:08:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:09:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:09:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:09:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:09:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T09:09:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:09:50.382Z"
}
[2025-10-26T09:09:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T09:09:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:09:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:09:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:09:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:10:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:10:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:10:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:10:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T09:10:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:10:50.403Z"
}
[2025-10-26T09:10:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T09:10:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:10:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:10:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:10:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:11:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:11:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:11:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:11:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T09:11:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:11:50.382Z"
}
[2025-10-26T09:11:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T09:11:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:11:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:11:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:11:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:12:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:12:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:12:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:12:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T09:12:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:12:50.397Z"
}
[2025-10-26T09:12:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T09:12:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:12:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:12:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:12:50.462Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:13:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:13:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:13:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:13:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T09:13:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:13:50.381Z"
}
[2025-10-26T09:13:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T09:13:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:13:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:13:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:13:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:14:50.494Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:14:50.494Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:14:50.497Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:14:50.497Z] [LOG] [API] Checking server status...
[2025-10-26T09:14:50.506Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:14:50.498Z"
}
[2025-10-26T09:14:50.506Z] [LOG] [API] WebGPU available
[2025-10-26T09:14:50.510Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:14:50.510Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:14:50.510Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:14:50.517Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:15:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:15:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:15:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:15:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T09:15:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:15:50.386Z"
}
[2025-10-26T09:15:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T09:15:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:15:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:15:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:15:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:16:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:16:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:16:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:16:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T09:16:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:16:50.384Z"
}
[2025-10-26T09:16:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T09:16:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:16:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:16:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:16:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:17:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:17:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:17:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:17:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T09:17:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:17:50.386Z"
}
[2025-10-26T09:17:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T09:17:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:17:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:17:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:17:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:18:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:18:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:18:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:18:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T09:18:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:18:50.399Z"
}
[2025-10-26T09:18:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T09:18:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:18:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:18:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:18:50.464Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:19:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:19:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:19:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:19:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T09:19:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:19:50.383Z"
}
[2025-10-26T09:19:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T09:19:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:19:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:19:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:19:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:20:50.507Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:20:50.507Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:20:50.510Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:20:50.510Z] [LOG] [API] Checking server status...
[2025-10-26T09:20:50.521Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:20:50.511Z"
}
[2025-10-26T09:20:50.521Z] [LOG] [API] WebGPU available
[2025-10-26T09:20:50.527Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:20:50.528Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:20:50.528Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:20:50.533Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:21:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:21:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:21:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:21:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T09:21:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:21:50.378Z"
}
[2025-10-26T09:21:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T09:21:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:21:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:21:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:21:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:22:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:22:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:22:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:22:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T09:22:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:22:50.383Z"
}
[2025-10-26T09:22:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T09:22:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:22:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:22:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:22:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:23:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:23:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:23:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:23:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T09:23:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:23:50.381Z"
}
[2025-10-26T09:23:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T09:23:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:23:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:23:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:23:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:24:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:24:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:24:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:24:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T09:24:50.450Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:24:50.413Z"
}
[2025-10-26T09:24:50.450Z] [LOG] [API] WebGPU available
[2025-10-26T09:24:50.490Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:24:50.491Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:24:50.491Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:24:50.496Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:25:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:25:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:25:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:25:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T09:25:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:25:50.411Z"
}
[2025-10-26T09:25:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T09:25:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:25:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:25:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:25:50.449Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:26:50.536Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:26:50.536Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:26:50.538Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:26:50.538Z] [LOG] [API] Checking server status...
[2025-10-26T09:26:50.547Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:26:50.539Z"
}
[2025-10-26T09:26:50.547Z] [LOG] [API] WebGPU available
[2025-10-26T09:26:50.550Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:26:50.550Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:26:50.550Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:26:50.556Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:27:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:27:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:27:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:27:50.372Z] [LOG] [API] Checking server status...
[2025-10-26T09:27:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:27:50.374Z"
}
[2025-10-26T09:27:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T09:27:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:27:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:27:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:27:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:28:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:28:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:28:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:28:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T09:28:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:28:50.395Z"
}
[2025-10-26T09:28:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T09:28:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:28:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:28:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:28:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:29:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:29:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:29:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:29:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T09:29:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:29:50.376Z"
}
[2025-10-26T09:29:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T09:29:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:29:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:29:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:29:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:30:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:30:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:30:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:30:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T09:30:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:30:50.388Z"
}
[2025-10-26T09:30:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T09:30:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:30:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:30:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:30:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:31:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:31:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:31:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:31:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T09:31:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:31:50.377Z"
}
[2025-10-26T09:31:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T09:31:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:31:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:31:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:31:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:32:50.550Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:32:50.550Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:32:50.553Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:32:50.553Z] [LOG] [API] Checking server status...
[2025-10-26T09:32:50.564Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:32:50.554Z"
}
[2025-10-26T09:32:50.564Z] [LOG] [API] WebGPU available
[2025-10-26T09:32:50.572Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:32:50.572Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:32:50.572Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:32:50.577Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:33:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:33:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:33:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:33:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T09:33:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:33:50.377Z"
}
[2025-10-26T09:33:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T09:33:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:33:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:33:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:33:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:34:50.410Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:34:50.410Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:34:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:34:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T09:34:50.424Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:34:50.417Z"
}
[2025-10-26T09:34:50.424Z] [LOG] [API] WebGPU available
[2025-10-26T09:34:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:34:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:34:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:34:50.449Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:35:50.413Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:35:50.413Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:35:50.416Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:35:50.417Z] [LOG] [API] Checking server status...
[2025-10-26T09:35:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:35:50.418Z"
}
[2025-10-26T09:35:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T09:35:50.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:35:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:35:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:35:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:36:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:36:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:36:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:36:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T09:36:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:36:50.375Z"
}
[2025-10-26T09:36:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T09:36:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:36:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:36:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:36:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:37:50.400Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:37:50.401Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:37:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:37:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T09:37:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:37:50.405Z"
}
[2025-10-26T09:37:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T09:37:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:37:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:37:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:37:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:38:50.529Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:38:50.529Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:38:50.531Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:38:50.531Z] [LOG] [API] Checking server status...
[2025-10-26T09:38:50.541Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:38:50.532Z"
}
[2025-10-26T09:38:50.541Z] [LOG] [API] WebGPU available
[2025-10-26T09:38:50.544Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:38:50.544Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:38:50.544Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:38:50.552Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:39:50.405Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:39:50.405Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:39:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:39:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T09:39:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:39:50.411Z"
}
[2025-10-26T09:39:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T09:39:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:39:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:39:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:39:50.435Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:40:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:40:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:40:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:40:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T09:40:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:40:50.374Z"
}
[2025-10-26T09:40:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T09:40:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:40:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:40:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:40:50.401Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:41:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:41:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:41:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:41:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T09:41:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:41:50.381Z"
}
[2025-10-26T09:41:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T09:41:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:41:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:41:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:41:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:42:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:42:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:42:50.425Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:42:50.425Z] [LOG] [API] Checking server status...
[2025-10-26T09:42:50.435Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:42:50.426Z"
}
[2025-10-26T09:42:50.435Z] [LOG] [API] WebGPU available
[2025-10-26T09:42:50.439Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:42:50.440Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:42:50.440Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:42:50.450Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:43:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:43:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:43:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:43:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T09:43:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:43:50.366Z"
}
[2025-10-26T09:43:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T09:43:50.382Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:43:50.382Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:43:50.382Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:43:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:44:50.473Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:44:50.473Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:44:50.475Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:44:50.475Z] [LOG] [API] Checking server status...
[2025-10-26T09:44:50.485Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:44:50.476Z"
}
[2025-10-26T09:44:50.485Z] [LOG] [API] WebGPU available
[2025-10-26T09:44:50.491Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:44:50.491Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:44:50.491Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:44:50.496Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:45:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:45:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:45:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:45:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T09:45:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:45:50.373Z"
}
[2025-10-26T09:45:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T09:45:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:45:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:45:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:45:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:46:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:46:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:46:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:46:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T09:46:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:46:50.407Z"
}
[2025-10-26T09:46:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T09:46:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:46:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:46:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:46:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:47:50.403Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:47:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:47:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:47:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T09:47:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:47:50.409Z"
}
[2025-10-26T09:47:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T09:47:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:47:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:47:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:47:50.450Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:48:50.405Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:48:50.405Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:48:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:48:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T09:48:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:48:50.411Z"
}
[2025-10-26T09:48:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T09:48:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:48:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:48:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:48:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:49:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:49:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:49:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:49:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T09:49:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:49:50.413Z"
}
[2025-10-26T09:49:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T09:49:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:49:50.429Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:49:50.429Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:49:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:50:50.423Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:50:50.423Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:50:50.427Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:50:50.427Z] [LOG] [API] Checking server status...
[2025-10-26T09:50:50.439Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:50:50.429Z"
}
[2025-10-26T09:50:50.439Z] [LOG] [API] WebGPU available
[2025-10-26T09:50:50.445Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:50:50.445Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:50:50.445Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:50:50.473Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:51:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:51:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:51:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:51:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T09:51:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:51:50.374Z"
}
[2025-10-26T09:51:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T09:51:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:51:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:51:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:51:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:52:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:52:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:52:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:52:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T09:52:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:52:50.391Z"
}
[2025-10-26T09:52:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T09:52:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:52:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:52:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:52:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:53:50.412Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:53:50.413Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:53:50.416Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:53:50.416Z] [LOG] [API] Checking server status...
[2025-10-26T09:53:50.427Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:53:50.418Z"
}
[2025-10-26T09:53:50.427Z] [LOG] [API] WebGPU available
[2025-10-26T09:53:50.434Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:53:50.434Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:53:50.435Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:53:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:54:50.407Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:54:50.407Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:54:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:54:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T09:54:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:54:50.413Z"
}
[2025-10-26T09:54:50.422Z] [LOG] [API] WebGPU available
[2025-10-26T09:54:50.425Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:54:50.425Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:54:50.425Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:54:50.447Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:55:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:55:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:55:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:55:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T09:55:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:55:50.407Z"
}
[2025-10-26T09:55:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T09:55:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:55:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:55:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:55:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:56:50.489Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:56:50.489Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:56:50.491Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:56:50.491Z] [LOG] [API] Checking server status...
[2025-10-26T09:56:50.501Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:56:50.492Z"
}
[2025-10-26T09:56:50.501Z] [LOG] [API] WebGPU available
[2025-10-26T09:56:50.503Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:56:50.503Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:56:50.504Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:56:50.510Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:57:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:57:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:57:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:57:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T09:57:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:57:50.371Z"
}
[2025-10-26T09:57:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T09:57:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:57:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:57:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:57:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:58:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:58:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:58:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:58:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T09:58:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:58:50.394Z"
}
[2025-10-26T09:58:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T09:58:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:58:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:58:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:58:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T09:59:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T09:59:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T09:59:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T09:59:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T09:59:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T09:59:50.375Z"
}
[2025-10-26T09:59:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T09:59:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T09:59:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T09:59:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T09:59:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:00:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:00:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:00:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:00:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T10:00:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:00:50.407Z"
}
[2025-10-26T10:00:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T10:00:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:00:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:00:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:00:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:01:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:01:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:01:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:01:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T10:01:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:01:50.375Z"
}
[2025-10-26T10:01:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T10:01:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:01:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:01:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:01:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:02:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:02:50.403Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:02:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:02:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T10:02:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:02:50.408Z"
}
[2025-10-26T10:02:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T10:02:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:02:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:02:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:02:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:03:50.395Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:03:50.395Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:03:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:03:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T10:03:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:03:50.400Z"
}
[2025-10-26T10:03:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T10:03:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:03:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:03:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:03:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:04:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:04:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:04:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:04:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T10:04:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:04:50.409Z"
}
[2025-10-26T10:04:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T10:04:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:04:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:04:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:04:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:05:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:05:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:05:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:05:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T10:05:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:05:50.368Z"
}
[2025-10-26T10:05:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T10:05:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:05:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:05:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:05:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:06:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:06:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:06:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:06:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T10:06:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:06:50.393Z"
}
[2025-10-26T10:06:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T10:06:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:06:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:06:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:06:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:07:50.402Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:07:50.402Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:07:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:07:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T10:07:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:07:50.408Z"
}
[2025-10-26T10:07:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T10:07:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:07:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:07:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:07:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:08:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:08:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:08:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:08:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T10:08:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:08:50.384Z"
}
[2025-10-26T10:08:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T10:08:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:08:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:08:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:08:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:09:50.408Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:09:50.408Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:09:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:09:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T10:09:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:09:50.415Z"
}
[2025-10-26T10:09:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T10:09:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:09:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:09:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:09:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:10:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:10:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:10:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:10:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T10:10:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:10:50.377Z"
}
[2025-10-26T10:10:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T10:10:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:10:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:10:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:10:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:11:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:11:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:11:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:11:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T10:11:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:11:50.382Z"
}
[2025-10-26T10:11:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T10:11:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:11:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:11:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:11:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:12:50.399Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:12:50.399Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:12:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:12:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T10:12:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:12:50.405Z"
}
[2025-10-26T10:12:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T10:12:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:12:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:12:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:12:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:13:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:13:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:13:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:13:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T10:13:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:13:50.371Z"
}
[2025-10-26T10:13:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T10:13:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:13:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:13:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:13:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:14:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:14:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:14:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:14:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T10:14:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:14:50.378Z"
}
[2025-10-26T10:14:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T10:14:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:14:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:14:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:14:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:15:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:15:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:15:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:15:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T10:15:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:15:50.372Z"
}
[2025-10-26T10:15:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T10:15:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:15:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:15:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:15:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:16:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:16:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:16:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:16:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T10:16:50.425Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:16:50.412Z"
}
[2025-10-26T10:16:50.425Z] [LOG] [API] WebGPU available
[2025-10-26T10:16:50.442Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:16:50.442Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:16:50.442Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:16:50.468Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:17:50.391Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:17:50.391Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:17:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:17:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T10:17:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:17:50.396Z"
}
[2025-10-26T10:17:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T10:17:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:17:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:17:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:17:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:18:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:18:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:18:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:18:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T10:18:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:18:50.409Z"
}
[2025-10-26T10:18:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T10:18:50.436Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:18:50.436Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:18:50.436Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:18:50.456Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:19:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:19:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:19:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:19:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T10:19:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:19:50.376Z"
}
[2025-10-26T10:19:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T10:19:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:19:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:19:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:19:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:20:50.805Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:20:50.806Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:20:50.808Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:20:50.808Z] [LOG] [API] Checking server status...
[2025-10-26T10:20:50.811Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:20:50.809Z"
}
[2025-10-26T10:20:50.811Z] [LOG] [API] WebGPU available
[2025-10-26T10:20:50.815Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:20:50.815Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:20:50.815Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:20:50.830Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:20:55.340Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:20:55.340Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:20:55.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:20:55.395Z] [LOG] [API] Checking server status...
[2025-10-26T10:20:55.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:20:55.397Z"
}
[2025-10-26T10:20:55.407Z] [LOG] [API] WebGPU available
[2025-10-26T10:20:55.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:20:55.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:20:55.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:20:55.467Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:21:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:21:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:21:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:21:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T10:21:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:21:50.366Z"
}
[2025-10-26T10:21:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T10:21:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:21:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:21:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:21:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:22:50.415Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:22:50.416Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:22:50.419Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:22:50.419Z] [LOG] [API] Checking server status...
[2025-10-26T10:22:50.429Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:22:50.420Z"
}
[2025-10-26T10:22:50.429Z] [LOG] [API] WebGPU available
[2025-10-26T10:22:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:22:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:22:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:22:50.455Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:23:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:23:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:23:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:23:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T10:23:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:23:50.365Z"
}
[2025-10-26T10:23:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T10:23:50.380Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:23:50.380Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:23:50.380Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:23:50.392Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:24:50.397Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:24:50.397Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:24:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:24:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T10:24:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:24:50.403Z"
}
[2025-10-26T10:24:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T10:24:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:24:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:24:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:24:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:25:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:25:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:25:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:25:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T10:25:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:25:50.370Z"
}
[2025-10-26T10:25:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T10:25:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:25:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:25:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:25:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:26:50.554Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:26:50.555Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:26:50.557Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:26:50.557Z] [LOG] [API] Checking server status...
[2025-10-26T10:26:50.567Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:26:50.558Z"
}
[2025-10-26T10:26:50.567Z] [LOG] [API] WebGPU available
[2025-10-26T10:26:50.570Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:26:50.570Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:26:50.570Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:26:50.580Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:27:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:27:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:27:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:27:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T10:27:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:27:50.375Z"
}
[2025-10-26T10:27:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T10:27:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:27:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:27:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:27:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:28:50.404Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:28:50.404Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:28:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:28:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T10:28:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:28:50.409Z"
}
[2025-10-26T10:28:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T10:28:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:28:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:28:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:28:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:29:50.354Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:29:50.354Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:29:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:29:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T10:29:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:29:50.362Z"
}
[2025-10-26T10:29:50.375Z] [LOG] [API] WebGPU available
[2025-10-26T10:29:50.379Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:29:50.380Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:29:50.380Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:29:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:30:50.384Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:30:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:30:50.428Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:30:50.428Z] [LOG] [API] Checking server status...
[2025-10-26T10:30:50.438Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:30:50.430Z"
}
[2025-10-26T10:30:50.438Z] [LOG] [API] WebGPU available
[2025-10-26T10:30:50.445Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:30:50.446Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:30:50.446Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:30:50.470Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:31:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:31:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:31:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:31:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T10:31:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:31:50.364Z"
}
[2025-10-26T10:31:50.373Z] [LOG] [API] WebGPU available
[2025-10-26T10:31:50.377Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:31:50.378Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:31:50.378Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:31:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:32:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:32:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:32:50.409Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:32:50.409Z] [LOG] [API] Checking server status...
[2025-10-26T10:32:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:32:50.410Z"
}
[2025-10-26T10:32:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T10:32:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:32:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:32:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:32:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:33:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:33:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:33:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:33:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T10:33:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:33:50.366Z"
}
[2025-10-26T10:33:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T10:33:50.382Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:33:50.382Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:33:50.382Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:33:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:34:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:34:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:34:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:34:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T10:34:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:34:50.399Z"
}
[2025-10-26T10:34:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T10:34:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:34:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:34:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:34:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:35:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:35:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:35:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:35:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T10:35:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:35:50.373Z"
}
[2025-10-26T10:35:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T10:35:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:35:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:35:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:35:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:36:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:36:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:36:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:36:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T10:36:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:36:50.407Z"
}
[2025-10-26T10:36:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T10:36:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:36:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:36:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:36:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:37:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:37:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:37:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:37:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T10:37:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:37:50.372Z"
}
[2025-10-26T10:37:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T10:37:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:37:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:37:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:37:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:38:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:38:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:38:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:38:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T10:38:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:38:50.376Z"
}
[2025-10-26T10:38:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T10:38:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:38:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:38:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:38:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:39:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:39:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:39:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:39:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T10:39:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:39:50.383Z"
}
[2025-10-26T10:39:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T10:39:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:39:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:39:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:39:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:40:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:40:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:40:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:40:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T10:40:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:40:50.368Z"
}
[2025-10-26T10:40:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T10:40:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:40:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:40:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:40:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:41:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:41:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:41:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:41:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T10:41:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:41:50.378Z"
}
[2025-10-26T10:41:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T10:41:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:41:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:41:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:41:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:42:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:42:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:42:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:42:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T10:42:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:42:50.387Z"
}
[2025-10-26T10:42:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T10:42:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:42:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:42:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:42:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:43:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:43:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:43:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:43:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T10:43:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:43:50.370Z"
}
[2025-10-26T10:43:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T10:43:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:43:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:43:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:43:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:44:50.554Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:44:50.554Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:44:50.556Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:44:50.556Z] [LOG] [API] Checking server status...
[2025-10-26T10:44:50.567Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:44:50.558Z"
}
[2025-10-26T10:44:50.567Z] [LOG] [API] WebGPU available
[2025-10-26T10:44:50.574Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:44:50.574Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:44:50.574Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:44:50.580Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:45:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:45:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:45:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:45:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T10:45:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:45:50.377Z"
}
[2025-10-26T10:45:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T10:45:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:45:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:45:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:45:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:46:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:46:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:46:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:46:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T10:46:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:46:50.393Z"
}
[2025-10-26T10:46:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T10:46:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:46:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:46:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:46:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:47:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:47:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:47:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:47:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T10:47:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:47:50.363Z"
}
[2025-10-26T10:47:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T10:47:50.380Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:47:50.380Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:47:50.380Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:47:50.397Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:48:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:48:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:48:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:48:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T10:48:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:48:50.394Z"
}
[2025-10-26T10:48:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T10:48:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:48:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:48:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:48:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:49:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:49:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:49:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:49:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T10:49:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:49:50.376Z"
}
[2025-10-26T10:49:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T10:49:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:49:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:49:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:49:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:50:50.486Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:50:50.486Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:50:50.489Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:50:50.489Z] [LOG] [API] Checking server status...
[2025-10-26T10:50:50.500Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:50:50.490Z"
}
[2025-10-26T10:50:50.500Z] [LOG] [API] WebGPU available
[2025-10-26T10:50:50.506Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:50:50.506Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:50:50.506Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:50:50.512Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:51:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:51:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:51:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:51:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T10:51:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:51:50.379Z"
}
[2025-10-26T10:51:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T10:51:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:51:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:51:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:51:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:52:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:52:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:52:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:52:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T10:52:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:52:50.400Z"
}
[2025-10-26T10:52:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T10:52:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:52:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:52:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:52:50.466Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:53:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:53:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:53:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:53:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T10:53:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:53:50.380Z"
}
[2025-10-26T10:53:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T10:53:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:53:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:53:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:53:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:54:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:54:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:54:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:54:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T10:54:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:54:50.389Z"
}
[2025-10-26T10:54:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T10:54:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:54:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:54:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:54:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:55:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:55:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:55:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:55:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T10:55:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:55:50.376Z"
}
[2025-10-26T10:55:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T10:55:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:55:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:55:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:55:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:56:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:56:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:56:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:56:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T10:56:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:56:50.393Z"
}
[2025-10-26T10:56:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T10:56:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:56:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:56:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:56:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:57:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:57:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:57:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:57:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T10:57:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:57:50.378Z"
}
[2025-10-26T10:57:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T10:57:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:57:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:57:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:57:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:58:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:58:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:58:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:58:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T10:58:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:58:50.370Z"
}
[2025-10-26T10:58:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T10:58:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:58:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:58:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:58:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T10:59:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T10:59:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T10:59:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T10:59:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T10:59:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T10:59:50.385Z"
}
[2025-10-26T10:59:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T10:59:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T10:59:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T10:59:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T10:59:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:00:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:00:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:00:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:00:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T11:00:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:00:50.395Z"
}
[2025-10-26T11:00:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T11:00:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:00:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:00:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:00:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:01:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:01:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:01:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:01:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T11:01:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:01:50.380Z"
}
[2025-10-26T11:01:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T11:01:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:01:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:01:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:01:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:02:50.512Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:02:50.512Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:02:50.515Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:02:50.515Z] [LOG] [API] Checking server status...
[2025-10-26T11:02:50.531Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:02:50.516Z"
}
[2025-10-26T11:02:50.531Z] [LOG] [API] WebGPU available
[2025-10-26T11:02:50.540Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:02:50.540Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:02:50.540Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:02:50.552Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:03:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:03:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:03:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:03:50.372Z] [LOG] [API] Checking server status...
[2025-10-26T11:03:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:03:50.374Z"
}
[2025-10-26T11:03:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T11:03:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:03:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:03:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:03:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:04:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:04:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:04:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:04:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T11:04:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:04:50.388Z"
}
[2025-10-26T11:04:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T11:04:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:04:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:04:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:04:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:05:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:05:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:05:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:05:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T11:05:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:05:50.375Z"
}
[2025-10-26T11:05:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T11:05:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:05:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:05:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:05:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:06:50.353Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:06:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:06:50.363Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:06:50.363Z] [LOG] [API] Checking server status...
[2025-10-26T11:06:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:06:50.364Z"
}
[2025-10-26T11:06:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T11:06:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:06:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:06:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:06:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:07:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:07:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:07:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:07:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T11:07:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:07:50.378Z"
}
[2025-10-26T11:07:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T11:07:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:07:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:07:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:07:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:08:50.589Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:08:50.589Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:08:50.591Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:08:50.591Z] [LOG] [API] Checking server status...
[2025-10-26T11:08:50.600Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:08:50.592Z"
}
[2025-10-26T11:08:50.600Z] [LOG] [API] WebGPU available
[2025-10-26T11:08:50.607Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:08:50.607Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:08:50.607Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:08:50.613Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:09:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:09:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:09:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:09:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T11:09:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:09:50.375Z"
}
[2025-10-26T11:09:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T11:09:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:09:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:09:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:09:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:10:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:10:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:10:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:10:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T11:10:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:10:50.378Z"
}
[2025-10-26T11:10:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T11:10:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:10:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:10:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:10:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:11:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:11:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:11:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:11:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T11:11:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:11:50.377Z"
}
[2025-10-26T11:11:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T11:11:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:11:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:11:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:11:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:12:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:12:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:12:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:12:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T11:12:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:12:50.391Z"
}
[2025-10-26T11:12:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T11:12:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:12:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:12:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:12:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:13:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:13:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:13:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:13:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T11:13:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:13:50.370Z"
}
[2025-10-26T11:13:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T11:13:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:13:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:13:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:13:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:14:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:14:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:14:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:14:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T11:14:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:14:50.390Z"
}
[2025-10-26T11:14:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T11:14:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:14:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:14:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:14:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:15:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:15:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:15:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:15:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T11:15:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:15:50.384Z"
}
[2025-10-26T11:15:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T11:15:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:15:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:15:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:15:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:16:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:16:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:16:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:16:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T11:16:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:16:50.394Z"
}
[2025-10-26T11:16:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T11:16:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:16:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:16:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:16:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:17:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:17:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:17:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:17:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T11:17:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:17:50.375Z"
}
[2025-10-26T11:17:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T11:17:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:17:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:17:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:17:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:18:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:18:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:18:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:18:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T11:18:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:18:50.378Z"
}
[2025-10-26T11:18:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T11:18:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:18:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:18:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:18:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:19:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:19:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:19:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:19:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T11:19:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:19:50.370Z"
}
[2025-10-26T11:19:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T11:19:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:19:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:19:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:19:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:20:50.692Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:20:50.692Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:20:50.694Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:20:50.694Z] [LOG] [API] Checking server status...
[2025-10-26T11:20:50.703Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:20:50.695Z"
}
[2025-10-26T11:20:50.703Z] [LOG] [API] WebGPU available
[2025-10-26T11:20:50.710Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:20:50.710Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:20:50.710Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:20:50.716Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:21:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:21:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:21:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:21:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T11:21:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:21:50.376Z"
}
[2025-10-26T11:21:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T11:21:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:21:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:21:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:21:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:22:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:22:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:22:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:22:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T11:22:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:22:50.385Z"
}
[2025-10-26T11:22:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T11:22:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:22:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:22:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:22:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:23:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:23:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:23:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:23:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T11:23:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:23:50.375Z"
}
[2025-10-26T11:23:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T11:23:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:23:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:23:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:23:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:24:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:24:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:24:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:24:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T11:24:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:24:50.393Z"
}
[2025-10-26T11:24:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T11:24:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:24:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:24:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:24:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:25:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:25:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:25:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:25:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T11:25:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:25:50.379Z"
}
[2025-10-26T11:25:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T11:25:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:25:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:25:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:25:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:26:50.424Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:26:50.424Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:26:50.427Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:26:50.427Z] [LOG] [API] Checking server status...
[2025-10-26T11:26:50.439Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:26:50.428Z"
}
[2025-10-26T11:26:50.439Z] [LOG] [API] WebGPU available
[2025-10-26T11:26:50.447Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:26:50.447Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:26:50.447Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:26:50.456Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:27:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:27:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:27:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:27:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T11:27:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:27:50.370Z"
}
[2025-10-26T11:27:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T11:27:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:27:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:27:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:27:50.398Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:28:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:28:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:28:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:28:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T11:28:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:28:50.384Z"
}
[2025-10-26T11:28:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T11:28:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:28:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:28:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:28:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:29:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:29:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:29:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:29:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T11:29:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:29:50.372Z"
}
[2025-10-26T11:29:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T11:29:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:29:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:29:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:29:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:30:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:30:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:30:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:30:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T11:30:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:30:50.382Z"
}
[2025-10-26T11:30:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T11:30:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:30:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:30:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:30:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:31:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:31:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:31:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:31:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T11:31:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:31:50.373Z"
}
[2025-10-26T11:31:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T11:31:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:31:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:31:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:31:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:32:50.475Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:32:50.475Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:32:50.477Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:32:50.478Z] [LOG] [API] Checking server status...
[2025-10-26T11:32:50.487Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:32:50.479Z"
}
[2025-10-26T11:32:50.487Z] [LOG] [API] WebGPU available
[2025-10-26T11:32:50.490Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:32:50.490Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:32:50.490Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:32:50.500Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:33:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:33:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:33:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:33:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T11:33:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:33:50.371Z"
}
[2025-10-26T11:33:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T11:33:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:33:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:33:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:33:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:34:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:34:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:34:50.408Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:34:50.408Z] [LOG] [API] Checking server status...
[2025-10-26T11:34:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:34:50.410Z"
}
[2025-10-26T11:34:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T11:34:50.428Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:34:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:34:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:34:50.464Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:35:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:35:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:35:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:35:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T11:35:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:35:50.375Z"
}
[2025-10-26T11:35:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T11:35:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:35:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:35:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:35:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:36:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:36:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:36:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:36:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T11:36:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:36:50.395Z"
}
[2025-10-26T11:36:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T11:36:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:36:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:36:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:36:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:37:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:37:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:37:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:37:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T11:37:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:37:50.374Z"
}
[2025-10-26T11:37:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T11:37:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:37:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:37:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:37:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:38:50.566Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:38:50.566Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:38:50.569Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:38:50.569Z] [LOG] [API] Checking server status...
[2025-10-26T11:38:50.579Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:38:50.570Z"
}
[2025-10-26T11:38:50.579Z] [LOG] [API] WebGPU available
[2025-10-26T11:38:50.586Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:38:50.586Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:38:50.586Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:38:50.594Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:39:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:39:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:39:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:39:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T11:39:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:39:50.378Z"
}
[2025-10-26T11:39:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T11:39:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:39:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:39:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:39:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:40:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:40:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:40:50.441Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:40:50.441Z] [LOG] [API] Checking server status...
[2025-10-26T11:40:50.453Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:40:50.443Z"
}
[2025-10-26T11:40:50.453Z] [LOG] [API] WebGPU available
[2025-10-26T11:40:50.462Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:40:50.462Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:40:50.462Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:40:50.477Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:41:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:41:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:41:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:41:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T11:41:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:41:50.375Z"
}
[2025-10-26T11:41:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T11:41:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:41:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:41:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:41:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:42:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:42:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:42:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:42:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T11:42:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:42:50.375Z"
}
[2025-10-26T11:42:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T11:42:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:42:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:42:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:42:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:43:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:43:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:43:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:43:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T11:43:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:43:50.368Z"
}
[2025-10-26T11:43:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T11:43:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:43:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:43:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:43:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:44:50.453Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:44:50.453Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:44:50.456Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:44:50.456Z] [LOG] [API] Checking server status...
[2025-10-26T11:44:50.466Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:44:50.457Z"
}
[2025-10-26T11:44:50.466Z] [LOG] [API] WebGPU available
[2025-10-26T11:44:50.473Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:44:50.473Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:44:50.473Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:44:50.480Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:45:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:45:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:45:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:45:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T11:45:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:45:50.370Z"
}
[2025-10-26T11:45:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T11:45:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:45:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:45:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:45:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:46:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:46:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:46:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:46:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T11:46:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:46:50.401Z"
}
[2025-10-26T11:46:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T11:46:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:46:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:46:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:46:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:47:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:47:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:47:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:47:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T11:47:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:47:50.369Z"
}
[2025-10-26T11:47:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T11:47:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:47:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:47:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:47:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:48:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:48:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:48:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:48:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T11:48:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:48:50.391Z"
}
[2025-10-26T11:48:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T11:48:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:48:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:48:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:48:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:49:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:49:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:49:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:49:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T11:49:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:49:50.372Z"
}
[2025-10-26T11:49:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T11:49:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:49:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:49:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:49:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:50:50.478Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:50:50.478Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:50:50.481Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:50:50.481Z] [LOG] [API] Checking server status...
[2025-10-26T11:50:50.492Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:50:50.482Z"
}
[2025-10-26T11:50:50.492Z] [LOG] [API] WebGPU available
[2025-10-26T11:50:50.499Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:50:50.499Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:50:50.499Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:50:50.505Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:51:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:51:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:51:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:51:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T11:51:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:51:50.372Z"
}
[2025-10-26T11:51:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T11:51:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:51:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:51:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:51:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:52:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:52:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:52:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:52:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T11:52:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:52:50.384Z"
}
[2025-10-26T11:52:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T11:52:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:52:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:52:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:52:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:53:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:53:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:53:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:53:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T11:53:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:53:50.365Z"
}
[2025-10-26T11:53:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T11:53:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:53:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:53:50.384Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:53:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:54:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:54:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:54:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:54:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T11:54:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:54:50.406Z"
}
[2025-10-26T11:54:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T11:54:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:54:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:54:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:54:50.450Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:55:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:55:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:55:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:55:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T11:55:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:55:50.378Z"
}
[2025-10-26T11:55:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T11:55:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:55:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:55:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:55:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:56:50.494Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:56:50.494Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:56:50.497Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:56:50.497Z] [LOG] [API] Checking server status...
[2025-10-26T11:56:50.507Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:56:50.498Z"
}
[2025-10-26T11:56:50.507Z] [LOG] [API] WebGPU available
[2025-10-26T11:56:50.515Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:56:50.515Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:56:50.515Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:56:50.520Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:57:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:57:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:57:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:57:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T11:57:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:57:50.361Z"
}
[2025-10-26T11:57:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T11:57:50.378Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:57:50.378Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:57:50.378Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:57:50.388Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:58:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:58:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:58:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:58:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T11:58:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:58:50.382Z"
}
[2025-10-26T11:58:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T11:58:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:58:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:58:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:58:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T11:59:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T11:59:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T11:59:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T11:59:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T11:59:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T11:59:50.376Z"
}
[2025-10-26T11:59:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T11:59:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T11:59:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T11:59:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T11:59:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:00:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:00:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:00:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:00:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T12:00:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:00:50.384Z"
}
[2025-10-26T12:00:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T12:00:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:00:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:00:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:00:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:01:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:01:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:01:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:01:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T12:01:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:01:50.372Z"
}
[2025-10-26T12:01:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T12:01:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:01:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:01:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:01:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:02:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:02:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:02:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:02:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T12:02:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:02:50.397Z"
}
[2025-10-26T12:02:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T12:02:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:02:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:02:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:02:50.453Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:03:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:03:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:03:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:03:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T12:03:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:03:50.373Z"
}
[2025-10-26T12:03:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T12:03:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:03:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:03:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:03:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:04:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:04:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:04:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:04:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T12:04:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:04:50.405Z"
}
[2025-10-26T12:04:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T12:04:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:04:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:04:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:04:50.452Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:05:50.352Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:05:50.352Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:05:50.356Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:05:50.356Z] [LOG] [API] Checking server status...
[2025-10-26T12:05:50.370Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:05:50.358Z"
}
[2025-10-26T12:05:50.370Z] [LOG] [API] WebGPU available
[2025-10-26T12:05:50.380Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:05:50.380Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:05:50.380Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:05:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:06:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:06:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:06:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:06:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T12:06:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:06:50.380Z"
}
[2025-10-26T12:06:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T12:06:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:06:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:06:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:06:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:07:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:07:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:07:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:07:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T12:07:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:07:50.377Z"
}
[2025-10-26T12:07:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T12:07:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:07:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:07:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:07:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:08:50.456Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:08:50.456Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:08:50.458Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:08:50.458Z] [LOG] [API] Checking server status...
[2025-10-26T12:08:50.469Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:08:50.460Z"
}
[2025-10-26T12:08:50.469Z] [LOG] [API] WebGPU available
[2025-10-26T12:08:50.477Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:08:50.477Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:08:50.477Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:08:50.484Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:09:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:09:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:09:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:09:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T12:09:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:09:50.362Z"
}
[2025-10-26T12:09:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T12:09:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:09:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:09:50.384Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:09:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:10:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:10:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:10:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:10:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T12:10:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:10:50.380Z"
}
[2025-10-26T12:10:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T12:10:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:10:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:10:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:10:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:11:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:11:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:11:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:11:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T12:11:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:11:50.368Z"
}
[2025-10-26T12:11:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T12:11:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:11:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:11:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:11:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:12:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:12:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:12:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:12:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T12:12:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:12:50.392Z"
}
[2025-10-26T12:12:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T12:12:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:12:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:12:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:12:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:13:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:13:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:13:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:13:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T12:13:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:13:50.363Z"
}
[2025-10-26T12:13:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T12:13:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:13:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:13:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:13:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:14:50.515Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:14:50.515Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:14:50.518Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:14:50.518Z] [LOG] [API] Checking server status...
[2025-10-26T12:14:50.528Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:14:50.519Z"
}
[2025-10-26T12:14:50.528Z] [LOG] [API] WebGPU available
[2025-10-26T12:14:50.536Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:14:50.536Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:14:50.536Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:14:50.543Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:15:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:15:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:15:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:15:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T12:15:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:15:50.370Z"
}
[2025-10-26T12:15:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T12:15:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:15:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:15:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:15:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:16:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:16:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:16:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:16:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T12:16:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:16:50.398Z"
}
[2025-10-26T12:16:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T12:16:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:16:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:16:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:16:50.452Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:17:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:17:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:17:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:17:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T12:17:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:17:50.363Z"
}
[2025-10-26T12:17:50.375Z] [LOG] [API] WebGPU available
[2025-10-26T12:17:50.380Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:17:50.380Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:17:50.380Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:17:50.392Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:18:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:18:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:18:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:18:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T12:18:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:18:50.384Z"
}
[2025-10-26T12:18:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T12:18:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:18:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:18:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:18:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:19:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:19:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:19:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:19:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T12:19:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:19:50.372Z"
}
[2025-10-26T12:19:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T12:19:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:19:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:19:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:19:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:20:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:20:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:20:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:20:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T12:20:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:20:50.372Z"
}
[2025-10-26T12:20:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T12:20:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:20:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:20:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:20:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:21:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:21:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:21:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:21:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T12:21:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:21:50.378Z"
}
[2025-10-26T12:21:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T12:21:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:21:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:21:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:21:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:22:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:22:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:22:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:22:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T12:22:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:22:50.378Z"
}
[2025-10-26T12:22:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T12:22:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:22:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:22:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:22:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:23:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:23:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:23:50.363Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:23:50.363Z] [LOG] [API] Checking server status...
[2025-10-26T12:23:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:23:50.366Z"
}
[2025-10-26T12:23:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T12:23:50.380Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:23:50.380Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:23:50.380Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:23:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:24:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:24:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:24:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:24:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T12:24:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:24:50.401Z"
}
[2025-10-26T12:24:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T12:24:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:24:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:24:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:24:50.451Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:25:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:25:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:25:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:25:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T12:25:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:25:50.380Z"
}
[2025-10-26T12:25:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T12:25:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:25:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:25:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:25:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:26:50.498Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:26:50.498Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:26:50.501Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:26:50.501Z] [LOG] [API] Checking server status...
[2025-10-26T12:26:50.511Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:26:50.502Z"
}
[2025-10-26T12:26:50.511Z] [LOG] [API] WebGPU available
[2025-10-26T12:26:50.514Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:26:50.514Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:26:50.514Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:26:50.524Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:27:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:27:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:27:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:27:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T12:27:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:27:50.379Z"
}
[2025-10-26T12:27:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T12:27:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:27:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:27:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:27:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:28:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:28:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:28:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:28:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T12:28:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:28:50.397Z"
}
[2025-10-26T12:28:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T12:28:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:28:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:28:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:28:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:29:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:29:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:29:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:29:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T12:29:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:29:50.364Z"
}
[2025-10-26T12:29:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T12:29:50.381Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:29:50.381Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:29:50.382Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:29:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:30:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:30:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:30:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:30:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T12:30:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:30:50.382Z"
}
[2025-10-26T12:30:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T12:30:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:30:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:30:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:30:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:31:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:31:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:31:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:31:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T12:31:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:31:50.368Z"
}
[2025-10-26T12:31:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T12:31:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:31:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:31:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:31:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:32:50.472Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:32:50.472Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:32:50.475Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:32:50.475Z] [LOG] [API] Checking server status...
[2025-10-26T12:32:50.486Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:32:50.476Z"
}
[2025-10-26T12:32:50.486Z] [LOG] [API] WebGPU available
[2025-10-26T12:32:50.493Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:32:50.493Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:32:50.493Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:32:50.502Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:33:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:33:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:33:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:33:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T12:33:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:33:50.366Z"
}
[2025-10-26T12:33:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T12:33:50.380Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:33:50.380Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:33:50.381Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:33:50.395Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:34:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:34:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:34:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:34:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T12:34:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:34:50.380Z"
}
[2025-10-26T12:34:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T12:34:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:34:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:34:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:34:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:35:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:35:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:35:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:35:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T12:35:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:35:50.377Z"
}
[2025-10-26T12:35:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T12:35:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:35:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:35:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:35:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:36:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:36:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:36:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:36:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T12:36:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:36:50.389Z"
}
[2025-10-26T12:36:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T12:36:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:36:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:36:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:36:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:37:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:37:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:37:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:37:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T12:37:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:37:50.363Z"
}
[2025-10-26T12:37:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T12:37:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:37:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:37:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:37:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:38:50.510Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:38:50.510Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:38:50.513Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:38:50.513Z] [LOG] [API] Checking server status...
[2025-10-26T12:38:50.523Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:38:50.514Z"
}
[2025-10-26T12:38:50.524Z] [LOG] [API] WebGPU available
[2025-10-26T12:38:50.531Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:38:50.531Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:38:50.531Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:38:50.537Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:39:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:39:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:39:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:39:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T12:39:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:39:50.367Z"
}
[2025-10-26T12:39:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T12:39:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:39:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:39:50.384Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:39:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:40:50.352Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:40:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:40:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:40:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T12:40:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:40:50.372Z"
}
[2025-10-26T12:40:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T12:40:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:40:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:40:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:40:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:41:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:41:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:41:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:41:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T12:41:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:41:50.368Z"
}
[2025-10-26T12:41:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T12:41:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:41:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:41:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:41:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:42:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:42:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:42:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:42:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T12:42:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:42:50.387Z"
}
[2025-10-26T12:42:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T12:42:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:42:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:42:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:42:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:43:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:43:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:43:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:43:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T12:43:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:43:50.362Z"
}
[2025-10-26T12:43:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T12:43:50.378Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:43:50.378Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:43:50.378Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:43:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:44:50.511Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:44:50.511Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:44:50.514Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:44:50.514Z] [LOG] [API] Checking server status...
[2025-10-26T12:44:50.525Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:44:50.515Z"
}
[2025-10-26T12:44:50.525Z] [LOG] [API] WebGPU available
[2025-10-26T12:44:50.532Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:44:50.532Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:44:50.532Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:44:50.538Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:45:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:45:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:45:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:45:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T12:45:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:45:50.374Z"
}
[2025-10-26T12:45:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T12:45:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:45:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:45:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:45:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:46:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:46:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:46:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:46:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T12:46:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:46:50.377Z"
}
[2025-10-26T12:46:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T12:46:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:46:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:46:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:46:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:47:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:47:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:47:50.363Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:47:50.363Z] [LOG] [API] Checking server status...
[2025-10-26T12:47:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:47:50.366Z"
}
[2025-10-26T12:47:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T12:47:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:47:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:47:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:47:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:48:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:48:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:48:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:48:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T12:48:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:48:50.380Z"
}
[2025-10-26T12:48:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T12:48:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:48:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:48:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:48:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:49:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:49:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:49:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:49:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T12:49:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:49:50.382Z"
}
[2025-10-26T12:49:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T12:49:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:49:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:49:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:49:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:50:50.543Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:50:50.543Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:50:50.546Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:50:50.546Z] [LOG] [API] Checking server status...
[2025-10-26T12:50:50.556Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:50:50.547Z"
}
[2025-10-26T12:50:50.556Z] [LOG] [API] WebGPU available
[2025-10-26T12:50:50.564Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:50:50.564Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:50:50.564Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:50:50.572Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:51:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:51:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:51:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:51:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T12:51:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:51:50.393Z"
}
[2025-10-26T12:51:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T12:51:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:51:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:51:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:51:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:52:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:52:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:52:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:52:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T12:52:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:52:50.383Z"
}
[2025-10-26T12:52:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T12:52:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:52:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:52:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:52:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:53:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:53:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:53:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:53:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T12:53:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:53:50.368Z"
}
[2025-10-26T12:53:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T12:53:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:53:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:53:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:53:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:54:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:54:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:54:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:54:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T12:54:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:54:50.388Z"
}
[2025-10-26T12:54:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T12:54:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:54:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:54:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:54:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:55:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:55:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:55:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:55:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T12:55:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:55:50.369Z"
}
[2025-10-26T12:55:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T12:55:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:55:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:55:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:55:50.401Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:56:50.563Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:56:50.563Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:56:50.566Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:56:50.566Z] [LOG] [API] Checking server status...
[2025-10-26T12:56:50.576Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:56:50.567Z"
}
[2025-10-26T12:56:50.576Z] [LOG] [API] WebGPU available
[2025-10-26T12:56:50.584Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:56:50.584Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:56:50.584Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:56:50.589Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:57:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:57:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:57:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:57:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T12:57:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:57:50.370Z"
}
[2025-10-26T12:57:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T12:57:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:57:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:57:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:57:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:58:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:58:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:58:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:58:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T12:58:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:58:50.377Z"
}
[2025-10-26T12:58:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T12:58:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:58:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:58:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:58:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T12:59:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T12:59:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T12:59:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T12:59:50.372Z] [LOG] [API] Checking server status...
[2025-10-26T12:59:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T12:59:50.373Z"
}
[2025-10-26T12:59:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T12:59:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T12:59:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T12:59:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T12:59:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:00:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:00:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:00:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:00:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T13:00:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:00:50.370Z"
}
[2025-10-26T13:00:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T13:00:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:00:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:00:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:00:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:01:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:01:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:01:50.436Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:01:50.436Z] [LOG] [API] Checking server status...
[2025-10-26T13:01:50.446Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:01:50.437Z"
}
[2025-10-26T13:01:50.446Z] [LOG] [API] WebGPU available
[2025-10-26T13:01:50.454Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:01:50.454Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:01:50.454Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:01:50.460Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:02:50.516Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:02:50.516Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:02:50.519Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:02:50.519Z] [LOG] [API] Checking server status...
[2025-10-26T13:02:50.530Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:02:50.520Z"
}
[2025-10-26T13:02:50.530Z] [LOG] [API] WebGPU available
[2025-10-26T13:02:50.537Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:02:50.537Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:02:50.537Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:02:50.546Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:03:50.349Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:03:50.349Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:03:50.353Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:03:50.353Z] [LOG] [API] Checking server status...
[2025-10-26T13:03:50.366Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:03:50.354Z"
}
[2025-10-26T13:03:50.366Z] [LOG] [API] WebGPU available
[2025-10-26T13:03:50.370Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:03:50.370Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:03:50.370Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:03:50.382Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:04:50.354Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:04:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:04:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:04:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T13:04:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:04:50.384Z"
}
[2025-10-26T13:04:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T13:04:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:04:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:04:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:04:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:05:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:05:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:05:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:05:50.361Z] [LOG] [API] Checking server status...
[2025-10-26T13:05:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:05:50.362Z"
}
[2025-10-26T13:05:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T13:05:50.381Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:05:50.382Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:05:50.382Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:05:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:06:50.413Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:06:50.413Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:06:50.451Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:06:50.451Z] [LOG] [API] Checking server status...
[2025-10-26T13:06:50.463Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:06:50.452Z"
}
[2025-10-26T13:06:50.463Z] [LOG] [API] WebGPU available
[2025-10-26T13:06:50.471Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:06:50.472Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:06:50.472Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:06:50.478Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:07:50.352Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:07:50.352Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:07:50.357Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:07:50.357Z] [LOG] [API] Checking server status...
[2025-10-26T13:07:50.369Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:07:50.358Z"
}
[2025-10-26T13:07:50.369Z] [LOG] [API] WebGPU available
[2025-10-26T13:07:50.374Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:07:50.374Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:07:50.374Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:07:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:08:50.350Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:08:50.350Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:08:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:08:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T13:08:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:08:50.382Z"
}
[2025-10-26T13:08:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T13:08:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:08:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:08:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:08:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:09:50.352Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:09:50.352Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:09:50.356Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:09:50.356Z] [LOG] [API] Checking server status...
[2025-10-26T13:09:50.370Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:09:50.360Z"
}
[2025-10-26T13:09:50.370Z] [LOG] [API] WebGPU available
[2025-10-26T13:09:50.374Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:09:50.374Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:09:50.374Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:09:50.401Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:10:50.348Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:10:50.348Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:10:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:10:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T13:10:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:10:50.373Z"
}
[2025-10-26T13:10:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T13:10:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:10:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:10:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:10:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:11:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:11:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:11:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:11:50.361Z] [LOG] [API] Checking server status...
[2025-10-26T13:11:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:11:50.364Z"
}
[2025-10-26T13:11:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T13:11:50.379Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:11:50.379Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:11:50.379Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:11:50.394Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:12:50.351Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:12:50.351Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:12:50.428Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:12:50.428Z] [LOG] [API] Checking server status...
[2025-10-26T13:12:50.439Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:12:50.429Z"
}
[2025-10-26T13:12:50.439Z] [LOG] [API] WebGPU available
[2025-10-26T13:12:50.446Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:12:50.446Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:12:50.446Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:12:50.452Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:13:50.346Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:13:50.346Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:13:50.351Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:13:50.351Z] [LOG] [API] Checking server status...
[2025-10-26T13:13:50.365Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:13:50.353Z"
}
[2025-10-26T13:13:50.365Z] [LOG] [API] WebGPU available
[2025-10-26T13:13:50.374Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:13:50.374Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:13:50.374Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:13:50.382Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:14:50.446Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:14:50.446Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:14:50.449Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:14:50.449Z] [LOG] [API] Checking server status...
[2025-10-26T13:14:50.460Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:14:50.451Z"
}
[2025-10-26T13:14:50.460Z] [LOG] [API] WebGPU available
[2025-10-26T13:14:50.466Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:14:50.466Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:14:50.466Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:14:50.475Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:15:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:15:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:15:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:15:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T13:15:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:15:50.388Z"
}
[2025-10-26T13:15:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T13:15:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:15:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:15:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:15:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:16:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:16:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:16:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:16:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T13:16:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:16:50.391Z"
}
[2025-10-26T13:16:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T13:16:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:16:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:16:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:16:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:17:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:17:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:17:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:17:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T13:17:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:17:50.379Z"
}
[2025-10-26T13:17:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T13:17:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:17:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:17:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:17:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:18:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:18:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:18:50.416Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:18:50.416Z] [LOG] [API] Checking server status...
[2025-10-26T13:18:50.427Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:18:50.417Z"
}
[2025-10-26T13:18:50.428Z] [LOG] [API] WebGPU available
[2025-10-26T13:18:50.446Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:18:50.446Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:18:50.446Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:18:50.469Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:19:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:19:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:19:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:19:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T13:19:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:19:50.389Z"
}
[2025-10-26T13:19:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T13:19:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:19:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:19:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:19:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:20:50.439Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:20:50.439Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:20:50.442Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:20:50.442Z] [LOG] [API] Checking server status...
[2025-10-26T13:20:50.454Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:20:50.444Z"
}
[2025-10-26T13:20:50.454Z] [LOG] [API] WebGPU available
[2025-10-26T13:20:50.463Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:20:50.463Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:20:50.463Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:20:50.472Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:21:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:21:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:21:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:21:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T13:21:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:21:50.383Z"
}
[2025-10-26T13:21:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T13:21:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:21:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:21:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:21:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:22:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:22:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:22:50.452Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:22:50.452Z] [LOG] [API] Checking server status...
[2025-10-26T13:22:50.479Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:22:50.453Z"
}
[2025-10-26T13:22:50.479Z] [LOG] [API] WebGPU available
[2025-10-26T13:22:50.487Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:22:50.488Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:22:50.488Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:22:50.505Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:23:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:23:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:23:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:23:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T13:23:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:23:50.377Z"
}
[2025-10-26T13:23:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T13:23:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:23:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:23:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:23:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:24:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:24:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:24:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:24:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T13:24:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:24:50.398Z"
}
[2025-10-26T13:24:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T13:24:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:24:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:24:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:24:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:25:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:25:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:25:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:25:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T13:25:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:25:50.381Z"
}
[2025-10-26T13:25:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T13:25:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:25:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:25:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:25:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:26:50.531Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:26:50.531Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:26:50.534Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:26:50.534Z] [LOG] [API] Checking server status...
[2025-10-26T13:26:50.545Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:26:50.535Z"
}
[2025-10-26T13:26:50.545Z] [LOG] [API] WebGPU available
[2025-10-26T13:26:50.552Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:26:50.552Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:26:50.552Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:26:50.558Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:27:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:27:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:27:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:27:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T13:27:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:27:50.388Z"
}
[2025-10-26T13:27:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T13:27:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:27:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:27:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:27:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:28:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:28:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:28:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:28:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T13:28:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:28:50.403Z"
}
[2025-10-26T13:28:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T13:28:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:28:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:28:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:28:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:29:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:29:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:29:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:29:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T13:29:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:29:50.377Z"
}
[2025-10-26T13:29:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T13:29:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:29:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:29:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:29:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:30:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:30:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:30:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:30:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T13:30:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:30:50.401Z"
}
[2025-10-26T13:30:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T13:30:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:30:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:30:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:30:50.452Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:31:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:31:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:31:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:31:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T13:31:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:31:50.369Z"
}
[2025-10-26T13:31:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T13:31:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:31:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:31:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:31:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:32:50.474Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:32:50.474Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:32:50.477Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:32:50.477Z] [LOG] [API] Checking server status...
[2025-10-26T13:32:50.487Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:32:50.478Z"
}
[2025-10-26T13:32:50.487Z] [LOG] [API] WebGPU available
[2025-10-26T13:32:50.495Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:32:50.495Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:32:50.495Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:32:50.509Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:33:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:33:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:33:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:33:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T13:33:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:33:50.390Z"
}
[2025-10-26T13:33:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T13:33:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:33:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:33:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:33:50.456Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:34:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:34:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:34:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:34:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T13:34:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:34:50.381Z"
}
[2025-10-26T13:34:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T13:34:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:34:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:34:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:34:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:35:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:35:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:35:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:35:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T13:35:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:35:50.381Z"
}
[2025-10-26T13:35:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T13:35:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:35:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:35:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:35:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:36:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:36:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:36:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:36:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T13:36:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:36:50.382Z"
}
[2025-10-26T13:36:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T13:36:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:36:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:36:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:36:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:37:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:37:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:37:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:37:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T13:37:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:37:50.384Z"
}
[2025-10-26T13:37:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T13:37:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:37:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:37:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:37:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:38:50.586Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:38:50.586Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:38:50.588Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:38:50.588Z] [LOG] [API] Checking server status...
[2025-10-26T13:38:50.597Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:38:50.589Z"
}
[2025-10-26T13:38:50.597Z] [LOG] [API] WebGPU available
[2025-10-26T13:38:50.603Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:38:50.603Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:38:50.604Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:38:50.618Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:39:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:39:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:39:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:39:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T13:39:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:39:50.384Z"
}
[2025-10-26T13:39:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T13:39:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:39:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:39:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:39:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:40:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:40:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:40:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:40:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T13:40:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:40:50.403Z"
}
[2025-10-26T13:40:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T13:40:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:40:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:40:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:40:50.455Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:41:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:41:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:41:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:41:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T13:41:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:41:50.369Z"
}
[2025-10-26T13:41:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T13:41:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:41:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:41:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:41:50.395Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:42:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:42:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:42:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:42:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T13:42:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:42:50.397Z"
}
[2025-10-26T13:42:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T13:42:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:42:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:42:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:42:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:43:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:43:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:43:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:43:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T13:43:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:43:50.375Z"
}
[2025-10-26T13:43:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T13:43:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:43:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:43:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:43:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:44:50.650Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:44:50.651Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:44:50.653Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:44:50.653Z] [LOG] [API] Checking server status...
[2025-10-26T13:44:50.662Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:44:50.654Z"
}
[2025-10-26T13:44:50.663Z] [LOG] [API] WebGPU available
[2025-10-26T13:44:50.671Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:44:50.671Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:44:50.671Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:44:50.683Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:45:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:45:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:45:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:45:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T13:45:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:45:50.384Z"
}
[2025-10-26T13:45:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T13:45:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:45:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:45:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:45:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:46:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:46:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:46:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:46:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T13:46:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:46:50.372Z"
}
[2025-10-26T13:46:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T13:46:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:46:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:46:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:46:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:47:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:47:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:47:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:47:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T13:47:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:47:50.370Z"
}
[2025-10-26T13:47:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T13:47:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:47:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:47:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:47:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:48:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:48:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:48:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:48:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T13:48:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:48:50.376Z"
}
[2025-10-26T13:48:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T13:48:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:48:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:48:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:48:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:49:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:49:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:49:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:49:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T13:49:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:49:50.383Z"
}
[2025-10-26T13:49:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T13:49:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:49:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:49:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:49:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:50:50.593Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:50:50.593Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:50:50.595Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:50:50.595Z] [LOG] [API] Checking server status...
[2025-10-26T13:50:50.604Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:50:50.596Z"
}
[2025-10-26T13:50:50.604Z] [LOG] [API] WebGPU available
[2025-10-26T13:50:50.611Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:50:50.611Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:50:50.612Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:50:50.625Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:51:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:51:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:51:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:51:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T13:51:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:51:50.376Z"
}
[2025-10-26T13:51:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T13:51:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:51:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:51:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:51:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:52:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:52:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:52:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:52:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T13:52:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:52:50.394Z"
}
[2025-10-26T13:52:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T13:52:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:52:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:52:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:52:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:53:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:53:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:53:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:53:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T13:53:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:53:50.371Z"
}
[2025-10-26T13:53:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T13:53:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:53:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:53:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:53:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:54:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:54:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:54:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:54:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T13:54:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:54:50.390Z"
}
[2025-10-26T13:54:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T13:54:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:54:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:54:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:54:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:55:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:55:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:55:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:55:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T13:55:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:55:50.381Z"
}
[2025-10-26T13:55:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T13:55:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:55:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:55:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:55:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:56:50.524Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:56:50.524Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:56:50.527Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:56:50.527Z] [LOG] [API] Checking server status...
[2025-10-26T13:56:50.537Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:56:50.528Z"
}
[2025-10-26T13:56:50.537Z] [LOG] [API] WebGPU available
[2025-10-26T13:56:50.545Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:56:50.545Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:56:50.545Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:56:50.561Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:57:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:57:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:57:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:57:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T13:57:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:57:50.391Z"
}
[2025-10-26T13:57:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T13:57:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:57:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:57:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:57:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:58:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:58:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:58:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:58:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T13:58:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:58:50.381Z"
}
[2025-10-26T13:58:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T13:58:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:58:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:58:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:58:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T13:59:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T13:59:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T13:59:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T13:59:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T13:59:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T13:59:50.385Z"
}
[2025-10-26T13:59:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T13:59:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T13:59:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T13:59:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T13:59:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:00:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:00:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:00:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:00:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T14:00:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:00:50.372Z"
}
[2025-10-26T14:00:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T14:00:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:00:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:00:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:00:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:01:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:01:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:01:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:01:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T14:01:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:01:50.383Z"
}
[2025-10-26T14:01:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T14:01:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:01:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:01:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:01:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:02:50.551Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:02:50.551Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:02:50.554Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:02:50.554Z] [LOG] [API] Checking server status...
[2025-10-26T14:02:50.564Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:02:50.555Z"
}
[2025-10-26T14:02:50.564Z] [LOG] [API] WebGPU available
[2025-10-26T14:02:50.567Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:02:50.567Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:02:50.567Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:02:50.587Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:03:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:03:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:03:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:03:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T14:03:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:03:50.378Z"
}
[2025-10-26T14:03:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T14:03:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:03:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:03:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:03:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:04:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:04:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:04:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:04:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T14:04:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:04:50.395Z"
}
[2025-10-26T14:04:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T14:04:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:04:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:04:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:04:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:05:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:05:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:05:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:05:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T14:05:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:05:50.367Z"
}
[2025-10-26T14:05:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T14:05:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:05:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:05:50.384Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:05:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:06:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:06:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:06:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:06:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T14:06:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:06:50.413Z"
}
[2025-10-26T14:06:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T14:06:50.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:06:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:06:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:06:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:07:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:07:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:07:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:07:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T14:07:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:07:50.376Z"
}
[2025-10-26T14:07:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T14:07:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:07:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:07:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:07:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:08:50.498Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:08:50.498Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:08:50.501Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:08:50.501Z] [LOG] [API] Checking server status...
[2025-10-26T14:08:50.512Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:08:50.502Z"
}
[2025-10-26T14:08:50.512Z] [LOG] [API] WebGPU available
[2025-10-26T14:08:50.520Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:08:50.520Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:08:50.520Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:08:50.535Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:09:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:09:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:09:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:09:50.372Z] [LOG] [API] Checking server status...
[2025-10-26T14:09:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:09:50.374Z"
}
[2025-10-26T14:09:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T14:09:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:09:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:09:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:09:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:10:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:10:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:10:50.431Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:10:50.431Z] [LOG] [API] Checking server status...
[2025-10-26T14:10:50.445Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:10:50.433Z"
}
[2025-10-26T14:10:50.445Z] [LOG] [API] WebGPU available
[2025-10-26T14:10:50.462Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:10:50.462Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:10:50.462Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:10:50.497Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:11:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:11:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:11:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:11:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T14:11:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:11:50.384Z"
}
[2025-10-26T14:11:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T14:11:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:11:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:11:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:11:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:12:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:12:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:12:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:12:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T14:12:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:12:50.399Z"
}
[2025-10-26T14:12:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T14:12:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:12:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:12:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:12:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:13:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:13:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:13:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:13:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T14:13:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:13:50.386Z"
}
[2025-10-26T14:13:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T14:13:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:13:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:13:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:13:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:14:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:14:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:14:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:14:50.415Z] [LOG] [API] Checking server status...
[2025-10-26T14:14:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:14:50.416Z"
}
[2025-10-26T14:14:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T14:14:50.434Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:14:50.434Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:14:50.434Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:14:50.478Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:15:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:15:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:15:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:15:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T14:15:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:15:50.377Z"
}
[2025-10-26T14:15:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T14:15:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:15:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:15:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:15:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:16:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:16:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:16:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:16:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T14:16:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:16:50.393Z"
}
[2025-10-26T14:16:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T14:16:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:16:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:16:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:16:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:17:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:17:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:17:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:17:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T14:17:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:17:50.362Z"
}
[2025-10-26T14:17:50.373Z] [LOG] [API] WebGPU available
[2025-10-26T14:17:50.378Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:17:50.378Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:17:50.378Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:17:50.398Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:18:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:18:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:18:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:18:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T14:18:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:18:50.393Z"
}
[2025-10-26T14:18:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T14:18:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:18:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:18:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:18:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:19:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:19:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:19:50.363Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:19:50.363Z] [LOG] [API] Checking server status...
[2025-10-26T14:19:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:19:50.365Z"
}
[2025-10-26T14:19:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T14:19:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:19:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:19:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:19:50.394Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:20:50.530Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:20:50.530Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:20:50.533Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:20:50.533Z] [LOG] [API] Checking server status...
[2025-10-26T14:20:50.544Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:20:50.534Z"
}
[2025-10-26T14:20:50.544Z] [LOG] [API] WebGPU available
[2025-10-26T14:20:50.547Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:20:50.548Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:20:50.548Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:20:50.567Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:21:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:21:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:21:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:21:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T14:21:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:21:50.383Z"
}
[2025-10-26T14:21:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T14:21:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:21:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:21:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:21:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:22:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:22:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:22:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:22:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T14:22:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:22:50.408Z"
}
[2025-10-26T14:22:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T14:22:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:22:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:22:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:22:50.455Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:23:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:23:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:23:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:23:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T14:23:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:23:50.382Z"
}
[2025-10-26T14:23:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T14:23:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:23:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:23:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:23:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:24:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:24:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:24:50.422Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:24:50.422Z] [LOG] [API] Checking server status...
[2025-10-26T14:24:50.449Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:24:50.423Z"
}
[2025-10-26T14:24:50.449Z] [LOG] [API] WebGPU available
[2025-10-26T14:24:50.457Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:24:50.457Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:24:50.457Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:24:50.473Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:25:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:25:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:25:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:25:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T14:25:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:25:50.378Z"
}
[2025-10-26T14:25:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T14:25:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:25:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:25:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:25:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:26:50.472Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:26:50.472Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:26:50.475Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:26:50.475Z] [LOG] [API] Checking server status...
[2025-10-26T14:26:50.486Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:26:50.476Z"
}
[2025-10-26T14:26:50.487Z] [LOG] [API] WebGPU available
[2025-10-26T14:26:50.494Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:26:50.494Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:26:50.494Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:26:50.509Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:27:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:27:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:27:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:27:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T14:27:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:27:50.386Z"
}
[2025-10-26T14:27:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T14:27:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:27:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:27:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:27:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:28:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:28:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:28:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:28:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T14:28:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:28:50.387Z"
}
[2025-10-26T14:28:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T14:28:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:28:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:28:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:28:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:29:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:29:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:29:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:29:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T14:29:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:29:50.371Z"
}
[2025-10-26T14:29:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T14:29:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:29:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:29:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:29:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:30:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:30:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:30:50.428Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:30:50.428Z] [LOG] [API] Checking server status...
[2025-10-26T14:30:50.460Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:30:50.429Z"
}
[2025-10-26T14:30:50.460Z] [LOG] [API] WebGPU available
[2025-10-26T14:30:50.467Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:30:50.467Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:30:50.467Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:30:50.482Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:31:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:31:50.384Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:31:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:31:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T14:31:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:31:50.390Z"
}
[2025-10-26T14:31:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T14:31:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:31:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:31:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:31:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:32:50.501Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:32:50.501Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:32:50.504Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:32:50.504Z] [LOG] [API] Checking server status...
[2025-10-26T14:32:50.515Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:32:50.506Z"
}
[2025-10-26T14:32:50.515Z] [LOG] [API] WebGPU available
[2025-10-26T14:32:50.522Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:32:50.523Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:32:50.523Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:32:50.538Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:33:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:33:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:33:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:33:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T14:33:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:33:50.381Z"
}
[2025-10-26T14:33:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T14:33:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:33:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:33:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:33:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:34:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:34:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:34:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:34:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T14:34:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:34:50.405Z"
}
[2025-10-26T14:34:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T14:34:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:34:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:34:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:34:50.437Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:35:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:35:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:35:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:35:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T14:35:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:35:50.380Z"
}
[2025-10-26T14:35:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T14:35:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:35:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:35:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:35:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:36:50.387Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:36:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:36:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:36:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T14:36:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:36:50.402Z"
}
[2025-10-26T14:36:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T14:36:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:36:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:36:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:36:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:37:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:37:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:37:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:37:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T14:37:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:37:50.383Z"
}
[2025-10-26T14:37:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T14:37:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:37:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:37:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:37:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:38:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:38:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:38:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:38:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T14:38:50.425Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:38:50.415Z"
}
[2025-10-26T14:38:50.425Z] [LOG] [API] WebGPU available
[2025-10-26T14:38:50.428Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:38:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:38:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:38:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:39:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:39:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:39:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:39:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T14:39:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:39:50.374Z"
}
[2025-10-26T14:39:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T14:39:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:39:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:39:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:39:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:40:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:40:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:40:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:40:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T14:40:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:40:50.409Z"
}
[2025-10-26T14:40:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T14:40:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:40:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:40:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:40:50.455Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:41:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:41:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:41:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:41:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T14:41:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:41:50.375Z"
}
[2025-10-26T14:41:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T14:41:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:41:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:41:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:41:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:42:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:42:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:42:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:42:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T14:42:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:42:50.379Z"
}
[2025-10-26T14:42:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T14:42:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:42:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:42:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:42:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:43:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:43:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:43:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:43:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T14:43:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:43:50.376Z"
}
[2025-10-26T14:43:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T14:43:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:43:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:43:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:43:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:44:50.585Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:44:50.585Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:44:50.588Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:44:50.588Z] [LOG] [API] Checking server status...
[2025-10-26T14:44:50.599Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:44:50.589Z"
}
[2025-10-26T14:44:50.599Z] [LOG] [API] WebGPU available
[2025-10-26T14:44:50.607Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:44:50.607Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:44:50.607Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:44:50.621Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:45:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:45:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:45:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:45:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T14:45:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:45:50.384Z"
}
[2025-10-26T14:45:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T14:45:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:45:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:45:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:45:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:46:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:46:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:46:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:46:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T14:46:50.424Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:46:50.413Z"
}
[2025-10-26T14:46:50.424Z] [LOG] [API] WebGPU available
[2025-10-26T14:46:50.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:46:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:46:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:46:50.464Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:47:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:47:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:47:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:47:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T14:47:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:47:50.380Z"
}
[2025-10-26T14:47:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T14:47:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:47:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:47:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:47:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:48:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:48:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:48:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:48:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T14:48:50.418Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:48:50.406Z"
}
[2025-10-26T14:48:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T14:48:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:48:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:48:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:48:50.461Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:49:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:49:50.387Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:49:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:49:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T14:49:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:49:50.393Z"
}
[2025-10-26T14:49:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T14:49:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:49:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:49:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:49:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:50:50.522Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:50:50.522Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:50:50.525Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:50:50.525Z] [LOG] [API] Checking server status...
[2025-10-26T14:50:50.534Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:50:50.526Z"
}
[2025-10-26T14:50:50.535Z] [LOG] [API] WebGPU available
[2025-10-26T14:50:50.539Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:50:50.539Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:50:50.539Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:50:50.558Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:51:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:51:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:51:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:51:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T14:51:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:51:50.379Z"
}
[2025-10-26T14:51:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T14:51:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:51:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:51:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:51:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:52:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:52:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:52:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:52:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T14:52:50.425Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:52:50.412Z"
}
[2025-10-26T14:52:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T14:52:50.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:52:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:52:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:52:50.475Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:53:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:53:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:53:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:53:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T14:53:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:53:50.377Z"
}
[2025-10-26T14:53:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T14:53:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:53:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:53:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:53:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:54:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:54:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:54:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:54:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T14:54:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:54:50.390Z"
}
[2025-10-26T14:54:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T14:54:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:54:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:54:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:54:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:55:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:55:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:55:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:55:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T14:55:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:55:50.391Z"
}
[2025-10-26T14:55:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T14:55:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:55:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:55:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:55:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:56:50.454Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:56:50.454Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:56:50.456Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:56:50.456Z] [LOG] [API] Checking server status...
[2025-10-26T14:56:50.467Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:56:50.457Z"
}
[2025-10-26T14:56:50.467Z] [LOG] [API] WebGPU available
[2025-10-26T14:56:50.474Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:56:50.474Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:56:50.474Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:56:50.490Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:57:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:57:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:57:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:57:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T14:57:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:57:50.372Z"
}
[2025-10-26T14:57:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T14:57:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:57:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:57:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:57:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:58:50.383Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:58:50.383Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:58:50.419Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:58:50.419Z] [LOG] [API] Checking server status...
[2025-10-26T14:58:50.430Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:58:50.420Z"
}
[2025-10-26T14:58:50.430Z] [LOG] [API] WebGPU available
[2025-10-26T14:58:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:58:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:58:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:58:50.451Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T14:59:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T14:59:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T14:59:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T14:59:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T14:59:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T14:59:50.378Z"
}
[2025-10-26T14:59:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T14:59:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T14:59:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T14:59:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T14:59:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:00:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:00:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:00:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:00:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T15:00:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:00:50.398Z"
}
[2025-10-26T15:00:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T15:00:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:00:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:00:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:00:50.450Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:01:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:01:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:01:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:01:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T15:01:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:01:50.372Z"
}
[2025-10-26T15:01:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T15:01:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:01:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:01:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:01:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:02:50.509Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:02:50.509Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:02:50.511Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:02:50.511Z] [LOG] [API] Checking server status...
[2025-10-26T15:02:50.527Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:02:50.512Z"
}
[2025-10-26T15:02:50.527Z] [LOG] [API] WebGPU available
[2025-10-26T15:02:50.535Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:02:50.535Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:02:50.535Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:02:50.552Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:03:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:03:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:03:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:03:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T15:03:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:03:50.377Z"
}
[2025-10-26T15:03:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T15:03:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:03:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:03:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:03:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:04:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:04:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:04:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:04:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T15:04:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:04:50.413Z"
}
[2025-10-26T15:04:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T15:04:50.431Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:04:50.431Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:04:50.431Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:04:50.460Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:05:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:05:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:05:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:05:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T15:05:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:05:50.375Z"
}
[2025-10-26T15:05:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T15:05:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:05:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:05:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:05:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:06:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:06:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:06:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:06:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T15:06:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:06:50.388Z"
}
[2025-10-26T15:06:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T15:06:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:06:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:06:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:06:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:07:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:07:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:07:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:07:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T15:07:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:07:50.370Z"
}
[2025-10-26T15:07:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T15:07:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:07:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:07:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:07:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:08:50.418Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:08:50.418Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:08:50.424Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:08:50.424Z] [LOG] [API] Checking server status...
[2025-10-26T15:08:50.435Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:08:50.425Z"
}
[2025-10-26T15:08:50.435Z] [LOG] [API] WebGPU available
[2025-10-26T15:08:50.444Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:08:50.445Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:08:50.445Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:08:50.460Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:09:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:09:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:09:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:09:50.372Z] [LOG] [API] Checking server status...
[2025-10-26T15:09:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:09:50.375Z"
}
[2025-10-26T15:09:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T15:09:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:09:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:09:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:09:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:10:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:10:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:10:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:10:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T15:10:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:10:50.380Z"
}
[2025-10-26T15:10:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T15:10:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:10:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:10:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:10:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:11:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:11:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:11:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:11:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T15:11:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:11:50.388Z"
}
[2025-10-26T15:11:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T15:11:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:11:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:11:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:11:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:12:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:12:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:12:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:12:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T15:12:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:12:50.404Z"
}
[2025-10-26T15:12:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T15:12:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:12:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:12:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:12:50.454Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:13:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:13:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:13:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:13:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T15:13:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:13:50.374Z"
}
[2025-10-26T15:13:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T15:13:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:13:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:13:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:13:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:14:50.623Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:14:50.623Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:14:50.626Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:14:50.626Z] [LOG] [API] Checking server status...
[2025-10-26T15:14:50.637Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:14:50.627Z"
}
[2025-10-26T15:14:50.637Z] [LOG] [API] WebGPU available
[2025-10-26T15:14:50.658Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:14:50.658Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:14:50.658Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:14:50.679Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:15:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:15:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:15:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:15:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T15:15:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:15:50.382Z"
}
[2025-10-26T15:15:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T15:15:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:15:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:15:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:15:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:16:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:16:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:16:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:16:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T15:16:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:16:50.412Z"
}
[2025-10-26T15:16:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T15:16:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:16:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:16:50.434Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:16:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:17:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:17:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:17:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:17:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T15:17:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:17:50.380Z"
}
[2025-10-26T15:17:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T15:17:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:17:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:17:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:17:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:18:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:18:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:18:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:18:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T15:18:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:18:50.397Z"
}
[2025-10-26T15:18:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T15:18:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:18:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:18:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:18:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:19:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:19:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:19:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:19:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T15:19:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:19:50.375Z"
}
[2025-10-26T15:19:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T15:19:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:19:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:19:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:19:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:20:50.492Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:20:50.492Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:20:50.495Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:20:50.495Z] [LOG] [API] Checking server status...
[2025-10-26T15:20:50.506Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:20:50.496Z"
}
[2025-10-26T15:20:50.506Z] [LOG] [API] WebGPU available
[2025-10-26T15:20:50.513Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:20:50.513Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:20:50.513Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:20:50.528Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:21:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:21:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:21:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:21:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T15:21:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:21:50.377Z"
}
[2025-10-26T15:21:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T15:21:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:21:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:21:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:21:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:22:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:22:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:22:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:22:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T15:22:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:22:50.400Z"
}
[2025-10-26T15:22:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T15:22:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:22:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:22:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:22:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:23:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:23:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:23:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:23:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T15:23:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:23:50.373Z"
}
[2025-10-26T15:23:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T15:23:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:23:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:23:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:23:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:24:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:24:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:24:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:24:50.385Z] [LOG] [API] Checking server status...
[2025-10-26T15:24:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:24:50.387Z"
}
[2025-10-26T15:24:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T15:24:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:24:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:24:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:24:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:25:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:25:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:25:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:25:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T15:25:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:25:50.383Z"
}
[2025-10-26T15:25:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T15:25:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:25:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:25:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:25:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:26:50.557Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:26:50.557Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:26:50.560Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:26:50.560Z] [LOG] [API] Checking server status...
[2025-10-26T15:26:50.568Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:26:50.561Z"
}
[2025-10-26T15:26:50.568Z] [LOG] [API] WebGPU available
[2025-10-26T15:26:50.575Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:26:50.575Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:26:50.575Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:26:50.602Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:27:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:27:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:27:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:27:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T15:27:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:27:50.371Z"
}
[2025-10-26T15:27:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T15:27:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:27:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:27:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:27:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:28:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:28:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:28:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:28:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T15:28:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:28:50.397Z"
}
[2025-10-26T15:28:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T15:28:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:28:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:28:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:28:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:29:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:29:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:29:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:29:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T15:29:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:29:50.384Z"
}
[2025-10-26T15:29:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T15:29:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:29:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:29:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:29:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:30:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:30:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:30:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:30:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T15:30:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:30:50.410Z"
}
[2025-10-26T15:30:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T15:30:50.434Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:30:50.435Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:30:50.435Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:30:50.457Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:31:50.386Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:31:50.386Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:31:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:31:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T15:31:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:31:50.396Z"
}
[2025-10-26T15:31:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T15:31:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:31:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:31:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:31:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:32:50.500Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:32:50.500Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:32:50.502Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:32:50.502Z] [LOG] [API] Checking server status...
[2025-10-26T15:32:50.512Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:32:50.504Z"
}
[2025-10-26T15:32:50.512Z] [LOG] [API] WebGPU available
[2025-10-26T15:32:50.514Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:32:50.514Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:32:50.514Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:32:50.545Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:33:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:33:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:33:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:33:50.372Z] [LOG] [API] Checking server status...
[2025-10-26T15:33:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:33:50.375Z"
}
[2025-10-26T15:33:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T15:33:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:33:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:33:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:33:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:34:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:34:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:34:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:34:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T15:34:50.416Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:34:50.404Z"
}
[2025-10-26T15:34:50.416Z] [LOG] [API] WebGPU available
[2025-10-26T15:34:50.433Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:34:50.433Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:34:50.433Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:34:50.461Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:35:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:35:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:35:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:35:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T15:35:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:35:50.392Z"
}
[2025-10-26T15:35:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T15:35:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:35:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:35:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:35:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:36:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:36:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:36:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:36:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T15:36:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:36:50.381Z"
}
[2025-10-26T15:36:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T15:36:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:36:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:36:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:36:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:37:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:37:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:37:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:37:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T15:37:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:37:50.368Z"
}
[2025-10-26T15:37:50.375Z] [LOG] [API] WebGPU available
[2025-10-26T15:37:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:37:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:37:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:37:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:38:50.461Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:38:50.461Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:38:50.464Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:38:50.464Z] [LOG] [API] Checking server status...
[2025-10-26T15:38:50.476Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:38:50.465Z"
}
[2025-10-26T15:38:50.476Z] [LOG] [API] WebGPU available
[2025-10-26T15:38:50.483Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:38:50.483Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:38:50.483Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:38:50.497Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:39:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:39:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:39:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:39:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T15:39:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:39:50.389Z"
}
[2025-10-26T15:39:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T15:39:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:39:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:39:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:39:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:40:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:40:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:40:50.552Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:40:50.552Z] [LOG] [API] Checking server status...
[2025-10-26T15:40:50.563Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:40:50.553Z"
}
[2025-10-26T15:40:50.563Z] [LOG] [API] WebGPU available
[2025-10-26T15:40:50.571Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:40:50.571Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:40:50.571Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:40:50.587Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:41:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:41:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:41:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:41:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T15:41:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:41:50.373Z"
}
[2025-10-26T15:41:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T15:41:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:41:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:41:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:41:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:42:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:42:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:42:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:42:50.385Z] [LOG] [API] Checking server status...
[2025-10-26T15:42:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:42:50.388Z"
}
[2025-10-26T15:42:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T15:42:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:42:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:42:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:42:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:43:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:43:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:43:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:43:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T15:43:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:43:50.377Z"
}
[2025-10-26T15:43:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T15:43:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:43:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:43:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:43:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:44:50.471Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:44:50.471Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:44:50.475Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:44:50.475Z] [LOG] [API] Checking server status...
[2025-10-26T15:44:50.485Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:44:50.476Z"
}
[2025-10-26T15:44:50.485Z] [LOG] [API] WebGPU available
[2025-10-26T15:44:50.495Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:44:50.495Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:44:50.495Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:44:50.508Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:45:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:45:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:45:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:45:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T15:45:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:45:50.375Z"
}
[2025-10-26T15:45:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T15:45:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:45:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:45:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:45:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:46:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:46:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:46:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:46:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T15:46:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:46:50.397Z"
}
[2025-10-26T15:46:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T15:46:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:46:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:46:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:46:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:47:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:47:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:47:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:47:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T15:47:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:47:50.376Z"
}
[2025-10-26T15:47:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T15:47:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:47:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:47:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:47:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:48:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:48:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:48:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:48:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T15:48:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:48:50.407Z"
}
[2025-10-26T15:48:50.418Z] [LOG] [API] WebGPU available
[2025-10-26T15:48:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:48:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:48:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:48:50.455Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:49:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:49:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:49:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:49:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T15:49:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:49:50.372Z"
}
[2025-10-26T15:49:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T15:49:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:49:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:49:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:49:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:50:50.519Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:50:50.519Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:50:50.521Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:50:50.521Z] [LOG] [API] Checking server status...
[2025-10-26T15:50:50.530Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:50:50.522Z"
}
[2025-10-26T15:50:50.530Z] [LOG] [API] WebGPU available
[2025-10-26T15:50:50.537Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:50:50.537Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:50:50.537Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:50:50.565Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:51:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:51:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:51:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:51:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T15:51:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:51:50.380Z"
}
[2025-10-26T15:51:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T15:51:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:51:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:51:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:51:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:52:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:52:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:52:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:52:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T15:52:50.421Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:52:50.411Z"
}
[2025-10-26T15:52:50.421Z] [LOG] [API] WebGPU available
[2025-10-26T15:52:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:52:50.429Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:52:50.429Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:52:50.474Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:53:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:53:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:53:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:53:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T15:53:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:53:50.379Z"
}
[2025-10-26T15:53:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T15:53:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:53:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:53:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:53:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:54:50.452Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:54:50.452Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:54:50.456Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:54:50.456Z] [LOG] [API] Checking server status...
[2025-10-26T15:54:50.466Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:54:50.457Z"
}
[2025-10-26T15:54:50.466Z] [LOG] [API] WebGPU available
[2025-10-26T15:54:50.473Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:54:50.473Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:54:50.473Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:54:50.489Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:55:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:55:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:55:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:55:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T15:55:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:55:50.379Z"
}
[2025-10-26T15:55:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T15:55:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:55:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:55:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:55:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:56:50.555Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:56:50.555Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:56:50.558Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:56:50.558Z] [LOG] [API] Checking server status...
[2025-10-26T15:56:50.568Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:56:50.559Z"
}
[2025-10-26T15:56:50.568Z] [LOG] [API] WebGPU available
[2025-10-26T15:56:50.579Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:56:50.579Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:56:50.579Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:56:50.591Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:57:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:57:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:57:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:57:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T15:57:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:57:50.375Z"
}
[2025-10-26T15:57:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T15:57:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:57:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:57:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:57:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:58:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:58:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:58:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:58:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T15:58:50.460Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:58:50.415Z"
}
[2025-10-26T15:58:50.460Z] [LOG] [API] WebGPU available
[2025-10-26T15:58:50.470Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:58:50.470Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:58:50.470Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:58:50.482Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T15:59:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T15:59:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T15:59:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T15:59:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T15:59:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T15:59:50.392Z"
}
[2025-10-26T15:59:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T15:59:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T15:59:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T15:59:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T15:59:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:00:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:00:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:00:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:00:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T16:00:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:00:50.388Z"
}
[2025-10-26T16:00:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T16:00:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:00:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:00:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:00:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:01:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:01:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:01:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:01:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T16:01:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:01:50.370Z"
}
[2025-10-26T16:01:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T16:01:50.382Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:01:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:01:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:01:50.398Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:02:50.430Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:02:50.430Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:02:50.433Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:02:50.433Z] [LOG] [API] Checking server status...
[2025-10-26T16:02:50.442Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:02:50.434Z"
}
[2025-10-26T16:02:50.442Z] [LOG] [API] WebGPU available
[2025-10-26T16:02:50.446Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:02:50.446Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:02:50.446Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:02:50.467Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:03:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:03:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:03:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:03:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T16:03:50.391Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:03:50.378Z"
}
[2025-10-26T16:03:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T16:03:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:03:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:03:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:03:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:04:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:04:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:04:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:04:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T16:04:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:04:50.385Z"
}
[2025-10-26T16:04:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T16:04:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:04:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:04:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:04:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:05:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:05:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:05:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:05:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T16:05:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:05:50.365Z"
}
[2025-10-26T16:05:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T16:05:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:05:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:05:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:05:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:06:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:06:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:06:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:06:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T16:06:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:06:50.397Z"
}
[2025-10-26T16:06:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T16:06:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:06:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:06:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:06:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:07:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:07:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:07:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:07:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T16:07:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:07:50.386Z"
}
[2025-10-26T16:07:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T16:07:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:07:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:07:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:07:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:08:50.555Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:08:50.555Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:08:50.558Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:08:50.558Z] [LOG] [API] Checking server status...
[2025-10-26T16:08:50.567Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:08:50.559Z"
}
[2025-10-26T16:08:50.567Z] [LOG] [API] WebGPU available
[2025-10-26T16:08:50.585Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:08:50.585Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:08:50.585Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:08:50.606Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:09:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:09:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:09:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:09:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T16:09:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:09:50.378Z"
}
[2025-10-26T16:09:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T16:09:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:09:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:09:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:09:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:10:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:10:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:10:50.406Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:10:50.406Z] [LOG] [API] Checking server status...
[2025-10-26T16:10:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:10:50.408Z"
}
[2025-10-26T16:10:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T16:10:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:10:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:10:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:10:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:11:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:11:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:11:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:11:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T16:11:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:11:50.372Z"
}
[2025-10-26T16:11:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T16:11:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:11:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:11:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:11:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:12:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:12:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:12:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:12:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T16:12:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:12:50.402Z"
}
[2025-10-26T16:12:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T16:12:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:12:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:12:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:12:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:13:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:13:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:13:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:13:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T16:13:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:13:50.382Z"
}
[2025-10-26T16:13:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T16:13:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:13:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:13:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:13:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:14:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:14:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:14:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:14:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T16:14:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:14:50.382Z"
}
[2025-10-26T16:14:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T16:14:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:14:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:14:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:14:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:15:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:15:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:15:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:15:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T16:15:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:15:50.371Z"
}
[2025-10-26T16:15:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T16:15:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:15:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:15:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:15:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:16:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:16:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:16:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:16:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T16:16:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:16:50.390Z"
}
[2025-10-26T16:16:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T16:16:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:16:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:16:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:16:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:17:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:17:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:17:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:17:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T16:17:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:17:50.366Z"
}
[2025-10-26T16:17:50.373Z] [LOG] [API] WebGPU available
[2025-10-26T16:17:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:17:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:17:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:17:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:18:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:18:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:18:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:18:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T16:18:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:18:50.403Z"
}
[2025-10-26T16:18:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T16:18:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:18:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:18:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:18:50.452Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:19:50.449Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:19:50.450Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:19:50.452Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:19:50.452Z] [LOG] [API] Checking server status...
[2025-10-26T16:19:50.464Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:19:50.453Z"
}
[2025-10-26T16:19:50.464Z] [LOG] [API] WebGPU available
[2025-10-26T16:19:50.472Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:19:50.472Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:19:50.472Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:19:50.488Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:20:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:20:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:20:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:20:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T16:20:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:20:50.367Z"
}
[2025-10-26T16:20:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T16:20:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:20:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:20:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:20:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:21:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:21:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:21:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:21:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T16:21:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:21:50.386Z"
}
[2025-10-26T16:21:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T16:21:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:21:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:21:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:21:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:22:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:22:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:22:50.359Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:22:50.359Z] [LOG] [API] Checking server status...
[2025-10-26T16:22:50.369Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:22:50.361Z"
}
[2025-10-26T16:22:50.369Z] [LOG] [API] WebGPU available
[2025-10-26T16:22:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:22:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:22:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:22:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:23:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:23:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:23:50.443Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:23:50.443Z] [LOG] [API] Checking server status...
[2025-10-26T16:23:50.454Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:23:50.444Z"
}
[2025-10-26T16:23:50.454Z] [LOG] [API] WebGPU available
[2025-10-26T16:23:50.458Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:23:50.458Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:23:50.458Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:23:50.477Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:24:50.353Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:24:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:24:50.355Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:24:50.355Z] [LOG] [API] Checking server status...
[2025-10-26T16:24:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:24:50.357Z"
}
[2025-10-26T16:24:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T16:24:50.380Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:24:50.380Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:24:50.380Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:24:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:25:50.555Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:25:50.555Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:25:50.558Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:25:50.558Z] [LOG] [API] Checking server status...
[2025-10-26T16:25:50.570Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:25:50.559Z"
}
[2025-10-26T16:25:50.570Z] [LOG] [API] WebGPU available
[2025-10-26T16:25:50.578Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:25:50.578Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:25:50.578Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:25:50.584Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:26:50.353Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:26:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:26:50.356Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:26:50.356Z] [LOG] [API] Checking server status...
[2025-10-26T16:26:50.367Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:26:50.358Z"
}
[2025-10-26T16:26:50.368Z] [LOG] [API] WebGPU available
[2025-10-26T16:26:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:26:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:26:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:26:50.401Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:27:50.353Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:27:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:27:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:27:50.416Z] [LOG] [API] Checking server status...
[2025-10-26T16:27:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:27:50.417Z"
}
[2025-10-26T16:27:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T16:27:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:27:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:27:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:27:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:28:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:28:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:28:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:28:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T16:28:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:28:50.364Z"
}
[2025-10-26T16:28:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T16:28:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:28:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:28:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:28:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:29:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:29:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:29:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:29:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T16:29:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:29:50.392Z"
}
[2025-10-26T16:29:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T16:29:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:29:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:29:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:29:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:30:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:30:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:30:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:30:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T16:30:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:30:50.367Z"
}
[2025-10-26T16:30:50.375Z] [LOG] [API] WebGPU available
[2025-10-26T16:30:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:30:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:30:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:30:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:31:50.515Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:31:50.515Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:31:50.518Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:31:50.518Z] [LOG] [API] Checking server status...
[2025-10-26T16:31:50.528Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:31:50.519Z"
}
[2025-10-26T16:31:50.528Z] [LOG] [API] WebGPU available
[2025-10-26T16:31:50.537Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:31:50.537Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:31:50.537Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:31:50.550Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:32:50.344Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:32:50.344Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:32:50.347Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:32:50.347Z] [LOG] [API] Checking server status...
[2025-10-26T16:32:50.357Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:32:50.349Z"
}
[2025-10-26T16:32:50.357Z] [LOG] [API] WebGPU available
[2025-10-26T16:32:50.360Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:32:50.360Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:32:50.360Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:32:50.389Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:33:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:33:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:33:50.418Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:33:50.418Z] [LOG] [API] Checking server status...
[2025-10-26T16:33:50.430Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:33:50.419Z"
}
[2025-10-26T16:33:50.430Z] [LOG] [API] WebGPU available
[2025-10-26T16:33:50.438Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:33:50.438Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:33:50.438Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:33:50.470Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:34:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:34:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:34:50.359Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:34:50.359Z] [LOG] [API] Checking server status...
[2025-10-26T16:34:50.372Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:34:50.361Z"
}
[2025-10-26T16:34:50.372Z] [LOG] [API] WebGPU available
[2025-10-26T16:34:50.377Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:34:50.377Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:34:50.377Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:34:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:35:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:35:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:35:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:35:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T16:35:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:35:50.389Z"
}
[2025-10-26T16:35:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T16:35:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:35:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:35:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:35:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:36:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:36:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:36:50.359Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:36:50.359Z] [LOG] [API] Checking server status...
[2025-10-26T16:36:50.369Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:36:50.361Z"
}
[2025-10-26T16:36:50.369Z] [LOG] [API] WebGPU available
[2025-10-26T16:36:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:36:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:36:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:36:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:37:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:37:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:37:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:37:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T16:37:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:37:50.400Z"
}
[2025-10-26T16:37:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T16:37:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:37:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:37:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:37:50.463Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:38:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:38:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:38:50.359Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:38:50.359Z] [LOG] [API] Checking server status...
[2025-10-26T16:38:50.371Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:38:50.361Z"
}
[2025-10-26T16:38:50.371Z] [LOG] [API] WebGPU available
[2025-10-26T16:38:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:38:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:38:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:38:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:39:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:39:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:39:50.423Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:39:50.423Z] [LOG] [API] Checking server status...
[2025-10-26T16:39:50.464Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:39:50.424Z"
}
[2025-10-26T16:39:50.464Z] [LOG] [API] WebGPU available
[2025-10-26T16:39:50.476Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:39:50.476Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:39:50.477Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:39:50.487Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:40:50.353Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:40:50.354Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:40:50.357Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:40:50.357Z] [LOG] [API] Checking server status...
[2025-10-26T16:40:50.368Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:40:50.358Z"
}
[2025-10-26T16:40:50.368Z] [LOG] [API] WebGPU available
[2025-10-26T16:40:50.374Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:40:50.374Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:40:50.374Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:40:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:41:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:41:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:41:50.435Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:41:50.436Z] [LOG] [API] Checking server status...
[2025-10-26T16:41:50.448Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:41:50.437Z"
}
[2025-10-26T16:41:50.448Z] [LOG] [API] WebGPU available
[2025-10-26T16:41:50.459Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:41:50.459Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:41:50.459Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:41:50.471Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:42:50.400Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:42:50.400Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:42:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:42:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T16:42:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:42:50.405Z"
}
[2025-10-26T16:42:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T16:42:50.440Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:42:50.440Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:42:50.440Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:42:50.459Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:43:50.483Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:43:50.483Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:43:50.485Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:43:50.485Z] [LOG] [API] Checking server status...
[2025-10-26T16:43:50.495Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:43:50.486Z"
}
[2025-10-26T16:43:50.495Z] [LOG] [API] WebGPU available
[2025-10-26T16:43:50.499Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:43:50.499Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:43:50.499Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:43:50.518Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:44:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:44:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:44:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:44:50.361Z] [LOG] [API] Checking server status...
[2025-10-26T16:44:50.372Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:44:50.362Z"
}
[2025-10-26T16:44:50.372Z] [LOG] [API] WebGPU available
[2025-10-26T16:44:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:44:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:44:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:44:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:45:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:45:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:45:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:45:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T16:45:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:45:50.389Z"
}
[2025-10-26T16:45:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T16:45:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:45:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:45:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:45:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:46:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:46:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:46:50.359Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:46:50.359Z] [LOG] [API] Checking server status...
[2025-10-26T16:46:50.369Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:46:50.361Z"
}
[2025-10-26T16:46:50.369Z] [LOG] [API] WebGPU available
[2025-10-26T16:46:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:46:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:46:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:46:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:47:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:47:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:47:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:47:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T16:47:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:47:50.396Z"
}
[2025-10-26T16:47:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T16:47:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:47:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:47:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:47:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:48:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:48:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:48:50.363Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:48:50.363Z] [LOG] [API] Checking server status...
[2025-10-26T16:48:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:48:50.365Z"
}
[2025-10-26T16:48:50.373Z] [LOG] [API] WebGPU available
[2025-10-26T16:48:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:48:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:48:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:48:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:49:50.461Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:49:50.461Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:49:50.464Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:49:50.464Z] [LOG] [API] Checking server status...
[2025-10-26T16:49:50.476Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:49:50.466Z"
}
[2025-10-26T16:49:50.476Z] [LOG] [API] WebGPU available
[2025-10-26T16:49:50.478Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:49:50.478Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:49:50.479Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:49:50.499Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:50:50.353Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:50:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:50:50.356Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:50:50.356Z] [LOG] [API] Checking server status...
[2025-10-26T16:50:50.367Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:50:50.358Z"
}
[2025-10-26T16:50:50.367Z] [LOG] [API] WebGPU available
[2025-10-26T16:50:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:50:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:50:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:50:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:51:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:51:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:51:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:51:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T16:51:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:51:50.401Z"
}
[2025-10-26T16:51:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T16:51:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:51:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:51:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:51:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:52:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:52:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:52:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:52:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T16:52:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:52:50.393Z"
}
[2025-10-26T16:52:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T16:52:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:52:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:52:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:52:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:53:50.352Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:53:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:53:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:53:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T16:53:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:53:50.392Z"
}
[2025-10-26T16:53:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T16:53:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:53:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:53:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:53:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:54:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:54:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:54:50.359Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:54:50.359Z] [LOG] [API] Checking server status...
[2025-10-26T16:54:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:54:50.361Z"
}
[2025-10-26T16:54:50.373Z] [LOG] [API] WebGPU available
[2025-10-26T16:54:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:54:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:54:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:54:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:55:50.499Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:55:50.500Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:55:50.730Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:55:50.730Z] [LOG] [API] Checking server status...
[2025-10-26T16:55:50.740Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:55:50.731Z"
}
[2025-10-26T16:55:50.740Z] [LOG] [API] WebGPU available
[2025-10-26T16:55:50.747Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:55:50.747Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:55:50.747Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:55:50.759Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:56:50.354Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:56:50.354Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:56:50.357Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:56:50.357Z] [LOG] [API] Checking server status...
[2025-10-26T16:56:50.367Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:56:50.359Z"
}
[2025-10-26T16:56:50.367Z] [LOG] [API] WebGPU available
[2025-10-26T16:56:50.373Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:56:50.373Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:56:50.373Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:56:50.398Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:57:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:57:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:57:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:57:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T16:57:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:57:50.395Z"
}
[2025-10-26T16:57:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T16:57:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:57:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:57:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:57:50.431Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:58:50.449Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:58:50.449Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:58:50.452Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:58:50.452Z] [LOG] [API] Checking server status...
[2025-10-26T16:58:50.462Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:58:50.454Z"
}
[2025-10-26T16:58:50.462Z] [LOG] [API] WebGPU available
[2025-10-26T16:58:50.489Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:58:50.489Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:58:50.489Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:58:50.495Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T16:59:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T16:59:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T16:59:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T16:59:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T16:59:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T16:59:50.366Z"
}
[2025-10-26T16:59:50.373Z] [LOG] [API] WebGPU available
[2025-10-26T16:59:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T16:59:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T16:59:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T16:59:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:00:50.351Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:00:50.352Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:00:50.355Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:00:50.355Z] [LOG] [API] Checking server status...
[2025-10-26T17:00:50.364Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:00:50.356Z"
}
[2025-10-26T17:00:50.364Z] [LOG] [API] WebGPU available
[2025-10-26T17:00:50.372Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:00:50.372Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:00:50.372Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:00:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:01:50.518Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:01:50.518Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:01:50.521Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:01:50.521Z] [LOG] [API] Checking server status...
[2025-10-26T17:01:50.531Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:01:50.522Z"
}
[2025-10-26T17:01:50.531Z] [LOG] [API] WebGPU available
[2025-10-26T17:01:50.539Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:01:50.539Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:01:50.539Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:01:50.554Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:02:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:02:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:02:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:02:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T17:02:50.372Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:02:50.361Z"
}
[2025-10-26T17:02:50.372Z] [LOG] [API] WebGPU available
[2025-10-26T17:02:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:02:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:02:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:02:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:03:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:03:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:03:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:03:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T17:03:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:03:50.399Z"
}
[2025-10-26T17:03:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T17:03:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:03:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:03:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:03:50.451Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:04:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:04:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:04:50.358Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:04:50.359Z] [LOG] [API] Checking server status...
[2025-10-26T17:04:50.369Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:04:50.360Z"
}
[2025-10-26T17:04:50.369Z] [LOG] [API] WebGPU available
[2025-10-26T17:04:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:04:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:04:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:04:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:05:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:05:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:05:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:05:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T17:05:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:05:50.403Z"
}
[2025-10-26T17:05:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T17:05:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:05:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:05:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:05:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:06:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:06:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:06:50.359Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:06:50.359Z] [LOG] [API] Checking server status...
[2025-10-26T17:06:50.370Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:06:50.360Z"
}
[2025-10-26T17:06:50.370Z] [LOG] [API] WebGPU available
[2025-10-26T17:06:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:06:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:06:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:06:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:07:50.444Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:07:50.444Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:07:50.446Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:07:50.446Z] [LOG] [API] Checking server status...
[2025-10-26T17:07:50.457Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:07:50.448Z"
}
[2025-10-26T17:07:50.457Z] [LOG] [API] WebGPU available
[2025-10-26T17:07:50.465Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:07:50.466Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:07:50.466Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:07:50.483Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:08:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:08:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:08:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:08:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T17:08:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:08:50.368Z"
}
[2025-10-26T17:08:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T17:08:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:08:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:08:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:08:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:09:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:09:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:09:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:09:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T17:09:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:09:50.391Z"
}
[2025-10-26T17:09:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T17:09:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:09:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:09:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:09:50.451Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:10:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:10:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:10:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:10:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T17:10:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:10:50.373Z"
}
[2025-10-26T17:10:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T17:10:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:10:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:10:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:10:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:11:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:11:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:11:50.439Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:11:50.439Z] [LOG] [API] Checking server status...
[2025-10-26T17:11:50.451Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:11:50.440Z"
}
[2025-10-26T17:11:50.451Z] [LOG] [API] WebGPU available
[2025-10-26T17:11:50.461Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:11:50.461Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:11:50.461Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:11:50.476Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:12:50.511Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:12:50.511Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:12:50.538Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:12:50.538Z] [LOG] [API] Checking server status...
[2025-10-26T17:12:50.552Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:12:50.539Z"
}
[2025-10-26T17:12:50.552Z] [LOG] [API] WebGPU available
[2025-10-26T17:12:50.559Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:12:50.559Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:12:50.559Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:12:50.577Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:13:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:13:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:13:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:13:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T17:13:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:13:50.370Z"
}
[2025-10-26T17:13:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T17:13:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:13:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:13:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:13:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:14:50.481Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:14:50.482Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:14:50.484Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:14:50.484Z] [LOG] [API] Checking server status...
[2025-10-26T17:14:50.496Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:14:50.486Z"
}
[2025-10-26T17:14:50.496Z] [LOG] [API] WebGPU available
[2025-10-26T17:14:50.503Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:14:50.503Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:14:50.503Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:14:50.509Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:15:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:15:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:15:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:15:50.361Z] [LOG] [API] Checking server status...
[2025-10-26T17:15:50.371Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:15:50.362Z"
}
[2025-10-26T17:15:50.371Z] [LOG] [API] WebGPU available
[2025-10-26T17:15:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:15:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:15:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:15:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:16:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:16:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:16:50.435Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:16:50.435Z] [LOG] [API] Checking server status...
[2025-10-26T17:16:50.447Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:16:50.436Z"
}
[2025-10-26T17:16:50.447Z] [LOG] [API] WebGPU available
[2025-10-26T17:16:50.454Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:16:50.454Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:16:50.454Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:16:50.460Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:17:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:17:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:17:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:17:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T17:17:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:17:50.364Z"
}
[2025-10-26T17:17:50.373Z] [LOG] [API] WebGPU available
[2025-10-26T17:17:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:17:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:17:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:17:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:18:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:18:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:18:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:18:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T17:18:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:18:50.399Z"
}
[2025-10-26T17:18:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T17:18:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:18:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:18:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:18:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:19:50.351Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:19:50.351Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:19:50.354Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:19:50.354Z] [LOG] [API] Checking server status...
[2025-10-26T17:19:50.365Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:19:50.357Z"
}
[2025-10-26T17:19:50.365Z] [LOG] [API] WebGPU available
[2025-10-26T17:19:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:19:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:19:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:19:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:20:50.559Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:20:50.559Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:20:50.562Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:20:50.562Z] [LOG] [API] Checking server status...
[2025-10-26T17:20:50.574Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:20:50.563Z"
}
[2025-10-26T17:20:50.574Z] [LOG] [API] WebGPU available
[2025-10-26T17:20:50.582Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:20:50.582Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:20:50.583Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:20:50.588Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:21:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:21:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:21:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:21:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T17:21:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:21:50.372Z"
}
[2025-10-26T17:21:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T17:21:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:21:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:21:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:21:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:22:50.351Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:22:50.352Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:22:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:22:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T17:22:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:22:50.396Z"
}
[2025-10-26T17:22:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T17:22:50.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:22:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:22:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:22:50.459Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:23:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:23:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:23:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:23:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T17:23:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:23:50.367Z"
}
[2025-10-26T17:23:50.375Z] [LOG] [API] WebGPU available
[2025-10-26T17:23:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:23:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:23:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:23:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:24:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:24:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:24:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:24:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T17:24:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:24:50.400Z"
}
[2025-10-26T17:24:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T17:24:50.414Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:24:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:24:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:24:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:25:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:25:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:25:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:25:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T17:25:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:25:50.370Z"
}
[2025-10-26T17:25:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T17:25:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:25:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:25:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:25:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:26:50.564Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:26:50.565Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:26:50.567Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:26:50.567Z] [LOG] [API] Checking server status...
[2025-10-26T17:26:50.576Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:26:50.568Z"
}
[2025-10-26T17:26:50.576Z] [LOG] [API] WebGPU available
[2025-10-26T17:26:50.582Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:26:50.583Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:26:50.583Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:26:50.594Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:27:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:27:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:27:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:27:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T17:27:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:27:50.365Z"
}
[2025-10-26T17:27:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T17:27:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:27:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:27:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:27:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:28:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:28:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:28:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:28:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T17:28:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:28:50.399Z"
}
[2025-10-26T17:28:50.410Z] [LOG] [API] WebGPU available
[2025-10-26T17:28:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:28:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:28:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:28:50.450Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:29:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:29:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:29:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:29:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T17:29:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:29:50.379Z"
}
[2025-10-26T17:29:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T17:29:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:29:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:29:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:29:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:30:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:30:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:30:50.441Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:30:50.441Z] [LOG] [API] Checking server status...
[2025-10-26T17:30:50.454Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:30:50.443Z"
}
[2025-10-26T17:30:50.454Z] [LOG] [API] WebGPU available
[2025-10-26T17:30:50.464Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:30:50.464Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:30:50.464Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:30:50.482Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:31:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:31:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:31:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:31:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T17:31:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:31:50.365Z"
}
[2025-10-26T17:31:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T17:31:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:31:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:31:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:31:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:32:50.451Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:32:50.452Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:32:50.454Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:32:50.454Z] [LOG] [API] Checking server status...
[2025-10-26T17:32:50.462Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:32:50.455Z"
}
[2025-10-26T17:32:50.462Z] [LOG] [API] WebGPU available
[2025-10-26T17:32:50.465Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:32:50.465Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:32:50.465Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:32:50.480Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:33:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:33:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:33:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:33:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T17:33:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:33:50.367Z"
}
[2025-10-26T17:33:50.375Z] [LOG] [API] WebGPU available
[2025-10-26T17:33:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:33:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:33:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:33:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:34:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:34:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:34:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:34:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T17:34:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:34:50.389Z"
}
[2025-10-26T17:34:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T17:34:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:34:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:34:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:34:50.438Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:35:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:35:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:35:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:35:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T17:35:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:35:50.378Z"
}
[2025-10-26T17:35:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T17:35:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:35:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:35:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:35:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:36:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:36:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:36:50.422Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:36:50.422Z] [LOG] [API] Checking server status...
[2025-10-26T17:36:50.430Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:36:50.423Z"
}
[2025-10-26T17:36:50.430Z] [LOG] [API] WebGPU available
[2025-10-26T17:36:50.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:36:50.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:36:50.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:36:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:37:50.353Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:37:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:37:50.356Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:37:50.356Z] [LOG] [API] Checking server status...
[2025-10-26T17:37:50.366Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:37:50.358Z"
}
[2025-10-26T17:37:50.366Z] [LOG] [API] WebGPU available
[2025-10-26T17:37:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:37:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:37:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:37:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:38:50.480Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:38:50.480Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:38:50.482Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:38:50.482Z] [LOG] [API] Checking server status...
[2025-10-26T17:38:50.492Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:38:50.483Z"
}
[2025-10-26T17:38:50.492Z] [LOG] [API] WebGPU available
[2025-10-26T17:38:50.496Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:38:50.496Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:38:50.496Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:38:50.520Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:39:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:39:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:39:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:39:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T17:39:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:39:50.364Z"
}
[2025-10-26T17:39:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T17:39:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:39:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:39:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:39:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:40:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:40:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:40:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:40:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T17:40:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:40:50.392Z"
}
[2025-10-26T17:40:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T17:40:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:40:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:40:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:40:50.457Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:41:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:41:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:41:50.358Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:41:50.358Z] [LOG] [API] Checking server status...
[2025-10-26T17:41:50.368Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:41:50.360Z"
}
[2025-10-26T17:41:50.368Z] [LOG] [API] WebGPU available
[2025-10-26T17:41:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:41:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:41:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:41:50.395Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:42:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:42:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:42:50.443Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:42:50.443Z] [LOG] [API] Checking server status...
[2025-10-26T17:42:50.456Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:42:50.444Z"
}
[2025-10-26T17:42:50.456Z] [LOG] [API] WebGPU available
[2025-10-26T17:42:50.463Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:42:50.463Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:42:50.463Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:42:50.479Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:43:50.353Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:43:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:43:50.356Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:43:50.356Z] [LOG] [API] Checking server status...
[2025-10-26T17:43:50.367Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:43:50.357Z"
}
[2025-10-26T17:43:50.367Z] [LOG] [API] WebGPU available
[2025-10-26T17:43:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:43:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:43:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:43:50.392Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:44:50.521Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:44:50.521Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:44:50.523Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:44:50.523Z] [LOG] [API] Checking server status...
[2025-10-26T17:44:50.531Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:44:50.524Z"
}
[2025-10-26T17:44:50.531Z] [LOG] [API] WebGPU available
[2025-10-26T17:44:50.535Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:44:50.535Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:44:50.535Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:44:50.552Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:45:50.351Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:45:50.352Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:45:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:45:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T17:45:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:45:50.373Z"
}
[2025-10-26T17:45:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T17:45:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:45:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:45:50.384Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:45:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:46:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:46:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:46:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:46:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T17:46:50.401Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:46:50.392Z"
}
[2025-10-26T17:46:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T17:46:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:46:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:46:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:46:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:47:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:47:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:47:50.358Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:47:50.358Z] [LOG] [API] Checking server status...
[2025-10-26T17:47:50.369Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:47:50.360Z"
}
[2025-10-26T17:47:50.369Z] [LOG] [API] WebGPU available
[2025-10-26T17:47:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:47:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:47:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:47:50.395Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:48:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:48:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:48:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:48:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T17:48:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:48:50.380Z"
}
[2025-10-26T17:48:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T17:48:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:48:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:48:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:48:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:49:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:49:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:49:50.359Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:49:50.359Z] [LOG] [API] Checking server status...
[2025-10-26T17:49:50.369Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:49:50.360Z"
}
[2025-10-26T17:49:50.369Z] [LOG] [API] WebGPU available
[2025-10-26T17:49:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:49:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:49:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:49:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:50:50.507Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:50:50.507Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:50:50.509Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:50:50.509Z] [LOG] [API] Checking server status...
[2025-10-26T17:50:50.519Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:50:50.510Z"
}
[2025-10-26T17:50:50.519Z] [LOG] [API] WebGPU available
[2025-10-26T17:50:50.526Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:50:50.526Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:50:50.526Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:50:50.541Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:51:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:51:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:51:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:51:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T17:51:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:51:50.365Z"
}
[2025-10-26T17:51:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T17:51:50.380Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:51:50.381Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:51:50.381Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:51:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:52:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:52:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:52:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:52:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T17:52:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:52:50.394Z"
}
[2025-10-26T17:52:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T17:52:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:52:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:52:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:52:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:53:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:53:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:53:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:53:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T17:53:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:53:50.371Z"
}
[2025-10-26T17:53:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T17:53:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:53:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:53:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:53:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:54:50.349Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:54:50.349Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:54:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:54:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T17:54:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:54:50.372Z"
}
[2025-10-26T17:54:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T17:54:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:54:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:54:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:54:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:55:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:55:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:55:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:55:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T17:55:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:55:50.371Z"
}
[2025-10-26T17:55:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T17:55:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:55:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:55:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:55:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:56:50.531Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:56:50.531Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:56:50.533Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:56:50.533Z] [LOG] [API] Checking server status...
[2025-10-26T17:56:50.544Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:56:50.535Z"
}
[2025-10-26T17:56:50.544Z] [LOG] [API] WebGPU available
[2025-10-26T17:56:50.549Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:56:50.550Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:56:50.550Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:56:50.563Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:57:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:57:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:57:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:57:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T17:57:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:57:50.364Z"
}
[2025-10-26T17:57:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T17:57:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:57:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:57:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:57:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:58:50.406Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:58:50.406Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:58:50.493Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:58:50.493Z] [LOG] [API] Checking server status...
[2025-10-26T17:58:50.504Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:58:50.494Z"
}
[2025-10-26T17:58:50.504Z] [LOG] [API] WebGPU available
[2025-10-26T17:58:50.513Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:58:50.513Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:58:50.513Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:58:50.529Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T17:59:50.351Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T17:59:50.351Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T17:59:50.354Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T17:59:50.354Z] [LOG] [API] Checking server status...
[2025-10-26T17:59:50.365Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T17:59:50.356Z"
}
[2025-10-26T17:59:50.366Z] [LOG] [API] WebGPU available
[2025-10-26T17:59:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T17:59:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T17:59:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T17:59:50.398Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:00:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:00:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:00:50.440Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:00:50.440Z] [LOG] [API] Checking server status...
[2025-10-26T18:00:50.451Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:00:50.441Z"
}
[2025-10-26T18:00:50.451Z] [LOG] [API] WebGPU available
[2025-10-26T18:00:50.460Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:00:50.460Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:00:50.460Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:00:50.477Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:01:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:01:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:01:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:01:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T18:01:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:01:50.366Z"
}
[2025-10-26T18:01:50.375Z] [LOG] [API] WebGPU available
[2025-10-26T18:01:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:01:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:01:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:01:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:02:50.453Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:02:50.453Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:02:50.455Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:02:50.455Z] [LOG] [API] Checking server status...
[2025-10-26T18:02:50.463Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:02:50.457Z"
}
[2025-10-26T18:02:50.463Z] [LOG] [API] WebGPU available
[2025-10-26T18:02:50.467Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:02:50.467Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:02:50.467Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:02:50.486Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:03:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:03:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:03:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:03:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T18:03:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:03:50.365Z"
}
[2025-10-26T18:03:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T18:03:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:03:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:03:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:03:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:04:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:04:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:04:50.467Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:04:50.467Z] [LOG] [API] Checking server status...
[2025-10-26T18:04:50.476Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:04:50.468Z"
}
[2025-10-26T18:04:50.476Z] [LOG] [API] WebGPU available
[2025-10-26T18:04:50.483Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:04:50.483Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:04:50.483Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:04:50.505Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:05:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:05:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:05:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:05:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T18:05:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:05:50.367Z"
}
[2025-10-26T18:05:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T18:05:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:05:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:05:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:05:50.401Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:06:50.351Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:06:50.352Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:06:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:06:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T18:06:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:06:50.401Z"
}
[2025-10-26T18:06:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T18:06:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:06:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:06:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:06:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:07:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:07:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:07:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:07:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T18:07:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:07:50.379Z"
}
[2025-10-26T18:07:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T18:07:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:07:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:07:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:07:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:08:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:08:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:08:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:08:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T18:08:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:08:50.381Z"
}
[2025-10-26T18:08:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T18:08:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:08:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:08:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:08:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:09:50.352Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:09:50.352Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:09:50.355Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:09:50.355Z] [LOG] [API] Checking server status...
[2025-10-26T18:09:50.367Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:09:50.357Z"
}
[2025-10-26T18:09:50.367Z] [LOG] [API] WebGPU available
[2025-10-26T18:09:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:09:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:09:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:09:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:10:50.354Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:10:50.354Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:10:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:10:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T18:10:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:10:50.378Z"
}
[2025-10-26T18:10:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T18:10:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:10:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:10:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:10:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:11:50.351Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:11:50.351Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:11:50.354Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:11:50.354Z] [LOG] [API] Checking server status...
[2025-10-26T18:11:50.364Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:11:50.356Z"
}
[2025-10-26T18:11:50.365Z] [LOG] [API] WebGPU available
[2025-10-26T18:11:50.381Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:11:50.381Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:11:50.381Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:11:50.400Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:12:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:12:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:12:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:12:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T18:12:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:12:50.383Z"
}
[2025-10-26T18:12:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T18:12:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:12:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:12:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:12:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:13:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:13:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:13:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:13:50.361Z] [LOG] [API] Checking server status...
[2025-10-26T18:13:50.371Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:13:50.362Z"
}
[2025-10-26T18:13:50.371Z] [LOG] [API] WebGPU available
[2025-10-26T18:13:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:13:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:13:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:13:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:14:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:14:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:14:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:14:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T18:14:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:14:50.396Z"
}
[2025-10-26T18:14:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T18:14:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:14:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:14:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:14:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:15:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:15:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:15:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:15:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T18:15:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:15:50.373Z"
}
[2025-10-26T18:15:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T18:15:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:15:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:15:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:15:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:16:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:16:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:16:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:16:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T18:16:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:16:50.392Z"
}
[2025-10-26T18:16:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T18:16:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:16:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:16:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:16:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:17:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:17:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:17:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:17:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T18:17:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:17:50.369Z"
}
[2025-10-26T18:17:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T18:17:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:17:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:17:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:17:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:18:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:18:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:18:50.426Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:18:50.426Z] [LOG] [API] Checking server status...
[2025-10-26T18:18:50.437Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:18:50.427Z"
}
[2025-10-26T18:18:50.437Z] [LOG] [API] WebGPU available
[2025-10-26T18:18:50.445Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:18:50.445Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:18:50.445Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:18:50.456Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:19:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:19:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:19:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:19:50.361Z] [LOG] [API] Checking server status...
[2025-10-26T18:19:50.372Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:19:50.363Z"
}
[2025-10-26T18:19:50.372Z] [LOG] [API] WebGPU available
[2025-10-26T18:19:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:19:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:19:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:19:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:20:50.530Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:20:50.530Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:20:50.532Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:20:50.532Z] [LOG] [API] Checking server status...
[2025-10-26T18:20:50.541Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:20:50.533Z"
}
[2025-10-26T18:20:50.541Z] [LOG] [API] WebGPU available
[2025-10-26T18:20:50.550Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:20:50.550Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:20:50.550Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:20:50.564Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:21:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:21:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:21:50.358Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:21:50.358Z] [LOG] [API] Checking server status...
[2025-10-26T18:21:50.368Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:21:50.359Z"
}
[2025-10-26T18:21:50.368Z] [LOG] [API] WebGPU available
[2025-10-26T18:21:50.385Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:21:50.385Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:21:50.385Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:21:50.395Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:22:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:22:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:22:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:22:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T18:22:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:22:50.396Z"
}
[2025-10-26T18:22:50.404Z] [LOG] [API] WebGPU available
[2025-10-26T18:22:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:22:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:22:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:22:50.434Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:23:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:23:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:23:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:23:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T18:23:50.371Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:23:50.362Z"
}
[2025-10-26T18:23:50.371Z] [LOG] [API] WebGPU available
[2025-10-26T18:23:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:23:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:23:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:23:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:24:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:24:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:24:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:24:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T18:24:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:24:50.397Z"
}
[2025-10-26T18:24:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T18:24:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:24:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:24:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:24:50.436Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:25:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:25:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:25:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:25:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T18:25:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:25:50.379Z"
}
[2025-10-26T18:25:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T18:25:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:25:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:25:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:25:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:26:50.706Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:26:50.707Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:26:50.709Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:26:50.709Z] [LOG] [API] Checking server status...
[2025-10-26T18:26:50.720Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:26:50.711Z"
}
[2025-10-26T18:26:50.720Z] [LOG] [API] WebGPU available
[2025-10-26T18:26:50.729Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:26:50.729Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:26:50.729Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:26:50.739Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:27:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:27:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:27:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:27:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T18:27:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:27:50.380Z"
}
[2025-10-26T18:27:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T18:27:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:27:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:27:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:27:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:28:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:28:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:28:50.433Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:28:50.433Z] [LOG] [API] Checking server status...
[2025-10-26T18:28:50.444Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:28:50.435Z"
}
[2025-10-26T18:28:50.444Z] [LOG] [API] WebGPU available
[2025-10-26T18:28:50.452Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:28:50.452Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:28:50.452Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:28:50.464Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:29:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:29:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:29:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:29:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T18:29:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:29:50.370Z"
}
[2025-10-26T18:29:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T18:29:50.381Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:29:50.381Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:29:50.381Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:29:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:30:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:30:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:30:50.435Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:30:50.435Z] [LOG] [API] Checking server status...
[2025-10-26T18:30:50.444Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:30:50.437Z"
}
[2025-10-26T18:30:50.444Z] [LOG] [API] WebGPU available
[2025-10-26T18:30:50.447Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:30:50.447Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:30:50.447Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:30:50.464Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:31:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:31:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:31:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:31:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T18:31:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:31:50.368Z"
}
[2025-10-26T18:31:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T18:31:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:31:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:31:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:31:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:32:50.512Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:32:50.512Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:32:50.514Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:32:50.514Z] [LOG] [API] Checking server status...
[2025-10-26T18:32:50.524Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:32:50.516Z"
}
[2025-10-26T18:32:50.524Z] [LOG] [API] WebGPU available
[2025-10-26T18:32:50.531Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:32:50.531Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:32:50.531Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:32:50.545Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:33:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:33:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:33:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:33:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T18:33:50.371Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:33:50.362Z"
}
[2025-10-26T18:33:50.371Z] [LOG] [API] WebGPU available
[2025-10-26T18:33:50.378Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:33:50.378Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:33:50.378Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:33:50.401Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:34:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:34:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:34:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:34:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T18:34:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:34:50.399Z"
}
[2025-10-26T18:34:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T18:34:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:34:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:34:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:34:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:35:50.417Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:35:50.417Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:35:50.420Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:35:50.420Z] [LOG] [API] Checking server status...
[2025-10-26T18:35:50.430Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:35:50.422Z"
}
[2025-10-26T18:35:50.430Z] [LOG] [API] WebGPU available
[2025-10-26T18:35:50.448Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:35:50.448Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:35:50.448Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:35:50.470Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:36:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:36:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:36:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:36:50.372Z] [LOG] [API] Checking server status...
[2025-10-26T18:36:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:36:50.374Z"
}
[2025-10-26T18:36:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T18:36:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:36:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:36:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:36:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:37:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:37:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:37:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:37:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T18:37:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:37:50.404Z"
}
[2025-10-26T18:37:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T18:37:50.416Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:37:50.416Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:37:50.416Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:37:50.460Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:38:50.550Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:38:50.550Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:38:50.552Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:38:50.552Z] [LOG] [API] Checking server status...
[2025-10-26T18:38:50.561Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:38:50.553Z"
}
[2025-10-26T18:38:50.561Z] [LOG] [API] WebGPU available
[2025-10-26T18:38:50.568Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:38:50.568Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:38:50.568Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:38:50.582Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:39:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:39:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:39:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:39:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T18:39:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:39:50.371Z"
}
[2025-10-26T18:39:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T18:39:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:39:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:39:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:39:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:40:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:40:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:40:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:40:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T18:40:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:40:50.401Z"
}
[2025-10-26T18:40:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T18:40:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:40:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:40:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:40:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:41:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:41:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:41:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:41:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T18:41:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:41:50.375Z"
}
[2025-10-26T18:41:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T18:41:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:41:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:41:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:41:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:42:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:42:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:42:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:42:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T18:42:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:42:50.384Z"
}
[2025-10-26T18:42:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T18:42:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:42:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:42:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:42:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:43:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:43:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:43:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:43:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T18:43:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:43:50.372Z"
}
[2025-10-26T18:43:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T18:43:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:43:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:43:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:43:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:44:50.447Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:44:50.447Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:44:50.450Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:44:50.450Z] [LOG] [API] Checking server status...
[2025-10-26T18:44:50.460Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:44:50.451Z"
}
[2025-10-26T18:44:50.460Z] [LOG] [API] WebGPU available
[2025-10-26T18:44:50.466Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:44:50.466Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:44:50.466Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:44:50.480Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:45:50.418Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:45:50.418Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:45:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:45:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T18:45:50.432Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:45:50.423Z"
}
[2025-10-26T18:45:50.432Z] [LOG] [API] WebGPU available
[2025-10-26T18:45:50.453Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:45:50.453Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:45:50.454Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:45:50.460Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:46:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:46:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:46:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:46:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T18:46:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:46:50.394Z"
}
[2025-10-26T18:46:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T18:46:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:46:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:46:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:46:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:47:50.353Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:47:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:47:50.356Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:47:50.356Z] [LOG] [API] Checking server status...
[2025-10-26T18:47:50.367Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:47:50.357Z"
}
[2025-10-26T18:47:50.367Z] [LOG] [API] WebGPU available
[2025-10-26T18:47:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:47:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:47:50.384Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:47:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:48:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:48:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:48:50.403Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:48:50.403Z] [LOG] [API] Checking server status...
[2025-10-26T18:48:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:48:50.404Z"
}
[2025-10-26T18:48:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T18:48:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:48:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:48:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:48:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:49:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:49:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:49:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:49:50.361Z] [LOG] [API] Checking server status...
[2025-10-26T18:49:50.372Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:49:50.363Z"
}
[2025-10-26T18:49:50.372Z] [LOG] [API] WebGPU available
[2025-10-26T18:49:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:49:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:49:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:49:50.398Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:50:50.514Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:50:50.514Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:50:50.517Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:50:50.517Z] [LOG] [API] Checking server status...
[2025-10-26T18:50:50.528Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:50:50.518Z"
}
[2025-10-26T18:50:50.529Z] [LOG] [API] WebGPU available
[2025-10-26T18:50:50.531Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:50:50.531Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:50:50.531Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:50:50.549Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:51:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:51:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:51:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:51:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T18:51:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:51:50.366Z"
}
[2025-10-26T18:51:50.375Z] [LOG] [API] WebGPU available
[2025-10-26T18:51:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:51:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:51:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:51:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:52:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:52:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:52:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:52:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T18:52:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:52:50.390Z"
}
[2025-10-26T18:52:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T18:52:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:52:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:52:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:52:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:53:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:53:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:53:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:53:50.372Z] [LOG] [API] Checking server status...
[2025-10-26T18:53:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:53:50.374Z"
}
[2025-10-26T18:53:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T18:53:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:53:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:53:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:53:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:54:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:54:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:54:50.437Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:54:50.437Z] [LOG] [API] Checking server status...
[2025-10-26T18:54:50.448Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:54:50.438Z"
}
[2025-10-26T18:54:50.448Z] [LOG] [API] WebGPU available
[2025-10-26T18:54:50.454Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:54:50.454Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:54:50.454Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:54:50.467Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:55:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:55:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:55:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:55:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T18:55:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:55:50.377Z"
}
[2025-10-26T18:55:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T18:55:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:55:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:55:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:55:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:56:50.518Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:56:50.518Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:56:50.520Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:56:50.520Z] [LOG] [API] Checking server status...
[2025-10-26T18:56:50.529Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:56:50.521Z"
}
[2025-10-26T18:56:50.529Z] [LOG] [API] WebGPU available
[2025-10-26T18:56:50.533Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:56:50.533Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:56:50.533Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:56:50.552Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:57:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:57:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:57:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:57:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T18:57:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:57:50.370Z"
}
[2025-10-26T18:57:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T18:57:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:57:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:57:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:57:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:58:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:58:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:58:50.429Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:58:50.429Z] [LOG] [API] Checking server status...
[2025-10-26T18:58:50.438Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:58:50.430Z"
}
[2025-10-26T18:58:50.438Z] [LOG] [API] WebGPU available
[2025-10-26T18:58:50.445Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:58:50.445Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:58:50.445Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:58:50.458Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T18:59:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T18:59:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T18:59:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T18:59:50.372Z] [LOG] [API] Checking server status...
[2025-10-26T18:59:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T18:59:50.375Z"
}
[2025-10-26T18:59:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T18:59:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T18:59:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T18:59:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T18:59:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:00:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:00:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:00:50.457Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:00:50.458Z] [LOG] [API] Checking server status...
[2025-10-26T19:00:50.468Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:00:50.459Z"
}
[2025-10-26T19:00:50.468Z] [LOG] [API] WebGPU available
[2025-10-26T19:00:50.476Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:00:50.476Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:00:50.476Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:00:50.489Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:01:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:01:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:01:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:01:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T19:01:50.372Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:01:50.363Z"
}
[2025-10-26T19:01:50.372Z] [LOG] [API] WebGPU available
[2025-10-26T19:01:50.378Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:01:50.378Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:01:50.378Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:01:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:02:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:02:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:02:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:02:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T19:02:50.404Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:02:50.397Z"
}
[2025-10-26T19:02:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T19:02:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:02:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:02:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:02:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:03:50.533Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:03:50.533Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:03:50.606Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:03:50.606Z] [LOG] [API] Checking server status...
[2025-10-26T19:03:50.617Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:03:50.608Z"
}
[2025-10-26T19:03:50.617Z] [LOG] [API] WebGPU available
[2025-10-26T19:03:50.624Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:03:50.624Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:03:50.624Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:03:50.629Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:04:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:04:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:04:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:04:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T19:04:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:04:50.380Z"
}
[2025-10-26T19:04:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T19:04:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:04:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:04:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:04:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:05:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:05:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:05:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:05:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T19:05:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:05:50.401Z"
}
[2025-10-26T19:05:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T19:05:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:05:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:05:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:05:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:06:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:06:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:06:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:06:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T19:06:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:06:50.366Z"
}
[2025-10-26T19:06:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T19:06:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:06:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:06:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:06:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:07:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:07:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:07:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:07:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T19:07:50.420Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:07:50.411Z"
}
[2025-10-26T19:07:50.420Z] [LOG] [API] WebGPU available
[2025-10-26T19:07:50.426Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:07:50.426Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:07:50.426Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:07:50.463Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:08:50.550Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:08:50.550Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:08:50.552Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:08:50.552Z] [LOG] [API] Checking server status...
[2025-10-26T19:08:50.561Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:08:50.553Z"
}
[2025-10-26T19:08:50.561Z] [LOG] [API] WebGPU available
[2025-10-26T19:08:50.568Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:08:50.568Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:08:50.568Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:08:50.582Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:09:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:09:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:09:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:09:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T19:09:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:09:50.372Z"
}
[2025-10-26T19:09:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T19:09:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:09:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:09:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:09:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:10:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:10:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:10:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:10:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T19:10:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:10:50.387Z"
}
[2025-10-26T19:10:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T19:10:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:10:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:10:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:10:50.431Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:11:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:11:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:11:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:11:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T19:11:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:11:50.369Z"
}
[2025-10-26T19:11:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T19:11:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:11:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:11:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:11:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:12:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:12:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:12:50.428Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:12:50.428Z] [LOG] [API] Checking server status...
[2025-10-26T19:12:50.437Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:12:50.429Z"
}
[2025-10-26T19:12:50.437Z] [LOG] [API] WebGPU available
[2025-10-26T19:12:50.441Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:12:50.441Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:12:50.441Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:12:50.459Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:13:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:13:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:13:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:13:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T19:13:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:13:50.366Z"
}
[2025-10-26T19:13:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T19:13:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:13:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:13:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:13:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:14:50.508Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:14:50.508Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:14:50.511Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:14:50.511Z] [LOG] [API] Checking server status...
[2025-10-26T19:14:50.520Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:14:50.512Z"
}
[2025-10-26T19:14:50.520Z] [LOG] [API] WebGPU available
[2025-10-26T19:14:50.524Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:14:50.524Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:14:50.524Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:14:50.542Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:15:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:15:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:15:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:15:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T19:15:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:15:50.369Z"
}
[2025-10-26T19:15:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T19:15:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:15:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:15:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:15:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:16:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:16:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:16:50.436Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:16:50.436Z] [LOG] [API] Checking server status...
[2025-10-26T19:16:50.445Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:16:50.437Z"
}
[2025-10-26T19:16:50.445Z] [LOG] [API] WebGPU available
[2025-10-26T19:16:50.447Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:16:50.447Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:16:50.447Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:16:50.465Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:17:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:17:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:17:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:17:50.361Z] [LOG] [API] Checking server status...
[2025-10-26T19:17:50.370Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:17:50.362Z"
}
[2025-10-26T19:17:50.370Z] [LOG] [API] WebGPU available
[2025-10-26T19:17:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:17:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:17:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:17:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:18:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:18:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:18:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:18:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T19:18:50.413Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:18:50.403Z"
}
[2025-10-26T19:18:50.413Z] [LOG] [API] WebGPU available
[2025-10-26T19:18:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:18:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:18:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:18:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:19:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:19:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:19:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:19:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T19:19:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:19:50.377Z"
}
[2025-10-26T19:19:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T19:19:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:19:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:19:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:19:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:20:50.490Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:20:50.490Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:20:50.492Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:20:50.492Z] [LOG] [API] Checking server status...
[2025-10-26T19:20:50.501Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:20:50.493Z"
}
[2025-10-26T19:20:50.501Z] [LOG] [API] WebGPU available
[2025-10-26T19:20:50.504Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:20:50.504Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:20:50.504Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:20:50.522Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:21:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:21:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:21:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:21:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T19:21:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:21:50.363Z"
}
[2025-10-26T19:21:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T19:21:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:21:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:21:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:21:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:22:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:22:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:22:50.415Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:22:50.415Z] [LOG] [API] Checking server status...
[2025-10-26T19:22:50.426Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:22:50.417Z"
}
[2025-10-26T19:22:50.426Z] [LOG] [API] WebGPU available
[2025-10-26T19:22:50.452Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:22:50.452Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:22:50.452Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:22:50.472Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:23:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:23:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:23:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:23:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T19:23:50.370Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:23:50.363Z"
}
[2025-10-26T19:23:50.370Z] [LOG] [API] WebGPU available
[2025-10-26T19:23:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:23:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:23:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:23:50.396Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:24:50.539Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:24:50.540Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:24:50.581Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:24:50.581Z] [LOG] [API] Checking server status...
[2025-10-26T19:24:50.591Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:24:50.583Z"
}
[2025-10-26T19:24:50.591Z] [LOG] [API] WebGPU available
[2025-10-26T19:24:50.597Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:24:50.597Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:24:50.597Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:24:50.602Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:25:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:25:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:25:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:25:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T19:25:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:25:50.376Z"
}
[2025-10-26T19:25:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T19:25:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:25:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:25:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:25:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:26:50.485Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:26:50.487Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:26:50.549Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:26:50.549Z] [LOG] [API] Checking server status...
[2025-10-26T19:26:50.560Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:26:50.550Z"
}
[2025-10-26T19:26:50.560Z] [LOG] [API] WebGPU available
[2025-10-26T19:26:50.567Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:26:50.567Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:26:50.567Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:26:50.579Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:27:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:27:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:27:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:27:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T19:27:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:27:50.369Z"
}
[2025-10-26T19:27:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T19:27:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:27:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:27:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:27:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:28:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:28:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:28:50.382Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:28:50.382Z] [LOG] [API] Checking server status...
[2025-10-26T19:28:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:28:50.383Z"
}
[2025-10-26T19:28:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T19:28:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:28:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:28:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:28:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:29:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:29:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:29:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:29:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T19:29:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:29:50.366Z"
}
[2025-10-26T19:29:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T19:29:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:29:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:29:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:29:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:30:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:30:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:30:50.394Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:30:50.394Z] [LOG] [API] Checking server status...
[2025-10-26T19:30:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:30:50.395Z"
}
[2025-10-26T19:30:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T19:30:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:30:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:30:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:30:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:31:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:31:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:31:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:31:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T19:31:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:31:50.375Z"
}
[2025-10-26T19:31:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T19:31:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:31:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:31:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:31:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:32:50.464Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:32:50.464Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:32:50.466Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:32:50.466Z] [LOG] [API] Checking server status...
[2025-10-26T19:32:50.475Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:32:50.467Z"
}
[2025-10-26T19:32:50.475Z] [LOG] [API] WebGPU available
[2025-10-26T19:32:50.478Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:32:50.478Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:32:50.478Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:32:50.496Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:33:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:33:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:33:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:33:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T19:33:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:33:50.367Z"
}
[2025-10-26T19:33:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T19:33:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:33:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:33:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:33:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:34:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:34:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:34:50.425Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:34:50.425Z] [LOG] [API] Checking server status...
[2025-10-26T19:34:50.435Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:34:50.426Z"
}
[2025-10-26T19:34:50.435Z] [LOG] [API] WebGPU available
[2025-10-26T19:34:50.442Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:34:50.442Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:34:50.442Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:34:50.454Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:35:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:35:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:35:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:35:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T19:35:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:35:50.369Z"
}
[2025-10-26T19:35:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T19:35:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:35:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:35:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:35:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:36:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:36:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:36:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:36:50.399Z] [LOG] [API] Checking server status...
[2025-10-26T19:36:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:36:50.403Z"
}
[2025-10-26T19:36:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T19:36:50.417Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:36:50.417Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:36:50.417Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:36:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:37:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:37:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:37:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:37:50.361Z] [LOG] [API] Checking server status...
[2025-10-26T19:37:50.371Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:37:50.364Z"
}
[2025-10-26T19:37:50.371Z] [LOG] [API] WebGPU available
[2025-10-26T19:37:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:37:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:37:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:37:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:38:50.464Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:38:50.464Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:38:50.466Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:38:50.466Z] [LOG] [API] Checking server status...
[2025-10-26T19:38:50.475Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:38:50.467Z"
}
[2025-10-26T19:38:50.475Z] [LOG] [API] WebGPU available
[2025-10-26T19:38:50.478Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:38:50.478Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:38:50.478Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:38:50.497Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:39:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:39:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:39:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:39:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T19:39:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:39:50.369Z"
}
[2025-10-26T19:39:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T19:39:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:39:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:39:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:39:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:40:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:40:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:40:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:40:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T19:40:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:40:50.396Z"
}
[2025-10-26T19:40:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T19:40:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:40:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:40:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:40:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:41:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:41:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:41:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:41:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T19:41:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:41:50.371Z"
}
[2025-10-26T19:41:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T19:41:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:41:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:41:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:41:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:42:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:42:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:42:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:42:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T19:42:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:42:50.390Z"
}
[2025-10-26T19:42:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T19:42:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:42:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:42:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:42:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:43:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:43:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:43:50.363Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:43:50.363Z] [LOG] [API] Checking server status...
[2025-10-26T19:43:50.372Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:43:50.365Z"
}
[2025-10-26T19:43:50.372Z] [LOG] [API] WebGPU available
[2025-10-26T19:43:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:43:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:43:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:43:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:44:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:44:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:44:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:44:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T19:44:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:44:50.402Z"
}
[2025-10-26T19:44:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T19:44:50.423Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:44:50.423Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:44:50.423Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:44:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:45:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:45:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:45:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:45:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T19:45:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:45:50.379Z"
}
[2025-10-26T19:45:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T19:45:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:45:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:45:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:45:50.431Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:46:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:46:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:46:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:46:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T19:46:50.432Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:46:50.423Z"
}
[2025-10-26T19:46:50.432Z] [LOG] [API] WebGPU available
[2025-10-26T19:46:50.441Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:46:50.441Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:46:50.441Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:46:50.464Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:47:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:47:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:47:50.389Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:47:50.389Z] [LOG] [API] Checking server status...
[2025-10-26T19:47:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:47:50.391Z"
}
[2025-10-26T19:47:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T19:47:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:47:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:47:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:47:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:48:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:48:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:48:50.438Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:48:50.438Z] [LOG] [API] Checking server status...
[2025-10-26T19:48:50.447Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:48:50.439Z"
}
[2025-10-26T19:48:50.447Z] [LOG] [API] WebGPU available
[2025-10-26T19:48:50.454Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:48:50.454Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:48:50.454Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:48:50.469Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:49:50.418Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:49:50.418Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:49:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:49:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T19:49:50.431Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:49:50.423Z"
}
[2025-10-26T19:49:50.431Z] [LOG] [API] WebGPU available
[2025-10-26T19:49:50.449Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:49:50.449Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:49:50.449Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:49:50.463Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:50:50.440Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:50:50.440Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:50:50.443Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:50:50.443Z] [LOG] [API] Checking server status...
[2025-10-26T19:50:50.452Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:50:50.444Z"
}
[2025-10-26T19:50:50.452Z] [LOG] [API] WebGPU available
[2025-10-26T19:50:50.459Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:50:50.459Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:50:50.459Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:50:50.472Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:51:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:51:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:51:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:51:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T19:51:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:51:50.376Z"
}
[2025-10-26T19:51:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T19:51:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:51:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:51:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:51:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:52:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:52:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:52:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:52:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T19:52:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:52:50.387Z"
}
[2025-10-26T19:52:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T19:52:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:52:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:52:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:52:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:53:50.385Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:53:50.385Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:53:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:53:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T19:53:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:53:50.390Z"
}
[2025-10-26T19:53:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T19:53:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:53:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:53:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:53:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:54:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:54:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:54:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:54:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T19:54:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:54:50.405Z"
}
[2025-10-26T19:54:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T19:54:50.427Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:54:50.427Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:54:50.427Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:54:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:55:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:55:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:55:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:55:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T19:55:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:55:50.369Z"
}
[2025-10-26T19:55:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T19:55:50.393Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:55:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:55:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:55:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:56:50.411Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:56:50.411Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:56:50.440Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:56:50.440Z] [LOG] [API] Checking server status...
[2025-10-26T19:56:50.449Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:56:50.441Z"
}
[2025-10-26T19:56:50.449Z] [LOG] [API] WebGPU available
[2025-10-26T19:56:50.456Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:56:50.457Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:56:50.457Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:56:50.474Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:57:50.537Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:57:50.537Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:57:50.660Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:57:50.661Z] [LOG] [API] Checking server status...
[2025-10-26T19:57:50.670Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:57:50.662Z"
}
[2025-10-26T19:57:50.670Z] [LOG] [API] WebGPU available
[2025-10-26T19:57:50.675Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:57:50.676Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:57:50.676Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:57:50.680Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:58:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:58:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:58:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:58:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T19:58:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:58:50.376Z"
}
[2025-10-26T19:58:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T19:58:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:58:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:58:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:58:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T19:59:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T19:59:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T19:59:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T19:59:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T19:59:50.430Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T19:59:50.422Z"
}
[2025-10-26T19:59:50.430Z] [LOG] [API] WebGPU available
[2025-10-26T19:59:50.436Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T19:59:50.436Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T19:59:50.436Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T19:59:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:00:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:00:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:00:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:00:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T20:00:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:00:50.366Z"
}
[2025-10-26T20:00:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T20:00:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:00:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:00:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:00:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:01:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:01:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:01:50.466Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:01:50.466Z] [LOG] [API] Checking server status...
[2025-10-26T20:01:50.475Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:01:50.468Z"
}
[2025-10-26T20:01:50.475Z] [LOG] [API] WebGPU available
[2025-10-26T20:01:50.478Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:01:50.478Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:01:50.478Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:01:50.492Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:02:50.466Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:02:50.467Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:02:50.469Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:02:50.469Z] [LOG] [API] Checking server status...
[2025-10-26T20:02:50.480Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:02:50.470Z"
}
[2025-10-26T20:02:50.480Z] [LOG] [API] WebGPU available
[2025-10-26T20:02:50.482Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:02:50.482Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:02:50.482Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:02:50.499Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:03:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:03:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:03:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:03:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T20:03:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:03:50.376Z"
}
[2025-10-26T20:03:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T20:03:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:03:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:03:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:03:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:04:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:04:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:04:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:04:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T20:04:50.408Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:04:50.401Z"
}
[2025-10-26T20:04:50.408Z] [LOG] [API] WebGPU available
[2025-10-26T20:04:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:04:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:04:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:04:50.424Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:05:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:05:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:05:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:05:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T20:05:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:05:50.368Z"
}
[2025-10-26T20:05:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T20:05:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:05:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:05:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:05:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:06:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:06:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:06:50.423Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:06:50.423Z] [LOG] [API] Checking server status...
[2025-10-26T20:06:50.434Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:06:50.424Z"
}
[2025-10-26T20:06:50.434Z] [LOG] [API] WebGPU available
[2025-10-26T20:06:50.436Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:06:50.436Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:06:50.436Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:06:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:07:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:07:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:07:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:07:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T20:07:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:07:50.367Z"
}
[2025-10-26T20:07:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T20:07:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:07:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:07:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:07:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:08:50.542Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:08:50.542Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:08:50.545Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:08:50.545Z] [LOG] [API] Checking server status...
[2025-10-26T20:08:50.555Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:08:50.546Z"
}
[2025-10-26T20:08:50.555Z] [LOG] [API] WebGPU available
[2025-10-26T20:08:50.560Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:08:50.560Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:08:50.560Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:08:50.577Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:09:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:09:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:09:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:09:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T20:09:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:09:50.373Z"
}
[2025-10-26T20:09:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T20:09:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:09:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:09:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:09:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:10:50.544Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:10:50.544Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:10:50.579Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:10:50.579Z] [LOG] [API] Checking server status...
[2025-10-26T20:10:50.588Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:10:50.580Z"
}
[2025-10-26T20:10:50.588Z] [LOG] [API] WebGPU available
[2025-10-26T20:10:50.593Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:10:50.593Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:10:50.593Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:10:50.618Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:11:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:11:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:11:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:11:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T20:11:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:11:50.381Z"
}
[2025-10-26T20:11:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T20:11:50.407Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:11:50.407Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:11:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:11:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:12:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:12:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:12:50.469Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:12:50.469Z] [LOG] [API] Checking server status...
[2025-10-26T20:12:50.478Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:12:50.470Z"
}
[2025-10-26T20:12:50.478Z] [LOG] [API] WebGPU available
[2025-10-26T20:12:50.482Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:12:50.482Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:12:50.482Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:12:50.500Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:13:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:13:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:13:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:13:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T20:13:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:13:50.378Z"
}
[2025-10-26T20:13:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T20:13:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:13:50.409Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:13:50.409Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:13:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:14:50.526Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:14:50.526Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:14:50.529Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:14:50.529Z] [LOG] [API] Checking server status...
[2025-10-26T20:14:50.538Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:14:50.530Z"
}
[2025-10-26T20:14:50.538Z] [LOG] [API] WebGPU available
[2025-10-26T20:14:50.545Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:14:50.545Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:14:50.545Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:14:50.559Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:15:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:15:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:15:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:15:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T20:15:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:15:50.373Z"
}
[2025-10-26T20:15:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T20:15:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:15:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:15:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:15:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:16:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:16:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:16:50.428Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:16:50.428Z] [LOG] [API] Checking server status...
[2025-10-26T20:16:50.438Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:16:50.429Z"
}
[2025-10-26T20:16:50.438Z] [LOG] [API] WebGPU available
[2025-10-26T20:16:50.445Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:16:50.445Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:16:50.445Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:16:50.459Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:17:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:17:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:17:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:17:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T20:17:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:17:50.393Z"
}
[2025-10-26T20:17:50.401Z] [LOG] [API] WebGPU available
[2025-10-26T20:17:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:17:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:17:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:17:50.440Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:18:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:18:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:18:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:18:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T20:18:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:18:50.403Z"
}
[2025-10-26T20:18:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T20:18:50.424Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:18:50.424Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:18:50.424Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:18:50.441Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:19:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:19:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:19:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:19:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T20:19:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:19:50.372Z"
}
[2025-10-26T20:19:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T20:19:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:19:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:19:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:19:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:20:50.474Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:20:50.475Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:20:50.477Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:20:50.477Z] [LOG] [API] Checking server status...
[2025-10-26T20:20:50.486Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:20:50.478Z"
}
[2025-10-26T20:20:50.486Z] [LOG] [API] WebGPU available
[2025-10-26T20:20:50.489Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:20:50.489Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:20:50.489Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:20:50.507Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:21:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:21:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:21:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:21:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T20:21:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:21:50.373Z"
}
[2025-10-26T20:21:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T20:21:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:21:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:21:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:21:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:22:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:22:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:22:50.410Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:22:50.410Z] [LOG] [API] Checking server status...
[2025-10-26T20:22:50.419Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:22:50.411Z"
}
[2025-10-26T20:22:50.419Z] [LOG] [API] WebGPU available
[2025-10-26T20:22:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:22:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:22:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:22:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:23:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:23:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:23:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:23:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T20:23:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:23:50.372Z"
}
[2025-10-26T20:23:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T20:23:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:23:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:23:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:23:50.407Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:24:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:24:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:24:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:24:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T20:24:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:24:50.407Z"
}
[2025-10-26T20:24:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T20:24:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:24:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:24:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:24:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:25:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:25:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:25:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:25:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T20:25:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:25:50.369Z"
}
[2025-10-26T20:25:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T20:25:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:25:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:25:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:25:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:26:50.448Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:26:50.448Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:26:50.450Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:26:50.450Z] [LOG] [API] Checking server status...
[2025-10-26T20:26:50.459Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:26:50.452Z"
}
[2025-10-26T20:26:50.459Z] [LOG] [API] WebGPU available
[2025-10-26T20:26:50.463Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:26:50.463Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:26:50.463Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:26:50.481Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:27:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:27:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:27:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:27:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T20:27:50.377Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:27:50.369Z"
}
[2025-10-26T20:27:50.377Z] [LOG] [API] WebGPU available
[2025-10-26T20:27:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:27:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:27:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:27:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:28:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:28:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:28:50.417Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:28:50.417Z] [LOG] [API] Checking server status...
[2025-10-26T20:28:50.428Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:28:50.420Z"
}
[2025-10-26T20:28:50.428Z] [LOG] [API] WebGPU available
[2025-10-26T20:28:50.440Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:28:50.440Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:28:50.440Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:28:50.456Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:29:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:29:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:29:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:29:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T20:29:50.375Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:29:50.367Z"
}
[2025-10-26T20:29:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T20:29:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:29:50.393Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:29:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:29:50.403Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:30:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:30:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:30:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:30:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T20:30:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:30:50.388Z"
}
[2025-10-26T20:30:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T20:30:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:30:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:30:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:30:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:31:50.552Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:31:50.552Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:31:50.629Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:31:50.629Z] [LOG] [API] Checking server status...
[2025-10-26T20:31:50.639Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:31:50.630Z"
}
[2025-10-26T20:31:50.639Z] [LOG] [API] WebGPU available
[2025-10-26T20:31:50.646Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:31:50.646Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:31:50.646Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:31:50.653Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:32:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:32:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:32:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:32:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T20:32:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:32:50.369Z"
}
[2025-10-26T20:32:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T20:32:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:32:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:32:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:32:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:33:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:33:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:33:50.398Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:33:50.398Z] [LOG] [API] Checking server status...
[2025-10-26T20:33:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:33:50.400Z"
}
[2025-10-26T20:33:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T20:33:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:33:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:33:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:33:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:34:50.361Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:34:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:34:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:34:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T20:34:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:34:50.365Z"
}
[2025-10-26T20:34:50.374Z] [LOG] [API] WebGPU available
[2025-10-26T20:34:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:34:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:34:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:34:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:35:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:35:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:35:50.515Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:35:50.515Z] [LOG] [API] Checking server status...
[2025-10-26T20:35:50.524Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:35:50.516Z"
}
[2025-10-26T20:35:50.524Z] [LOG] [API] WebGPU available
[2025-10-26T20:35:50.528Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:35:50.528Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:35:50.528Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:35:50.544Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:36:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:36:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:36:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:36:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T20:36:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:36:50.374Z"
}
[2025-10-26T20:36:50.383Z] [LOG] [API] WebGPU available
[2025-10-26T20:36:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:36:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:36:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:36:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:37:50.412Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:37:50.412Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:37:50.444Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:37:50.444Z] [LOG] [API] Checking server status...
[2025-10-26T20:37:50.454Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:37:50.445Z"
}
[2025-10-26T20:37:50.455Z] [LOG] [API] WebGPU available
[2025-10-26T20:37:50.461Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:37:50.461Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:37:50.461Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:37:50.499Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:38:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:38:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:38:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:38:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T20:38:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:38:50.377Z"
}
[2025-10-26T20:38:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T20:38:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:38:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:38:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:38:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:39:50.501Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:39:50.501Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:39:50.503Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:39:50.503Z] [LOG] [API] Checking server status...
[2025-10-26T20:39:50.513Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:39:50.505Z"
}
[2025-10-26T20:39:50.513Z] [LOG] [API] WebGPU available
[2025-10-26T20:39:50.519Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:39:50.519Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:39:50.519Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:39:50.524Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:40:50.382Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:40:50.382Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:40:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:40:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T20:40:50.396Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:40:50.387Z"
}
[2025-10-26T20:40:50.396Z] [LOG] [API] WebGPU available
[2025-10-26T20:40:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:40:50.414Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:40:50.414Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:40:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:41:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:41:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:41:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:41:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T20:41:50.417Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:41:50.408Z"
}
[2025-10-26T20:41:50.417Z] [LOG] [API] WebGPU available
[2025-10-26T20:41:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:41:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:41:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:41:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:42:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:42:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:42:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:42:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T20:42:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:42:50.382Z"
}
[2025-10-26T20:42:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T20:42:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:42:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:42:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:42:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:43:50.350Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:43:50.350Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:43:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:43:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T20:43:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:43:50.379Z"
}
[2025-10-26T20:43:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T20:43:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:43:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:43:50.392Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:43:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:44:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:44:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:44:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:44:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T20:44:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:44:50.378Z"
}
[2025-10-26T20:44:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T20:44:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:44:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:44:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:44:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:45:50.445Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:45:50.445Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:45:50.447Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:45:50.447Z] [LOG] [API] Checking server status...
[2025-10-26T20:45:50.457Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:45:50.448Z"
}
[2025-10-26T20:45:50.457Z] [LOG] [API] WebGPU available
[2025-10-26T20:45:50.459Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:45:50.459Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:45:50.459Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:45:50.476Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:46:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:46:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:46:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:46:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T20:46:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:46:50.371Z"
}
[2025-10-26T20:46:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T20:46:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:46:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:46:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:46:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:47:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:47:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:47:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:47:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T20:47:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:47:50.396Z"
}
[2025-10-26T20:47:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T20:47:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:47:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:47:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:47:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:48:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:48:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:48:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:48:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T20:48:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:48:50.373Z"
}
[2025-10-26T20:48:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T20:48:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:48:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:48:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:48:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:49:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:49:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:49:50.418Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:49:50.418Z] [LOG] [API] Checking server status...
[2025-10-26T20:49:50.427Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:49:50.419Z"
}
[2025-10-26T20:49:50.427Z] [LOG] [API] WebGPU available
[2025-10-26T20:49:50.434Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:49:50.435Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:49:50.435Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:49:50.459Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:50:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:50:50.362Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:50:50.365Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:50:50.365Z] [LOG] [API] Checking server status...
[2025-10-26T20:50:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:50:50.368Z"
}
[2025-10-26T20:50:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T20:50:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:50:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:50:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:50:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:51:50.523Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:51:50.523Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:51:50.525Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:51:50.525Z] [LOG] [API] Checking server status...
[2025-10-26T20:51:50.535Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:51:50.527Z"
}
[2025-10-26T20:51:50.535Z] [LOG] [API] WebGPU available
[2025-10-26T20:51:50.541Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:51:50.541Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:51:50.541Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:51:50.546Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:52:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:52:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:52:50.376Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:52:50.376Z] [LOG] [API] Checking server status...
[2025-10-26T20:52:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:52:50.378Z"
}
[2025-10-26T20:52:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T20:52:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:52:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:52:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:52:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:53:50.527Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:53:50.527Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:53:50.529Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:53:50.529Z] [LOG] [API] Checking server status...
[2025-10-26T20:53:50.540Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:53:50.531Z"
}
[2025-10-26T20:53:50.540Z] [LOG] [API] WebGPU available
[2025-10-26T20:53:50.547Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:53:50.547Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:53:50.547Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:53:50.555Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:54:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:54:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:54:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:54:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T20:54:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:54:50.384Z"
}
[2025-10-26T20:54:50.390Z] [LOG] [API] WebGPU available
[2025-10-26T20:54:50.394Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:54:50.394Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:54:50.394Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:54:50.431Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:55:50.428Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:55:50.429Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:55:50.499Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:55:50.499Z] [LOG] [API] Checking server status...
[2025-10-26T20:55:50.510Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:55:50.500Z"
}
[2025-10-26T20:55:50.510Z] [LOG] [API] WebGPU available
[2025-10-26T20:55:50.516Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:55:50.516Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:55:50.516Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:55:50.521Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:56:50.505Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:56:50.505Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:56:50.508Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:56:50.508Z] [LOG] [API] Checking server status...
[2025-10-26T20:56:50.518Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:56:50.510Z"
}
[2025-10-26T20:56:50.519Z] [LOG] [API] WebGPU available
[2025-10-26T20:56:50.535Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:56:50.535Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:56:50.535Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:56:50.558Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:57:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:57:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:57:50.380Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:57:50.380Z] [LOG] [API] Checking server status...
[2025-10-26T20:57:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:57:50.382Z"
}
[2025-10-26T20:57:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T20:57:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:57:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:57:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:57:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:58:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:58:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:58:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:58:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T20:58:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:58:50.416Z"
}
[2025-10-26T20:58:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T20:58:50.431Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:58:50.431Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:58:50.431Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:58:50.456Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T20:59:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T20:59:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T20:59:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T20:59:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T20:59:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T20:59:50.370Z"
}
[2025-10-26T20:59:50.380Z] [LOG] [API] WebGPU available
[2025-10-26T20:59:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T20:59:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T20:59:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T20:59:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:00:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:00:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:00:50.434Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:00:50.434Z] [LOG] [API] Checking server status...
[2025-10-26T21:00:50.444Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:00:50.436Z"
}
[2025-10-26T21:00:50.444Z] [LOG] [API] WebGPU available
[2025-10-26T21:00:50.451Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:00:50.451Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:00:50.451Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:00:50.460Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:01:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:01:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:01:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:01:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T21:01:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:01:50.373Z"
}
[2025-10-26T21:01:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T21:01:50.384Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:01:50.384Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:01:50.384Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:01:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:02:50.415Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:02:50.415Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:02:50.504Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:02:50.504Z] [LOG] [API] Checking server status...
[2025-10-26T21:02:50.513Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:02:50.506Z"
}
[2025-10-26T21:02:50.513Z] [LOG] [API] WebGPU available
[2025-10-26T21:02:50.521Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:02:50.521Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:02:50.521Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:02:50.540Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:03:50.410Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:03:50.410Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:03:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:03:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T21:03:50.423Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:03:50.414Z"
}
[2025-10-26T21:03:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T21:03:50.440Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:03:50.440Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:03:50.440Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:03:50.453Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:04:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:04:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:04:50.745Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:04:50.746Z] [LOG] [API] Checking server status...
[2025-10-26T21:04:50.757Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:04:50.747Z"
}
[2025-10-26T21:04:50.757Z] [LOG] [API] WebGPU available
[2025-10-26T21:04:50.767Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:04:50.767Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:04:50.767Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:04:50.785Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:05:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:05:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:05:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:05:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T21:05:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:05:50.377Z"
}
[2025-10-26T21:05:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T21:05:50.403Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:05:50.403Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:05:50.403Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:05:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:06:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:06:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:06:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:06:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T21:06:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:06:50.405Z"
}
[2025-10-26T21:06:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T21:06:50.420Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:06:50.420Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:06:50.420Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:06:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:07:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:07:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:07:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:07:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T21:07:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:07:50.375Z"
}
[2025-10-26T21:07:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T21:07:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:07:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:07:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:07:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:08:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:08:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:08:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:08:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T21:08:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:08:50.397Z"
}
[2025-10-26T21:08:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T21:08:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:08:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:08:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:08:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:09:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:09:50.375Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:09:50.378Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:09:50.378Z] [LOG] [API] Checking server status...
[2025-10-26T21:09:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:09:50.380Z"
}
[2025-10-26T21:09:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T21:09:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:09:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:09:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:09:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:10:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:10:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:10:50.392Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:10:50.392Z] [LOG] [API] Checking server status...
[2025-10-26T21:10:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:10:50.393Z"
}
[2025-10-26T21:10:50.402Z] [LOG] [API] WebGPU available
[2025-10-26T21:10:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:10:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:10:50.407Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:10:50.418Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:11:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:11:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:11:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:11:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T21:11:50.379Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:11:50.371Z"
}
[2025-10-26T21:11:50.379Z] [LOG] [API] WebGPU available
[2025-10-26T21:11:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:11:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:11:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:11:50.417Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:12:50.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:12:50.378Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:12:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:12:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T21:12:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:12:50.403Z"
}
[2025-10-26T21:12:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T21:12:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:12:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:12:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:12:50.444Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:13:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:13:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:13:50.375Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:13:50.375Z] [LOG] [API] Checking server status...
[2025-10-26T21:13:50.386Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:13:50.376Z"
}
[2025-10-26T21:13:50.386Z] [LOG] [API] WebGPU available
[2025-10-26T21:13:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:13:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:13:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:13:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:14:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:14:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:14:50.387Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:14:50.387Z] [LOG] [API] Checking server status...
[2025-10-26T21:14:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:14:50.388Z"
}
[2025-10-26T21:14:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T21:14:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:14:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:14:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:14:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:15:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:15:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:15:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:15:50.372Z] [LOG] [API] Checking server status...
[2025-10-26T21:15:50.383Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:15:50.374Z"
}
[2025-10-26T21:15:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T21:15:50.401Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:15:50.401Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:15:50.401Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:15:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:16:50.421Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:16:50.421Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:16:50.509Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:16:50.509Z] [LOG] [API] Checking server status...
[2025-10-26T21:16:50.519Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:16:50.510Z"
}
[2025-10-26T21:16:50.520Z] [LOG] [API] WebGPU available
[2025-10-26T21:16:50.533Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:16:50.533Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:16:50.533Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:16:50.558Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:17:50.375Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:17:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:17:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:17:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T21:17:50.388Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:17:50.382Z"
}
[2025-10-26T21:17:50.388Z] [LOG] [API] WebGPU available
[2025-10-26T21:17:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:17:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:17:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:17:50.428Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:18:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:18:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:18:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:18:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T21:18:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:18:50.399Z"
}
[2025-10-26T21:18:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T21:18:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:18:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:18:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:18:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:19:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:19:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:19:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:19:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T21:19:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:19:50.373Z"
}
[2025-10-26T21:19:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T21:19:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:19:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:19:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:19:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:20:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:20:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:20:50.422Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:20:50.422Z] [LOG] [API] Checking server status...
[2025-10-26T21:20:50.433Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:20:50.423Z"
}
[2025-10-26T21:20:50.433Z] [LOG] [API] WebGPU available
[2025-10-26T21:20:50.438Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:20:50.438Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:20:50.438Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:20:50.472Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:21:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:21:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:21:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:21:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T21:21:50.394Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:21:50.386Z"
}
[2025-10-26T21:21:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T21:21:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:21:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:21:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:21:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:22:50.518Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:22:50.518Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:22:50.521Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:22:50.521Z] [LOG] [API] Checking server status...
[2025-10-26T21:22:50.530Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:22:50.522Z"
}
[2025-10-26T21:22:50.530Z] [LOG] [API] WebGPU available
[2025-10-26T21:22:50.539Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:22:50.539Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:22:50.539Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:22:50.553Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:23:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:23:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:23:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:23:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T21:23:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:23:50.372Z"
}
[2025-10-26T21:23:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T21:23:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:23:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:23:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:23:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:24:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:24:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:24:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:24:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T21:24:50.410Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:24:50.402Z"
}
[2025-10-26T21:24:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T21:24:50.415Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:24:50.415Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:24:50.415Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:24:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:25:50.371Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:25:50.371Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:25:50.374Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:25:50.374Z] [LOG] [API] Checking server status...
[2025-10-26T21:25:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:25:50.377Z"
}
[2025-10-26T21:25:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T21:25:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:25:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:25:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:25:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:26:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:26:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:26:50.390Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:26:50.390Z] [LOG] [API] Checking server status...
[2025-10-26T21:26:50.402Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:26:50.391Z"
}
[2025-10-26T21:26:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T21:26:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:26:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:26:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:26:50.451Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:27:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:27:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:27:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:27:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T21:27:50.380Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:27:50.372Z"
}
[2025-10-26T21:27:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T21:27:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:27:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:27:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:27:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:28:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:28:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:28:50.405Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:28:50.405Z] [LOG] [API] Checking server status...
[2025-10-26T21:28:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:28:50.406Z"
}
[2025-10-26T21:28:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T21:28:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:28:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:28:50.418Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:28:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:29:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:29:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:29:50.372Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:29:50.372Z] [LOG] [API] Checking server status...
[2025-10-26T21:29:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:29:50.374Z"
}
[2025-10-26T21:29:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T21:29:50.399Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:29:50.399Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:29:50.399Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:29:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:30:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:30:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:30:50.391Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:30:50.391Z] [LOG] [API] Checking server status...
[2025-10-26T21:30:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:30:50.392Z"
}
[2025-10-26T21:30:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T21:30:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:30:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:30:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:30:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:31:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:31:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:31:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:31:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T21:31:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:31:50.369Z"
}
[2025-10-26T21:31:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T21:31:50.381Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:31:50.381Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:31:50.381Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:31:50.415Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:32:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:32:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:32:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:32:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T21:32:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:32:50.392Z"
}
[2025-10-26T21:32:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T21:32:50.408Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:32:50.408Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:32:50.408Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:32:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:33:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:33:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:33:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:33:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T21:33:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:33:50.365Z"
}
[2025-10-26T21:33:50.373Z] [LOG] [API] WebGPU available
[2025-10-26T21:33:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:33:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:33:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:33:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:34:50.423Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:34:50.423Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:34:50.455Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:34:50.455Z] [LOG] [API] Checking server status...
[2025-10-26T21:34:50.467Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:34:50.457Z"
}
[2025-10-26T21:34:50.467Z] [LOG] [API] WebGPU available
[2025-10-26T21:34:50.473Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:34:50.473Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:34:50.473Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:34:50.479Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:35:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:35:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:35:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:35:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T21:35:50.384Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:35:50.375Z"
}
[2025-10-26T21:35:50.384Z] [LOG] [API] WebGPU available
[2025-10-26T21:35:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:35:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:35:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:35:50.423Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:36:50.370Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:36:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:36:50.400Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:36:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T21:36:50.409Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:36:50.401Z"
}
[2025-10-26T21:36:50.409Z] [LOG] [API] WebGPU available
[2025-10-26T21:36:50.412Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:36:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:36:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:36:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:37:50.374Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:37:50.374Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:37:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:37:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T21:37:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:37:50.379Z"
}
[2025-10-26T21:37:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T21:37:50.405Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:37:50.405Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:37:50.405Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:37:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:38:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:38:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:38:50.404Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:38:50.404Z] [LOG] [API] Checking server status...
[2025-10-26T21:38:50.414Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:38:50.407Z"
}
[2025-10-26T21:38:50.414Z] [LOG] [API] WebGPU available
[2025-10-26T21:38:50.422Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:38:50.422Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:38:50.422Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:38:50.445Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:39:50.381Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:39:50.381Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:39:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:39:50.385Z] [LOG] [API] Checking server status...
[2025-10-26T21:39:50.398Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:39:50.387Z"
}
[2025-10-26T21:39:50.398Z] [LOG] [API] WebGPU available
[2025-10-26T21:39:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:39:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:39:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:39:50.429Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:40:50.506Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:40:50.506Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:40:50.508Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:40:50.508Z] [LOG] [API] Checking server status...
[2025-10-26T21:40:50.517Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:40:50.509Z"
}
[2025-10-26T21:40:50.517Z] [LOG] [API] WebGPU available
[2025-10-26T21:40:50.524Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:40:50.524Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:40:50.524Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:40:50.539Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:41:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:41:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:41:50.363Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:41:50.363Z] [LOG] [API] Checking server status...
[2025-10-26T21:41:50.374Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:41:50.364Z"
}
[2025-10-26T21:41:50.375Z] [LOG] [API] WebGPU available
[2025-10-26T21:41:50.390Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:41:50.390Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:41:50.390Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:41:50.412Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:42:50.364Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:42:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:42:50.435Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:42:50.435Z] [LOG] [API] Checking server status...
[2025-10-26T21:42:50.443Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:42:50.436Z"
}
[2025-10-26T21:42:50.443Z] [LOG] [API] WebGPU available
[2025-10-26T21:42:50.447Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:42:50.447Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:42:50.447Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:42:50.464Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:43:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:43:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:43:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:43:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T21:43:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:43:50.372Z"
}
[2025-10-26T21:43:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T21:43:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:43:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:43:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:43:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:44:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:44:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:44:50.397Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:44:50.397Z] [LOG] [API] Checking server status...
[2025-10-26T21:44:50.406Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:44:50.398Z"
}
[2025-10-26T21:44:50.406Z] [LOG] [API] WebGPU available
[2025-10-26T21:44:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:44:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:44:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:44:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:45:50.289Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:45:50.289Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:45:50.292Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:45:50.292Z] [LOG] [API] Checking server status...
[2025-10-26T21:45:50.302Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:45:50.294Z"
}
[2025-10-26T21:45:50.302Z] [LOG] [API] WebGPU available
[2025-10-26T21:45:50.321Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:45:50.321Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:45:50.322Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:45:50.343Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:46:50.465Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:46:50.466Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:46:50.468Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:46:50.468Z] [LOG] [API] Checking server status...
[2025-10-26T21:46:50.478Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:46:50.469Z"
}
[2025-10-26T21:46:50.478Z] [LOG] [API] WebGPU available
[2025-10-26T21:46:50.481Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:46:50.481Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:46:50.481Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:46:50.501Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:47:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:47:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:47:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:47:50.361Z] [LOG] [API] Checking server status...
[2025-10-26T21:47:50.371Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:47:50.363Z"
}
[2025-10-26T21:47:50.371Z] [LOG] [API] WebGPU available
[2025-10-26T21:47:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:47:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:47:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:47:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:48:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:48:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:48:50.412Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:48:50.412Z] [LOG] [API] Checking server status...
[2025-10-26T21:48:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:48:50.417Z"
}
[2025-10-26T21:48:50.423Z] [LOG] [API] WebGPU available
[2025-10-26T21:48:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:48:50.429Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:48:50.429Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:48:50.456Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:49:33.550Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:49:33.551Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:49:33.558Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:49:33.558Z] [LOG] [API] Checking server status...
[2025-10-26T21:49:33.564Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:49:33.559Z"
}
[2025-10-26T21:49:33.564Z] [LOG] [API] WebGPU available
[2025-10-26T21:49:33.572Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:49:33.572Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:49:33.572Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:49:33.579Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:49:39.135Z] [INFO] {"timestamp":"2025-10-26T21:49:39.135Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T21:49:39.135Z] [INFO] {"timestamp":"2025-10-26T21:49:39.135Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T21:49:39.136Z] [INFO] {"timestamp":"2025-10-26T21:49:39.136Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T21:49:39.136Z] [INFO] {"timestamp":"2025-10-26T21:49:39.136Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T21:49:39.137Z] [INFO] {"timestamp":"2025-10-26T21:49:39.137Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-26T21:49:39.137Z] [INFO] {"timestamp":"2025-10-26T21:49:39.137Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-26T21:49:39.137Z] [INFO] {"timestamp":"2025-10-26T21:49:39.137Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-26T21:49:39.137Z] [INFO] {"timestamp":"2025-10-26T21:49:39.137Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-26T21:49:39.138Z] [INFO] {"timestamp":"2025-10-26T21:49:39.138Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-26T21:49:39.138Z] [INFO] {"timestamp":"2025-10-26T21:49:39.138Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-26T21:49:39.139Z] [INFO] {"timestamp":"2025-10-26T21:49:39.139Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-26T21:49:39.139Z] [INFO] {"timestamp":"2025-10-26T21:49:39.139Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-26T21:49:39.140Z] [INFO] {"timestamp":"2025-10-26T21:49:39.140Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-26T21:49:39.140Z] [INFO] {"timestamp":"2025-10-26T21:49:39.140Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-26T21:49:39.141Z] [INFO] {"timestamp":"2025-10-26T21:49:39.141Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-26T21:49:39.141Z] [INFO] {"timestamp":"2025-10-26T21:49:39.141Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-26T21:49:39.142Z] [INFO] {"timestamp":"2025-10-26T21:49:39.142Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-26T21:49:39.142Z] [INFO] {"timestamp":"2025-10-26T21:49:39.142Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-26T21:49:39.143Z] [INFO] {"timestamp":"2025-10-26T21:49:39.143Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-26T21:49:39.143Z] [INFO] {"timestamp":"2025-10-26T21:49:39.143Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-26T21:49:39.143Z] [INFO] {"timestamp":"2025-10-26T21:49:39.143Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-26T21:49:39.143Z] [INFO] {"timestamp":"2025-10-26T21:49:39.143Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-26T21:49:39.144Z] [INFO] {"timestamp":"2025-10-26T21:49:39.144Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-26T21:49:39.144Z] [INFO] {"timestamp":"2025-10-26T21:49:39.144Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-26T21:49:39.145Z] [INFO] {"timestamp":"2025-10-26T21:49:39.145Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-26T21:49:39.145Z] [INFO] {"timestamp":"2025-10-26T21:49:39.145Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-26T21:49:39.145Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-26T21:49:39.146Z] [ERROR] {"timestamp":"2025-10-26T21:49:39.145Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T21:49:39.146Z] [WARN] {"timestamp":"2025-10-26T21:49:39.146Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-26T21:49:39.146Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-26T21:49:39.146Z] [ERROR] {"timestamp":"2025-10-26T21:49:39.146Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T21:49:39.146Z] [WARN] {"timestamp":"2025-10-26T21:49:39.146Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-26T21:49:39.146Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-26T21:49:39.146Z] [ERROR] {"timestamp":"2025-10-26T21:49:39.146Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T21:49:39.146Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:49:39.146Z] [WARN] {"timestamp":"2025-10-26T21:49:39.146Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-26T21:49:39.147Z] [INFO] {"timestamp":"2025-10-26T21:49:39.147Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-26T21:49:39.147Z] [INFO] {"timestamp":"2025-10-26T21:49:39.147Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-26T21:49:39.148Z] [INFO] {"timestamp":"2025-10-26T21:49:39.148Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-26T21:49:39.148Z] [INFO] {"timestamp":"2025-10-26T21:49:39.148Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-26T21:49:39.149Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T21:49:39.149Z] [INFO] {"timestamp":"2025-10-26T21:49:39.149Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T21:49:39.149Z] [INFO] {"timestamp":"2025-10-26T21:49:39.149Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T21:49:39.150Z] [INFO] {"timestamp":"2025-10-26T21:49:39.150Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T21:49:39.150Z] [INFO] {"timestamp":"2025-10-26T21:49:39.150Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T21:49:39.151Z] [INFO] {"timestamp":"2025-10-26T21:49:39.151Z","level":"INFO","message":"[DIContainer] Registered module: GitVFS","details":{}}
[2025-10-26T21:49:39.151Z] [INFO] {"timestamp":"2025-10-26T21:49:39.151Z","level":"INFO","message":"[CoreLogic] Registered module: GitVFS from /upgrades/git-vfs.js","details":{}}
[2025-10-26T21:49:39.152Z] [INFO] {"timestamp":"2025-10-26T21:49:39.152Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T21:49:39.152Z] [INFO] {"timestamp":"2025-10-26T21:49:39.152Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T21:49:39.153Z] [INFO] {"timestamp":"2025-10-26T21:49:39.153Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-26T21:49:39.153Z] [INFO] {"timestamp":"2025-10-26T21:49:39.153Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-26T21:49:39.154Z] [LOG] [CoreLogic] Loaded module from /upgrades/diff-viewer-ui.js: NO_METADATA
[2025-10-26T21:49:39.154Z] [ERROR] {"timestamp":"2025-10-26T21:49:39.154Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/diff-viewer-ui.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T21:49:39.154Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:49:39.154Z] [WARN] {"timestamp":"2025-10-26T21:49:39.154Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/diff-viewer-ui.js missing metadata. Module:","details":{}}
[2025-10-26T21:49:39.155Z] [INFO] {"timestamp":"2025-10-26T21:49:39.155Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCSwarm","details":{}}
[2025-10-26T21:49:39.155Z] [INFO] {"timestamp":"2025-10-26T21:49:39.155Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCSwarm from /upgrades/webrtc-swarm.js","details":{}}
[2025-10-26T21:49:39.156Z] [INFO] {"timestamp":"2025-10-26T21:49:39.156Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCCoordinator","details":{}}
[2025-10-26T21:49:39.156Z] [INFO] {"timestamp":"2025-10-26T21:49:39.156Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCCoordinator from /upgrades/webrtc-coordinator.js","details":{}}
[2025-10-26T21:49:39.157Z] [INFO] {"timestamp":"2025-10-26T21:49:39.157Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T21:49:39.157Z] [INFO] {"timestamp":"2025-10-26T21:49:39.157Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T21:49:39.158Z] [INFO] {"timestamp":"2025-10-26T21:49:39.158Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T21:49:39.158Z] [INFO] {"timestamp":"2025-10-26T21:49:39.158Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T21:49:39.159Z] [INFO] {"timestamp":"2025-10-26T21:49:39.159Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T21:49:39.159Z] [INFO] {"timestamp":"2025-10-26T21:49:39.159Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T21:49:39.160Z] [INFO] {"timestamp":"2025-10-26T21:49:39.160Z","level":"INFO","message":"[DIContainer] Registered module: PeerReviewConsensus","details":{}}
[2025-10-26T21:49:39.160Z] [INFO] {"timestamp":"2025-10-26T21:49:39.160Z","level":"INFO","message":"[CoreLogic] Registered module: PeerReviewConsensus from /upgrades/peer-review-consensus.js","details":{}}
[2025-10-26T21:49:39.160Z] [INFO] {"timestamp":"2025-10-26T21:49:39.160Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T21:49:39.161Z] [INFO] {"timestamp":"2025-10-26T21:49:39.161Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T21:49:39.162Z] [INFO] {"timestamp":"2025-10-26T21:49:39.162Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T21:49:39.163Z] [INFO] {"timestamp":"2025-10-26T21:49:39.163Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-26T21:49:39.163Z] [INFO] {"timestamp":"2025-10-26T21:49:39.163Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-26T21:49:39.163Z] [WARN] {"timestamp":"2025-10-26T21:49:39.163Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-26T21:49:39.164Z] [INFO] {"timestamp":"2025-10-26T21:49:39.163Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-26T21:49:39.164Z] [INFO] {"timestamp":"2025-10-26T21:49:39.164Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-26T21:49:39.164Z] [WARN] {"timestamp":"2025-10-26T21:49:39.164Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-26T21:49:39.164Z] [INFO] {"timestamp":"2025-10-26T21:49:39.164Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-26T21:49:39.165Z] [WARN] {"timestamp":"2025-10-26T21:49:39.165Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-26T21:49:39.165Z] [INFO] {"timestamp":"2025-10-26T21:49:39.165Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-26T21:49:39.165Z] [INFO] {"timestamp":"2025-10-26T21:49:39.165Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-26T21:49:39.165Z] [INFO] {"timestamp":"2025-10-26T21:49:39.165Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-26T21:49:39.168Z] [INFO] {"timestamp":"2025-10-26T21:49:39.168Z","level":"INFO","message":"[ReflectionStore] Initializing reflection persistence","details":{}}
[2025-10-26T21:49:39.169Z] [INFO] {"timestamp":"2025-10-26T21:49:39.169Z","level":"INFO","message":"[ReflectionStore] Database opened successfully","details":{}}
[2025-10-26T21:49:39.169Z] [INFO] {"timestamp":"2025-10-26T21:49:39.169Z","level":"INFO","message":"[SelfTester] Initialized","details":{}}
[2025-10-26T21:49:39.169Z] [INFO] {"timestamp":"2025-10-26T21:49:39.169Z","level":"INFO","message":"[BrowserAPIs] Initializing web API integration...","details":{}}
[2025-10-26T21:49:39.170Z] [INFO] {"timestamp":"2025-10-26T21:49:39.170Z","level":"INFO","message":"[BrowserAPIs] Capabilities detected:","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-26T21:49:39.170Z] [INFO] {"timestamp":"2025-10-26T21:49:39.170Z","level":"INFO","message":"[EventBus] Emitting event: browser-apis:initialized","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-26T21:49:39.171Z] [INFO] {"timestamp":"2025-10-26T21:49:39.171Z","level":"INFO","message":"[GitVFS] Initialized successfully","details":{}}
[2025-10-26T21:49:39.172Z] [INFO] {"timestamp":"2025-10-26T21:49:39.172Z","level":"INFO","message":"[WebRTCSwarm] Initializing swarm module","details":{}}
[2025-10-26T21:49:39.172Z] [INFO] {"timestamp":"2025-10-26T21:49:39.172Z","level":"INFO","message":"[WebRTCSwarm] Local peer ID: reploid-id_1761515379172_jymeb9rw9","details":{}}
[2025-10-26T21:49:39.172Z] [INFO] {"timestamp":"2025-10-26T21:49:39.172Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T21:49:39.172Z] [INFO] {"timestamp":"2025-10-26T21:49:39.172Z","level":"INFO","message":"[WebRTCSwarm] Swarm initialized","details":{}}
[2025-10-26T21:49:39.172Z] [INFO] {"timestamp":"2025-10-26T21:49:39.172Z","level":"INFO","message":"[SwarmOrch] Initializing swarm orchestrator","details":{}}
[2025-10-26T21:49:39.172Z] [WARN] {"timestamp":"2025-10-26T21:49:39.172Z","level":"WARN","message":"[WebRTCSwarm] Cannot send signaling message: not connected","details":{}}
[2025-10-26T21:49:39.172Z] [INFO] {"timestamp":"2025-10-26T21:49:39.172Z","level":"INFO","message":"[SwarmOrch] Swarm orchestrator initialized","details":{"capabilities":["code-generation","file-management"]}}
[2025-10-26T21:49:39.174Z] [INFO] {"timestamp":"2025-10-26T21:49:39.174Z","level":"INFO","message":"[PyodideRuntime] Initializing Pyodide runtime...","details":{}}
[2025-10-26T21:49:39.174Z] [INFO] {"timestamp":"2025-10-26T21:49:39.174Z","level":"INFO","message":"[PyodideRuntime] Worker created","details":{}}
[2025-10-26T21:49:39.175Z] [ERROR] {"timestamp":"2025-10-26T21:49:39.175Z","level":"ERROR","message":"[WebRTCSwarm] Signaling WebSocket error:","details":{"isTrusted":true}}
[2025-10-26T21:49:39.175Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:49:39.176Z] [WARN] {"timestamp":"2025-10-26T21:49:39.176Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-26T21:49:40.133Z] [INFO] {"timestamp":"2025-10-26T21:49:40.133Z","level":"INFO","message":"[PyodideRuntime] Pyodide initialized","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-26T21:49:40.133Z] [INFO] {"timestamp":"2025-10-26T21:49:40.133Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:ready","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-26T21:49:40.134Z] [INFO] {"timestamp":"2025-10-26T21:49:40.134Z","level":"INFO","message":"[PyodideRuntime] Pyodide runtime ready","details":{}}
[2025-10-26T21:49:40.134Z] [INFO] {"timestamp":"2025-10-26T21:49:40.134Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:initialized","details":{"ready":true}}
[2025-10-26T21:49:40.136Z] [INFO] {"timestamp":"2025-10-26T21:49:40.136Z","level":"INFO","message":"Dashboard UI Manager (Event-Driven) taking control of DOM...","details":{}}
[2025-10-26T21:49:40.145Z] [INFO] {"timestamp":"2025-10-26T21:49:40.145Z","level":"INFO","message":"[UIManager] Initializing modular panel support...","details":{}}
[2025-10-26T21:49:40.145Z] [INFO] {"timestamp":"2025-10-26T21:49:40.145Z","level":"INFO","message":"[UIManager] Modular panel initialization complete","details":{}}
[2025-10-26T21:49:40.145Z] [INFO] {"timestamp":"2025-10-26T21:49:40.145Z","level":"INFO","message":"Dashboard UI Initialized. Listening for events.","details":{}}
[2025-10-26T21:49:40.145Z] [LOG] [CoreLogic] Attempting to initialize DiffViewerUI...
[2025-10-26T21:49:40.145Z] [LOG] [CoreLogic] Resolving DiffViewerUI from container...
[2025-10-26T21:49:40.145Z] [ERROR] [CoreLogic] DiffViewerUI initialization error: Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:681:33)), <anonymous>:61:23)
    at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:681:33), <anonymous>:317:46)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:701:9)
[2025-10-26T21:49:40.146Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:49:40.146Z] [WARN] {"timestamp":"2025-10-26T21:49:40.146Z","level":"WARN","message":"[CoreLogic] DiffViewerUI initialization failed:","details":"[DIContainer] Service not found: DiffViewerUI\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-26T21:49:40.146Z] [INFO] {"timestamp":"2025-10-26T21:49:40.146Z","level":"INFO","message":"[CoreLogic] Agent initialization complete. System is operational.","details":{}}
[2025-10-26T21:49:40.146Z] [INFO] {"timestamp":"2025-10-26T21:49:40.146Z","level":"INFO","message":"[CoreLogic] Creating genesis snapshot of initial boot state...","details":{}}
[2025-10-26T21:49:40.146Z] [WARN] {"timestamp":"2025-10-26T21:49:40.146Z","level":"WARN","message":"[CoreLogic] Failed to create genesis snapshot (non-fatal):","details":"[DIContainer] Service not found: GenesisSnapshot\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-26T21:49:40.146Z] [INFO] {"timestamp":"2025-10-26T21:49:40.146Z","level":"INFO","message":"[CoreLogic] Starting agent with initial goal:","details":"Analyze your own inefficiency patterns and improve yourself"}
[2025-10-26T21:49:40.146Z] [INFO] {"timestamp":"2025-10-26T21:49:40.146Z","level":"INFO","message":"[UI] updateGoal called with:","details":{"text":"Analyze your own inefficiency patterns and improve yourself","hasGoalTextRef":true}}
[2025-10-26T21:49:40.146Z] [INFO] {"timestamp":"2025-10-26T21:49:40.146Z","level":"INFO","message":"[UI] Goal text element updated successfully","details":{}}
[2025-10-26T21:49:40.167Z] [INFO] {"timestamp":"2025-10-26T21:49:40.167Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761515380150_54ea23f56a00c41c/session.json (SHA: 878008f)","details":{}}
[2025-10-26T21:49:40.167Z] [INFO] {"timestamp":"2025-10-26T21:49:40.167Z","level":"INFO","message":"[SessionManager] Created new session: session_1761515380150_54ea23f56a00c41c","details":{}}
[2025-10-26T21:49:40.172Z] [INFO] {"timestamp":"2025-10-26T21:49:40.172Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.checkpoints/checkpoint_1761515380167.json (SHA: 158b35f)","details":{}}
[2025-10-26T21:49:40.172Z] [INFO] {"timestamp":"2025-10-26T21:49:40.172Z","level":"INFO","message":"[StateManager] Created checkpoint: checkpoint_1761515380167 - Session session_1761515380150_54ea23f56a00c41c - Turn 0 start","details":{}}
[2025-10-26T21:49:40.172Z] [INFO] {"timestamp":"2025-10-26T21:49:40.172Z","level":"INFO","message":"[SessionManager] Created turn checkpoint: checkpoint_1761515380167","details":{}}
[2025-10-26T21:49:40.177Z] [INFO] {"timestamp":"2025-10-26T21:49:40.177Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761515380150_54ea23f56a00c41c/session.json (SHA: 9a6ac2b)","details":{}}
[2025-10-26T21:49:40.177Z] [INFO] {"timestamp":"2025-10-26T21:49:40.177Z","level":"INFO","message":"[SessionManager] Created turn 0 for session session_1761515380150_54ea23f56a00c41c","details":{}}
[2025-10-26T21:49:40.177Z] [INFO] {"timestamp":"2025-10-26T21:49:40.177Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"","progress":null}}
[2025-10-26T21:49:40.177Z] [INFO] {"timestamp":"2025-10-26T21:49:40.177Z","level":"INFO","message":"[SentinelFSM] State transition: IDLE -> CURATING_CONTEXT","details":{}}
[2025-10-26T21:49:40.177Z] [INFO] {"timestamp":"2025-10-26T21:49:40.177Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"IDLE","newState":"CURATING_CONTEXT","context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761515380150_54ea23f56a00c41c","turn":{"turn":0,"context_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.context.md","proposal_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515380167","createdAt":"2025-10-26T21:49:40.172Z"},"startTime":1761515380177,"iterations":0,"maxIterations":10}}}
[2025-10-26T21:49:40.177Z] [INFO] {"timestamp":"2025-10-26T21:49:40.177Z","level":"INFO","message":"[SentinelFSM] Executing state: CURATING_CONTEXT","details":{}}
[2025-10-26T21:49:40.177Z] [INFO] {"timestamp":"2025-10-26T21:49:40.177Z","level":"INFO","message":"[EventBus] Emitting event: agent:curating","details":{"goal":"Analyze your own inefficiency patterns and improve yourself"}}
[2025-10-26T21:49:40.177Z] [INFO] {"timestamp":"2025-10-26T21:49:40.177Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Analyzing project files...","progress":null}}
[2025-10-26T21:49:40.178Z] [WARN] {"timestamp":"2025-10-26T21:49:40.178Z","level":"WARN","message":"[SentinelTools] No files available for curation","details":{}}
[2025-10-26T21:49:40.178Z] [INFO] {"timestamp":"2025-10-26T21:49:40.178Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Found 0 relevant files","progress":50}}
[2025-10-26T21:49:40.178Z] [ERROR] {"timestamp":"2025-10-26T21:49:40.178Z","level":"ERROR","message":"[SentinelFSM] Error in state CURATING_CONTEXT:","details":{}}
[2025-10-26T21:49:40.178Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:49:40.178Z] [INFO] {"timestamp":"2025-10-26T21:49:40.178Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"ERROR","detail":"","progress":null}}
[2025-10-26T21:49:40.178Z] [INFO] {"timestamp":"2025-10-26T21:49:40.178Z","level":"INFO","message":"[SentinelFSM] State transition: CURATING_CONTEXT -> ERROR","details":{}}
[2025-10-26T21:49:40.178Z] [INFO] {"timestamp":"2025-10-26T21:49:40.178Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"CURATING_CONTEXT","newState":"ERROR","context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761515380150_54ea23f56a00c41c","turn":{"turn":0,"context_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.context.md","proposal_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515380167","createdAt":"2025-10-26T21:49:40.172Z"},"startTime":1761515380177,"iterations":0,"maxIterations":10}}}
[2025-10-26T21:49:40.178Z] [INFO] {"timestamp":"2025-10-26T21:49:40.178Z","level":"INFO","message":"[SentinelFSM] Executing state: ERROR","details":{}}
[2025-10-26T21:49:40.178Z] [INFO] {"timestamp":"2025-10-26T21:49:40.178Z","level":"INFO","message":"[EventBus] Emitting event: agent:error","details":{"state":{"from":"CURATING_CONTEXT","to":"ERROR","timestamp":1761515380178,"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761515380150_54ea23f56a00c41c","turn":{"turn":0,"context_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.context.md","proposal_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515380167","createdAt":"2025-10-26T21:49:40.172Z"},"startTime":1761515380177,"iterations":0,"maxIterations":10}},"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761515380150_54ea23f56a00c41c","turn":{"turn":0,"context_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.context.md","proposal_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515380167","createdAt":"2025-10-26T21:49:40.172Z"},"startTime":1761515380177,"iterations":0,"maxIterations":10}}}
[2025-10-26T21:49:40.178Z] [ERROR] {"timestamp":"2025-10-26T21:49:40.178Z","level":"ERROR","message":"[SentinelFSM] Error state reached","details":{"previous_state":{"from":"IDLE","to":"CURATING_CONTEXT","timestamp":1761515380177,"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761515380150_54ea23f56a00c41c","turn":{"turn":0,"context_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.context.md","proposal_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515380167","createdAt":"2025-10-26T21:49:40.172Z"},"startTime":1761515380177,"iterations":0,"maxIterations":10}},"context":{"goal":"Analyze your own inefficiency patterns and improve yourself","sessionId":"session_1761515380150_54ea23f56a00c41c","turn":{"turn":0,"context_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.context.md","proposal_path":"/sessions/session_1761515380150_54ea23f56a00c41c/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515380167","createdAt":"2025-10-26T21:49:40.172Z"},"startTime":1761515380177,"iterations":0,"maxIterations":10}}}
[2025-10-26T21:49:40.178Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:49:40.178Z] [INFO] {"timestamp":"2025-10-26T21:49:40.178Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"IDLE","detail":"","progress":null}}
[2025-10-26T21:49:40.178Z] [INFO] {"timestamp":"2025-10-26T21:49:40.178Z","level":"INFO","message":"[SentinelFSM] State transition: ERROR -> IDLE","details":{}}
[2025-10-26T21:49:40.178Z] [INFO] {"timestamp":"2025-10-26T21:49:40.178Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"ERROR","newState":"IDLE","context":null}}
[2025-10-26T21:49:40.178Z] [INFO] {"timestamp":"2025-10-26T21:49:40.178Z","level":"INFO","message":"[CoreLogic] Agent cycle started successfully","details":{}}
[2025-10-26T21:49:40.178Z] [LOG] [Boot] Agent system awakened successfully
[2025-10-26T21:49:44.732Z] [INFO] {"timestamp":"2025-10-26T21:49:44.732Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-26T21:49:44.732Z] [INFO] {"timestamp":"2025-10-26T21:49:44.732Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T21:49:44.737Z] [INFO] {"timestamp":"2025-10-26T21:49:44.737Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-26T21:49:44.739Z] [INFO] {"timestamp":"2025-10-26T21:49:44.739Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-26T21:49:50.471Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:49:50.472Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:49:50.525Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:49:50.525Z] [LOG] [API] Checking server status...
[2025-10-26T21:49:50.534Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:49:50.526Z"
}
[2025-10-26T21:49:50.534Z] [LOG] [API] WebGPU available
[2025-10-26T21:49:50.541Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:49:50.541Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:49:50.542Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:49:50.548Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:50:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:50:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:50:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:50:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T21:50:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:50:50.364Z"
}
[2025-10-26T21:50:50.373Z] [LOG] [API] WebGPU available
[2025-10-26T21:50:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:50:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:50:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:50:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:51:14.572Z] [WARN] {"timestamp":"2025-10-26T21:51:14.571Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-26T21:51:19.572Z] [INFO] {"timestamp":"2025-10-26T21:51:19.572Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-26T21:51:19.572Z] [INFO] {"timestamp":"2025-10-26T21:51:19.572Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T21:51:19.576Z] [INFO] {"timestamp":"2025-10-26T21:51:19.576Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-26T21:51:19.578Z] [INFO] {"timestamp":"2025-10-26T21:51:19.578Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-26T21:51:47.378Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:51:47.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:51:47.417Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:51:47.417Z] [LOG] [API] Checking server status...
[2025-10-26T21:51:47.425Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:51:47.420Z"
}
[2025-10-26T21:51:47.425Z] [LOG] [API] WebGPU available
[2025-10-26T21:51:47.432Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:51:47.432Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:51:47.432Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:51:47.449Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:51:50.999Z] [INFO] {"timestamp":"2025-10-26T21:51:50.999Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T21:51:50.999Z] [INFO] {"timestamp":"2025-10-26T21:51:50.999Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T21:51:51.000Z] [INFO] {"timestamp":"2025-10-26T21:51:51.000Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T21:51:51.000Z] [INFO] {"timestamp":"2025-10-26T21:51:51.000Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T21:51:51.001Z] [INFO] {"timestamp":"2025-10-26T21:51:51.001Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-26T21:51:51.001Z] [INFO] {"timestamp":"2025-10-26T21:51:51.001Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-26T21:51:51.001Z] [INFO] {"timestamp":"2025-10-26T21:51:51.001Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-26T21:51:51.001Z] [INFO] {"timestamp":"2025-10-26T21:51:51.001Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-26T21:51:51.002Z] [INFO] {"timestamp":"2025-10-26T21:51:51.002Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-26T21:51:51.002Z] [INFO] {"timestamp":"2025-10-26T21:51:51.002Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-26T21:51:51.004Z] [INFO] {"timestamp":"2025-10-26T21:51:51.004Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-26T21:51:51.004Z] [INFO] {"timestamp":"2025-10-26T21:51:51.004Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-26T21:51:51.005Z] [INFO] {"timestamp":"2025-10-26T21:51:51.005Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-26T21:51:51.005Z] [INFO] {"timestamp":"2025-10-26T21:51:51.005Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-26T21:51:51.006Z] [INFO] {"timestamp":"2025-10-26T21:51:51.006Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-26T21:51:51.006Z] [INFO] {"timestamp":"2025-10-26T21:51:51.006Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-26T21:51:51.006Z] [INFO] {"timestamp":"2025-10-26T21:51:51.006Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-26T21:51:51.006Z] [INFO] {"timestamp":"2025-10-26T21:51:51.006Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-26T21:51:51.007Z] [INFO] {"timestamp":"2025-10-26T21:51:51.007Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-26T21:51:51.007Z] [INFO] {"timestamp":"2025-10-26T21:51:51.007Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-26T21:51:51.008Z] [INFO] {"timestamp":"2025-10-26T21:51:51.008Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-26T21:51:51.008Z] [INFO] {"timestamp":"2025-10-26T21:51:51.008Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-26T21:51:51.009Z] [INFO] {"timestamp":"2025-10-26T21:51:51.009Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-26T21:51:51.009Z] [INFO] {"timestamp":"2025-10-26T21:51:51.009Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-26T21:51:51.010Z] [INFO] {"timestamp":"2025-10-26T21:51:51.010Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-26T21:51:51.010Z] [INFO] {"timestamp":"2025-10-26T21:51:51.010Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-26T21:51:51.010Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-26T21:51:51.010Z] [ERROR] {"timestamp":"2025-10-26T21:51:51.010Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T21:51:51.010Z] [WARN] {"timestamp":"2025-10-26T21:51:51.010Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-26T21:51:51.011Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-26T21:51:51.011Z] [ERROR] {"timestamp":"2025-10-26T21:51:51.011Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T21:51:51.011Z] [WARN] {"timestamp":"2025-10-26T21:51:51.011Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-26T21:51:51.011Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-26T21:51:51.011Z] [ERROR] {"timestamp":"2025-10-26T21:51:51.011Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T21:51:51.011Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:51:51.011Z] [WARN] {"timestamp":"2025-10-26T21:51:51.011Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-26T21:51:51.012Z] [INFO] {"timestamp":"2025-10-26T21:51:51.012Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-26T21:51:51.012Z] [INFO] {"timestamp":"2025-10-26T21:51:51.012Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-26T21:51:51.013Z] [INFO] {"timestamp":"2025-10-26T21:51:51.013Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-26T21:51:51.013Z] [INFO] {"timestamp":"2025-10-26T21:51:51.013Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-26T21:51:51.015Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T21:51:51.015Z] [INFO] {"timestamp":"2025-10-26T21:51:51.015Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T21:51:51.015Z] [INFO] {"timestamp":"2025-10-26T21:51:51.015Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T21:51:51.016Z] [INFO] {"timestamp":"2025-10-26T21:51:51.016Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T21:51:51.016Z] [INFO] {"timestamp":"2025-10-26T21:51:51.016Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T21:51:51.018Z] [INFO] {"timestamp":"2025-10-26T21:51:51.018Z","level":"INFO","message":"[DIContainer] Registered module: GitVFS","details":{}}
[2025-10-26T21:51:51.018Z] [INFO] {"timestamp":"2025-10-26T21:51:51.018Z","level":"INFO","message":"[CoreLogic] Registered module: GitVFS from /upgrades/git-vfs.js","details":{}}
[2025-10-26T21:51:51.019Z] [INFO] {"timestamp":"2025-10-26T21:51:51.019Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T21:51:51.019Z] [INFO] {"timestamp":"2025-10-26T21:51:51.019Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T21:51:51.020Z] [INFO] {"timestamp":"2025-10-26T21:51:51.020Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-26T21:51:51.020Z] [INFO] {"timestamp":"2025-10-26T21:51:51.020Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-26T21:51:51.021Z] [LOG] [CoreLogic] Loaded module from /upgrades/diff-viewer-ui.js: NO_METADATA
[2025-10-26T21:51:51.021Z] [ERROR] {"timestamp":"2025-10-26T21:51:51.021Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/diff-viewer-ui.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T21:51:51.022Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:51:51.022Z] [WARN] {"timestamp":"2025-10-26T21:51:51.022Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/diff-viewer-ui.js missing metadata. Module:","details":{}}
[2025-10-26T21:51:51.023Z] [INFO] {"timestamp":"2025-10-26T21:51:51.023Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCSwarm","details":{}}
[2025-10-26T21:51:51.023Z] [INFO] {"timestamp":"2025-10-26T21:51:51.023Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCSwarm from /upgrades/webrtc-swarm.js","details":{}}
[2025-10-26T21:51:51.024Z] [INFO] {"timestamp":"2025-10-26T21:51:51.024Z","level":"INFO","message":"[DIContainer] Registered module: WebRTCCoordinator","details":{}}
[2025-10-26T21:51:51.024Z] [INFO] {"timestamp":"2025-10-26T21:51:51.024Z","level":"INFO","message":"[CoreLogic] Registered module: WebRTCCoordinator from /upgrades/webrtc-coordinator.js","details":{}}
[2025-10-26T21:51:51.026Z] [INFO] {"timestamp":"2025-10-26T21:51:51.026Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T21:51:51.026Z] [INFO] {"timestamp":"2025-10-26T21:51:51.026Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T21:51:51.027Z] [INFO] {"timestamp":"2025-10-26T21:51:51.027Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T21:51:51.027Z] [INFO] {"timestamp":"2025-10-26T21:51:51.027Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T21:51:51.028Z] [INFO] {"timestamp":"2025-10-26T21:51:51.028Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T21:51:51.028Z] [INFO] {"timestamp":"2025-10-26T21:51:51.028Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T21:51:51.029Z] [INFO] {"timestamp":"2025-10-26T21:51:51.029Z","level":"INFO","message":"[DIContainer] Registered module: PeerReviewConsensus","details":{}}
[2025-10-26T21:51:51.029Z] [INFO] {"timestamp":"2025-10-26T21:51:51.029Z","level":"INFO","message":"[CoreLogic] Registered module: PeerReviewConsensus from /upgrades/peer-review-consensus.js","details":{}}
[2025-10-26T21:51:51.029Z] [INFO] {"timestamp":"2025-10-26T21:51:51.029Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T21:51:51.030Z] [INFO] {"timestamp":"2025-10-26T21:51:51.030Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T21:51:51.032Z] [INFO] {"timestamp":"2025-10-26T21:51:51.032Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T21:51:51.032Z] [INFO] {"timestamp":"2025-10-26T21:51:51.032Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-26T21:51:51.033Z] [INFO] {"timestamp":"2025-10-26T21:51:51.033Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-26T21:51:51.033Z] [WARN] {"timestamp":"2025-10-26T21:51:51.033Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-26T21:51:51.033Z] [INFO] {"timestamp":"2025-10-26T21:51:51.033Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-26T21:51:51.033Z] [INFO] {"timestamp":"2025-10-26T21:51:51.033Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-26T21:51:51.034Z] [WARN] {"timestamp":"2025-10-26T21:51:51.034Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-26T21:51:51.034Z] [INFO] {"timestamp":"2025-10-26T21:51:51.034Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-26T21:51:51.035Z] [WARN] {"timestamp":"2025-10-26T21:51:51.034Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-26T21:51:51.035Z] [INFO] {"timestamp":"2025-10-26T21:51:51.035Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-26T21:51:51.035Z] [INFO] {"timestamp":"2025-10-26T21:51:51.035Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-26T21:51:51.035Z] [INFO] {"timestamp":"2025-10-26T21:51:51.035Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-26T21:51:51.037Z] [INFO] {"timestamp":"2025-10-26T21:51:51.037Z","level":"INFO","message":"[ReflectionStore] Initializing reflection persistence","details":{}}
[2025-10-26T21:51:51.038Z] [INFO] {"timestamp":"2025-10-26T21:51:51.038Z","level":"INFO","message":"[ReflectionStore] Database opened successfully","details":{}}
[2025-10-26T21:51:51.039Z] [INFO] {"timestamp":"2025-10-26T21:51:51.039Z","level":"INFO","message":"[SelfTester] Initialized","details":{}}
[2025-10-26T21:51:51.039Z] [INFO] {"timestamp":"2025-10-26T21:51:51.039Z","level":"INFO","message":"[BrowserAPIs] Initializing web API integration...","details":{}}
[2025-10-26T21:51:51.039Z] [INFO] {"timestamp":"2025-10-26T21:51:51.039Z","level":"INFO","message":"[BrowserAPIs] Capabilities detected:","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-26T21:51:51.039Z] [INFO] {"timestamp":"2025-10-26T21:51:51.039Z","level":"INFO","message":"[EventBus] Emitting event: browser-apis:initialized","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-26T21:51:51.041Z] [INFO] {"timestamp":"2025-10-26T21:51:51.041Z","level":"INFO","message":"[GitVFS] Initialized successfully","details":{}}
[2025-10-26T21:51:51.041Z] [INFO] {"timestamp":"2025-10-26T21:51:51.041Z","level":"INFO","message":"[WebRTCSwarm] Initializing swarm module","details":{}}
[2025-10-26T21:51:51.042Z] [INFO] {"timestamp":"2025-10-26T21:51:51.041Z","level":"INFO","message":"[WebRTCSwarm] Local peer ID: reploid-id_1761515511041_nfsbahu6w","details":{}}
[2025-10-26T21:51:51.042Z] [INFO] {"timestamp":"2025-10-26T21:51:51.042Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T21:51:51.042Z] [INFO] {"timestamp":"2025-10-26T21:51:51.042Z","level":"INFO","message":"[WebRTCSwarm] Swarm initialized","details":{}}
[2025-10-26T21:51:51.042Z] [INFO] {"timestamp":"2025-10-26T21:51:51.042Z","level":"INFO","message":"[SwarmOrch] Initializing swarm orchestrator","details":{}}
[2025-10-26T21:51:51.042Z] [WARN] {"timestamp":"2025-10-26T21:51:51.042Z","level":"WARN","message":"[WebRTCSwarm] Cannot send signaling message: not connected","details":{}}
[2025-10-26T21:51:51.042Z] [INFO] {"timestamp":"2025-10-26T21:51:51.042Z","level":"INFO","message":"[SwarmOrch] Swarm orchestrator initialized","details":{"capabilities":["code-generation","file-management"]}}
[2025-10-26T21:51:51.044Z] [INFO] {"timestamp":"2025-10-26T21:51:51.044Z","level":"INFO","message":"[PyodideRuntime] Initializing Pyodide runtime...","details":{}}
[2025-10-26T21:51:51.044Z] [INFO] {"timestamp":"2025-10-26T21:51:51.044Z","level":"INFO","message":"[PyodideRuntime] Worker created","details":{}}
[2025-10-26T21:51:51.044Z] [ERROR] {"timestamp":"2025-10-26T21:51:51.044Z","level":"ERROR","message":"[WebRTCSwarm] Signaling WebSocket error:","details":{"isTrusted":true}}
[2025-10-26T21:51:51.044Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:51:51.044Z] [WARN] {"timestamp":"2025-10-26T21:51:51.044Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-26T21:51:50.300Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:51:50.300Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:51:50.332Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:51:50.332Z] [LOG] [API] Checking server status...
[2025-10-26T21:51:50.344Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:51:50.335Z"
}
[2025-10-26T21:51:50.344Z] [LOG] [API] WebGPU available
[2025-10-26T21:51:50.365Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:51:50.365Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:51:50.365Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:51:50.383Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:51:52.635Z] [INFO] {"timestamp":"2025-10-26T21:51:52.635Z","level":"INFO","message":"[PyodideRuntime] Pyodide initialized","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-26T21:51:52.635Z] [INFO] {"timestamp":"2025-10-26T21:51:52.635Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:ready","details":{"version":"0.26.4","platform":"emscripten"}}
[2025-10-26T21:51:52.635Z] [INFO] {"timestamp":"2025-10-26T21:51:52.635Z","level":"INFO","message":"[PyodideRuntime] Pyodide runtime ready","details":{}}
[2025-10-26T21:51:52.635Z] [INFO] {"timestamp":"2025-10-26T21:51:52.635Z","level":"INFO","message":"[EventBus] Emitting event: pyodide:initialized","details":{"ready":true}}
[2025-10-26T21:51:52.637Z] [INFO] {"timestamp":"2025-10-26T21:51:52.636Z","level":"INFO","message":"Dashboard UI Manager (Event-Driven) taking control of DOM...","details":{}}
[2025-10-26T21:51:52.646Z] [INFO] {"timestamp":"2025-10-26T21:51:52.646Z","level":"INFO","message":"[UIManager] Initializing modular panel support...","details":{}}
[2025-10-26T21:51:52.646Z] [INFO] {"timestamp":"2025-10-26T21:51:52.646Z","level":"INFO","message":"[UIManager] Modular panel initialization complete","details":{}}
[2025-10-26T21:51:52.647Z] [INFO] {"timestamp":"2025-10-26T21:51:52.647Z","level":"INFO","message":"Dashboard UI Initialized. Listening for events.","details":{}}
[2025-10-26T21:51:52.647Z] [LOG] [CoreLogic] Attempting to initialize DiffViewerUI...
[2025-10-26T21:51:52.647Z] [LOG] [CoreLogic] Resolving DiffViewerUI from container...
[2025-10-26T21:51:52.647Z] [ERROR] [CoreLogic] DiffViewerUI initialization error: Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
Error: [DIContainer] Service not found: DiffViewerUI
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus
Tip: Check module ID spelling and ensure the module is registered in config.json
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:681:33)), <anonymous>:61:23)
    at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:681:33), <anonymous>:317:46)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:701:9)
[2025-10-26T21:51:52.647Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:51:52.647Z] [WARN] {"timestamp":"2025-10-26T21:51:52.647Z","level":"WARN","message":"[CoreLogic] DiffViewerUI initialization failed:","details":"[DIContainer] Service not found: DiffViewerUI\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-26T21:51:52.647Z] [INFO] {"timestamp":"2025-10-26T21:51:52.647Z","level":"INFO","message":"[CoreLogic] Agent initialization complete. System is operational.","details":{}}
[2025-10-26T21:51:52.647Z] [INFO] {"timestamp":"2025-10-26T21:51:52.647Z","level":"INFO","message":"[CoreLogic] Creating genesis snapshot of initial boot state...","details":{}}
[2025-10-26T21:51:52.647Z] [WARN] {"timestamp":"2025-10-26T21:51:52.647Z","level":"WARN","message":"[CoreLogic] Failed to create genesis snapshot (non-fatal):","details":"[DIContainer] Service not found: GenesisSnapshot\nAvailable services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, AgentLogicPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, ReflectionStore, PerformanceMonitor, ToastNotifications, StreamingResponseHandler, ContextManager, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, UI, CycleLogic, GitVFS, SentinelTools, SentinelPanel, WebRTCSwarm, WebRTCCoordinator, SentinelFSM, MetaToolCreator, ModelArena, PeerReviewConsensus\nTip: Check module ID spelling and ensure the module is registered in config.json"}
[2025-10-26T21:51:52.647Z] [INFO] {"timestamp":"2025-10-26T21:51:52.647Z","level":"INFO","message":"[CoreLogic] Starting agent with initial goal:","details":"Build a self-modifying code generation system"}
[2025-10-26T21:51:52.647Z] [INFO] {"timestamp":"2025-10-26T21:51:52.647Z","level":"INFO","message":"[UI] updateGoal called with:","details":{"text":"Build a self-modifying code generation system","hasGoalTextRef":true}}
[2025-10-26T21:51:52.647Z] [INFO] {"timestamp":"2025-10-26T21:51:52.647Z","level":"INFO","message":"[UI] Goal text element updated successfully","details":{}}
[2025-10-26T21:51:52.671Z] [INFO] {"timestamp":"2025-10-26T21:51:52.670Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761515512650_f835db28302a4c3a/session.json (SHA: 3f93bac)","details":{}}
[2025-10-26T21:51:52.671Z] [INFO] {"timestamp":"2025-10-26T21:51:52.671Z","level":"INFO","message":"[SessionManager] Created new session: session_1761515512650_f835db28302a4c3a","details":{}}
[2025-10-26T21:51:52.679Z] [INFO] {"timestamp":"2025-10-26T21:51:52.679Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /.checkpoints/checkpoint_1761515512671.json (SHA: 3f3b1bd)","details":{}}
[2025-10-26T21:51:52.679Z] [INFO] {"timestamp":"2025-10-26T21:51:52.679Z","level":"INFO","message":"[StateManager] Created checkpoint: checkpoint_1761515512671 - Session session_1761515512650_f835db28302a4c3a - Turn 0 start","details":{}}
[2025-10-26T21:51:52.679Z] [INFO] {"timestamp":"2025-10-26T21:51:52.679Z","level":"INFO","message":"[SessionManager] Created turn checkpoint: checkpoint_1761515512671","details":{}}
[2025-10-26T21:51:52.687Z] [INFO] {"timestamp":"2025-10-26T21:51:52.687Z","level":"INFO","message":"[Storage-Git] Committed changes: Agent modified /sessions/session_1761515512650_f835db28302a4c3a/session.json (SHA: 4671179)","details":{}}
[2025-10-26T21:51:52.687Z] [INFO] {"timestamp":"2025-10-26T21:51:52.687Z","level":"INFO","message":"[SessionManager] Created turn 0 for session session_1761515512650_f835db28302a4c3a","details":{}}
[2025-10-26T21:51:52.687Z] [INFO] {"timestamp":"2025-10-26T21:51:52.687Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"","progress":null}}
[2025-10-26T21:51:52.687Z] [INFO] {"timestamp":"2025-10-26T21:51:52.687Z","level":"INFO","message":"[SentinelFSM] State transition: IDLE -> CURATING_CONTEXT","details":{}}
[2025-10-26T21:51:52.687Z] [INFO] {"timestamp":"2025-10-26T21:51:52.687Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"IDLE","newState":"CURATING_CONTEXT","context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761515512650_f835db28302a4c3a","turn":{"turn":0,"context_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.context.md","proposal_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515512671","createdAt":"2025-10-26T21:51:52.679Z"},"startTime":1761515512687,"iterations":0,"maxIterations":10}}}
[2025-10-26T21:51:52.688Z] [INFO] {"timestamp":"2025-10-26T21:51:52.688Z","level":"INFO","message":"[SentinelFSM] Executing state: CURATING_CONTEXT","details":{}}
[2025-10-26T21:51:52.688Z] [INFO] {"timestamp":"2025-10-26T21:51:52.688Z","level":"INFO","message":"[EventBus] Emitting event: agent:curating","details":{"goal":"Build a self-modifying code generation system"}}
[2025-10-26T21:51:52.688Z] [INFO] {"timestamp":"2025-10-26T21:51:52.688Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Analyzing project files...","progress":null}}
[2025-10-26T21:51:52.688Z] [WARN] {"timestamp":"2025-10-26T21:51:52.688Z","level":"WARN","message":"[SentinelTools] No files available for curation","details":{}}
[2025-10-26T21:51:52.688Z] [INFO] {"timestamp":"2025-10-26T21:51:52.688Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"CURATING_CONTEXT","detail":"Found 0 relevant files","progress":50}}
[2025-10-26T21:51:52.688Z] [ERROR] {"timestamp":"2025-10-26T21:51:52.688Z","level":"ERROR","message":"[SentinelFSM] Error in state CURATING_CONTEXT:","details":{}}
[2025-10-26T21:51:52.688Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:51:52.688Z] [INFO] {"timestamp":"2025-10-26T21:51:52.688Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"ERROR","detail":"","progress":null}}
[2025-10-26T21:51:52.688Z] [INFO] {"timestamp":"2025-10-26T21:51:52.688Z","level":"INFO","message":"[SentinelFSM] State transition: CURATING_CONTEXT -> ERROR","details":{}}
[2025-10-26T21:51:52.688Z] [INFO] {"timestamp":"2025-10-26T21:51:52.688Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"CURATING_CONTEXT","newState":"ERROR","context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761515512650_f835db28302a4c3a","turn":{"turn":0,"context_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.context.md","proposal_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515512671","createdAt":"2025-10-26T21:51:52.679Z"},"startTime":1761515512687,"iterations":0,"maxIterations":10}}}
[2025-10-26T21:51:52.688Z] [INFO] {"timestamp":"2025-10-26T21:51:52.688Z","level":"INFO","message":"[SentinelFSM] Executing state: ERROR","details":{}}
[2025-10-26T21:51:52.688Z] [INFO] {"timestamp":"2025-10-26T21:51:52.688Z","level":"INFO","message":"[EventBus] Emitting event: agent:error","details":{"state":{"from":"CURATING_CONTEXT","to":"ERROR","timestamp":1761515512688,"context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761515512650_f835db28302a4c3a","turn":{"turn":0,"context_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.context.md","proposal_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515512671","createdAt":"2025-10-26T21:51:52.679Z"},"startTime":1761515512687,"iterations":0,"maxIterations":10}},"context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761515512650_f835db28302a4c3a","turn":{"turn":0,"context_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.context.md","proposal_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515512671","createdAt":"2025-10-26T21:51:52.679Z"},"startTime":1761515512687,"iterations":0,"maxIterations":10}}}
[2025-10-26T21:51:52.689Z] [ERROR] {"timestamp":"2025-10-26T21:51:52.688Z","level":"ERROR","message":"[SentinelFSM] Error state reached","details":{"previous_state":{"from":"IDLE","to":"CURATING_CONTEXT","timestamp":1761515512687,"context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761515512650_f835db28302a4c3a","turn":{"turn":0,"context_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.context.md","proposal_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515512671","createdAt":"2025-10-26T21:51:52.679Z"},"startTime":1761515512687,"iterations":0,"maxIterations":10}},"context":{"goal":"Build a self-modifying code generation system","sessionId":"session_1761515512650_f835db28302a4c3a","turn":{"turn":0,"context_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.context.md","proposal_path":"/sessions/session_1761515512650_f835db28302a4c3a/turn-0.proposal.md","status":"pending_context","checkpointId":"checkpoint_1761515512671","createdAt":"2025-10-26T21:51:52.679Z"},"startTime":1761515512687,"iterations":0,"maxIterations":10}}}
[2025-10-26T21:51:52.689Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T21:51:52.689Z] [INFO] {"timestamp":"2025-10-26T21:51:52.689Z","level":"INFO","message":"[EventBus] Emitting event: status:updated","details":{"state":"IDLE","detail":"","progress":null}}
[2025-10-26T21:51:52.689Z] [INFO] {"timestamp":"2025-10-26T21:51:52.689Z","level":"INFO","message":"[SentinelFSM] State transition: ERROR -> IDLE","details":{}}
[2025-10-26T21:51:52.689Z] [INFO] {"timestamp":"2025-10-26T21:51:52.689Z","level":"INFO","message":"[EventBus] Emitting event: fsm:state:changed","details":{"oldState":"ERROR","newState":"IDLE","context":null}}
[2025-10-26T21:51:52.689Z] [INFO] {"timestamp":"2025-10-26T21:51:52.689Z","level":"INFO","message":"[CoreLogic] Agent cycle started successfully","details":{}}
[2025-10-26T21:51:52.689Z] [LOG] [Boot] Agent system awakened successfully
[2025-10-26T21:51:56.045Z] [INFO] {"timestamp":"2025-10-26T21:51:56.045Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-26T21:51:56.045Z] [INFO] {"timestamp":"2025-10-26T21:51:56.045Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T21:51:56.049Z] [INFO] {"timestamp":"2025-10-26T21:51:56.049Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-26T21:51:56.050Z] [INFO] {"timestamp":"2025-10-26T21:51:56.050Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-26T21:52:50.536Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:52:50.537Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:52:50.539Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:52:50.539Z] [LOG] [API] Checking server status...
[2025-10-26T21:52:50.548Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:52:50.541Z"
}
[2025-10-26T21:52:50.548Z] [LOG] [API] WebGPU available
[2025-10-26T21:52:50.552Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:52:50.552Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:52:50.552Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:52:50.570Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:53:11.265Z] [WARN] {"timestamp":"2025-10-26T21:53:11.265Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-26T21:53:16.265Z] [INFO] {"timestamp":"2025-10-26T21:53:16.265Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-26T21:53:16.266Z] [INFO] {"timestamp":"2025-10-26T21:53:16.266Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T21:53:16.269Z] [INFO] {"timestamp":"2025-10-26T21:53:16.269Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-26T21:53:16.270Z] [INFO] {"timestamp":"2025-10-26T21:53:16.270Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-26T21:53:29.245Z] [WARN] {"timestamp":"2025-10-26T21:53:29.245Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-26T21:53:34.245Z] [INFO] {"timestamp":"2025-10-26T21:53:34.245Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-26T21:53:34.246Z] [INFO] {"timestamp":"2025-10-26T21:53:34.246Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T21:53:34.250Z] [INFO] {"timestamp":"2025-10-26T21:53:34.249Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-26T21:53:34.251Z] [INFO] {"timestamp":"2025-10-26T21:53:34.251Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-26T21:53:50.344Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:53:50.344Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:53:50.348Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:53:50.348Z] [LOG] [API] Checking server status...
[2025-10-26T21:53:50.358Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:53:50.349Z"
}
[2025-10-26T21:53:50.358Z] [LOG] [API] WebGPU available
[2025-10-26T21:53:50.376Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:53:50.376Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:53:50.376Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:53:50.398Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:53:56.846Z] [WARN] {"timestamp":"2025-10-26T21:53:56.846Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-26T21:54:01.847Z] [INFO] {"timestamp":"2025-10-26T21:54:01.846Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-26T21:54:01.847Z] [INFO] {"timestamp":"2025-10-26T21:54:01.847Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T21:54:01.851Z] [INFO] {"timestamp":"2025-10-26T21:54:01.851Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-26T21:54:01.852Z] [INFO] {"timestamp":"2025-10-26T21:54:01.852Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-26T21:54:12.173Z] [WARN] {"timestamp":"2025-10-26T21:54:12.173Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-26T21:54:17.174Z] [INFO] {"timestamp":"2025-10-26T21:54:17.173Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-26T21:54:17.174Z] [INFO] {"timestamp":"2025-10-26T21:54:17.174Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T21:54:17.177Z] [INFO] {"timestamp":"2025-10-26T21:54:17.177Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-26T21:54:17.178Z] [INFO] {"timestamp":"2025-10-26T21:54:17.178Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-26T21:54:50.348Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:54:50.348Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:54:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:54:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T21:54:50.387Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:54:50.380Z"
}
[2025-10-26T21:54:50.387Z] [LOG] [API] WebGPU available
[2025-10-26T21:54:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:54:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:54:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:54:50.430Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:55:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:55:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:55:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:55:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T21:55:50.371Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:55:50.362Z"
}
[2025-10-26T21:55:50.371Z] [LOG] [API] WebGPU available
[2025-10-26T21:55:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:55:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:55:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:55:50.410Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:56:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:56:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:56:50.393Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:56:50.393Z] [LOG] [API] Checking server status...
[2025-10-26T21:56:50.403Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:56:50.395Z"
}
[2025-10-26T21:56:50.403Z] [LOG] [API] WebGPU available
[2025-10-26T21:56:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:56:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:56:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:56:50.422Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:57:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:57:50.358Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:57:50.361Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:57:50.361Z] [LOG] [API] Checking server status...
[2025-10-26T21:57:50.372Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:57:50.363Z"
}
[2025-10-26T21:57:50.372Z] [LOG] [API] WebGPU available
[2025-10-26T21:57:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:57:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:57:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:57:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:58:50.462Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:58:50.462Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:58:50.465Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:58:50.465Z] [LOG] [API] Checking server status...
[2025-10-26T21:58:50.474Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:58:50.466Z"
}
[2025-10-26T21:58:50.474Z] [LOG] [API] WebGPU available
[2025-10-26T21:58:50.476Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:58:50.476Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:58:50.476Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:58:50.495Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T21:59:45.104Z] [WARN] {"timestamp":"2025-10-26T21:59:45.104Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-26T21:59:50.105Z] [INFO] {"timestamp":"2025-10-26T21:59:50.104Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-26T21:59:50.105Z] [INFO] {"timestamp":"2025-10-26T21:59:50.105Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T21:59:50.108Z] [INFO] {"timestamp":"2025-10-26T21:59:50.108Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-26T21:59:50.110Z] [INFO] {"timestamp":"2025-10-26T21:59:50.110Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-26T21:59:50.354Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T21:59:50.354Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T21:59:50.357Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T21:59:50.357Z] [LOG] [API] Checking server status...
[2025-10-26T21:59:50.367Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T21:59:50.359Z"
}
[2025-10-26T21:59:50.367Z] [LOG] [API] WebGPU available
[2025-10-26T21:59:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T21:59:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T21:59:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T21:59:50.404Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:00:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:00:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:00:50.437Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:00:50.437Z] [LOG] [API] Checking server status...
[2025-10-26T22:00:50.446Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:00:50.438Z"
}
[2025-10-26T22:00:50.446Z] [LOG] [API] WebGPU available
[2025-10-26T22:00:50.453Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:00:50.453Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:00:50.453Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:00:50.467Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:01:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:01:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:01:50.369Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:01:50.369Z] [LOG] [API] Checking server status...
[2025-10-26T22:01:50.381Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:01:50.371Z"
}
[2025-10-26T22:01:50.381Z] [LOG] [API] WebGPU available
[2025-10-26T22:01:50.387Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:01:50.387Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:01:50.387Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:01:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:02:50.451Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:02:50.452Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:02:50.495Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:02:50.495Z] [LOG] [API] Checking server status...
[2025-10-26T22:02:50.513Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:02:50.499Z"
}
[2025-10-26T22:02:50.513Z] [LOG] [API] WebGPU available
[2025-10-26T22:02:50.531Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:02:50.531Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:02:50.532Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:02:50.546Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:03:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:03:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:03:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:03:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T22:03:50.392Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:03:50.385Z"
}
[2025-10-26T22:03:50.392Z] [LOG] [API] WebGPU available
[2025-10-26T22:03:50.410Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:03:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:03:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:03:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:04:50.487Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:04:50.488Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:04:50.490Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:04:50.490Z] [LOG] [API] Checking server status...
[2025-10-26T22:04:50.500Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:04:50.491Z"
}
[2025-10-26T22:04:50.500Z] [LOG] [API] WebGPU available
[2025-10-26T22:04:50.504Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:04:50.504Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:04:50.504Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:04:50.523Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:05:50.365Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:05:50.365Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:05:50.368Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:05:50.368Z] [LOG] [API] Checking server status...
[2025-10-26T22:05:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:05:50.371Z"
}
[2025-10-26T22:05:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T22:05:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:05:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:05:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:05:50.409Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:06:50.356Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:06:50.356Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:06:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:06:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T22:06:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:06:50.402Z"
}
[2025-10-26T22:06:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T22:06:50.419Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:06:50.419Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:06:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:06:50.426Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:07:07.934Z] [WARN] {"timestamp":"2025-10-26T22:07:07.933Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-26T22:07:12.934Z] [INFO] {"timestamp":"2025-10-26T22:07:12.934Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-26T22:07:12.934Z] [INFO] {"timestamp":"2025-10-26T22:07:12.934Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T22:07:12.938Z] [INFO] {"timestamp":"2025-10-26T22:07:12.938Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-26T22:07:12.940Z] [INFO] {"timestamp":"2025-10-26T22:07:12.939Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-26T22:07:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:07:50.361Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:07:50.364Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:07:50.364Z] [LOG] [API] Checking server status...
[2025-10-26T22:07:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:07:50.367Z"
}
[2025-10-26T22:07:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T22:07:50.392Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:07:50.392Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:07:50.393Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:07:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:08:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:08:50.366Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:08:50.399Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:08:50.400Z] [LOG] [API] Checking server status...
[2025-10-26T22:08:50.411Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:08:50.402Z"
}
[2025-10-26T22:08:50.411Z] [LOG] [API] WebGPU available
[2025-10-26T22:08:50.418Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:08:50.418Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:08:50.419Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:08:50.446Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:09:33.178Z] [WARN] {"timestamp":"2025-10-26T22:09:33.178Z","level":"WARN","message":"[WebRTCSwarm] Disconnected from signaling server","details":{}}
[2025-10-26T22:09:38.178Z] [INFO] {"timestamp":"2025-10-26T22:09:38.178Z","level":"INFO","message":"[WebRTCSwarm] Attempting to reconnect to signaling server","details":{}}
[2025-10-26T22:09:38.179Z] [INFO] {"timestamp":"2025-10-26T22:09:38.179Z","level":"INFO","message":"[WebRTCSwarm] Connecting to signaling server: ws://localhost:8000/signaling","details":{}}
[2025-10-26T22:09:38.182Z] [INFO] {"timestamp":"2025-10-26T22:09:38.182Z","level":"INFO","message":"[WebRTCSwarm] Connected to signaling server","details":{}}
[2025-10-26T22:09:38.184Z] [INFO] {"timestamp":"2025-10-26T22:09:38.184Z","level":"INFO","message":"[WebRTCSwarm] Joined room reploid-swarm-default","details":{}}
[2025-10-26T22:09:50.369Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:09:50.369Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:09:50.373Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:09:50.373Z] [LOG] [API] Checking server status...
[2025-10-26T22:09:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:09:50.375Z"
}
[2025-10-26T22:09:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T22:09:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:09:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:09:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:09:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:10:50.455Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:10:50.455Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:10:50.580Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:10:50.580Z] [LOG] [API] Checking server status...
[2025-10-26T22:10:50.592Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:10:50.581Z"
}
[2025-10-26T22:10:50.592Z] [LOG] [API] WebGPU available
[2025-10-26T22:10:50.599Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:10:50.599Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:10:50.599Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:10:50.612Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:10:52.537Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:10:52.537Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:10:52.543Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:10:52.544Z] [LOG] [API] Checking server status...
[2025-10-26T22:10:52.549Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:10:52.544Z"
}
[2025-10-26T22:10:52.549Z] [LOG] [API] WebGPU available
[2025-10-26T22:10:52.556Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:10:52.556Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:10:52.556Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:10:52.573Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:11:05.327Z] [LOG] [Boot] Boot mode selected: meta
[2025-10-26T22:11:09.668Z] [LOG] [Boot] awakenAgent() called
[2025-10-26T22:11:09.668Z] [LOG] [Boot] Goal from input: Create tools to make tools to make tools
[2025-10-26T22:11:09.668Z] [LOG] [Boot] Awakening agent with goal: Create tools to make tools to make tools
[2025-10-26T22:11:09.673Z] [LOG] [Boot] Saving genesis boot state to VFS: {
  "timestamp": "2025-10-26T22:11:09.673Z",
  "goal": "Create tools to make tools to make tools",
  "bootMode": "meta",
  "consensusType": "arena",
  "selectedModels": [
    {
      "id": "gpt-oss:120b",
      "name": "gpt-oss:120b",
      "provider": "ollama",
      "hostType": "ollama-proxy",
      "queryMethod": "proxy",
      "keySource": "none",
      "keyId": null
    }
  ],
  "deploymentMode": "cloud",
  "environment": {
    "hasProxy": true,
    "hasWebGPU": true,
    "hasServer": true
  }
}
[2025-10-26T22:11:09.678Z] [LOG] [Boot] Genesis boot state saved to VFS successfully
[2025-10-26T22:11:09.678Z] [LOG] [BootLoader] Initializing VFS and loading modules...
[2025-10-26T22:11:09.678Z] [LOG] [BootLoader] Fetching module manifest...
[2025-10-26T22:11:09.683Z] [LOG] [BootLoader] Manifest loaded: 1.0 with 13 load groups
[2025-10-26T22:11:09.683Z] [LOG] [BootLoader] Boot mode: meta - Loading 18 modules
[2025-10-26T22:11:09.683Z] [LOG] [BootLoader] Fetching config.json...
[2025-10-26T22:11:09.686Z] [LOG] [BootLoader] Fetching 18 module files from server...
[2025-10-26T22:11:09.704Z] [LOG] [BootLoader] Successfully fetched 18 modules
[2025-10-26T22:11:09.704Z] [LOG] [BootLoader] Initializing GitVFS...
[2025-10-26T22:11:09.705Z] [ERROR] [Boot] Failed to awaken agent: TypeError: LightningFS is not a constructor
TypeError: LightningFS is not a constructor
    at HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:705:20)
[2025-10-26T22:11:50.353Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:11:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:11:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:11:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T22:11:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:11:50.397Z"
}
[2025-10-26T22:11:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T22:11:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:11:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:11:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:11:50.433Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:12:50.367Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:12:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:12:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:12:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T22:12:50.405Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:12:50.397Z"
}
[2025-10-26T22:12:50.405Z] [LOG] [API] WebGPU available
[2025-10-26T22:12:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:12:50.411Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:12:50.411Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:12:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:13:50.357Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:13:50.357Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:13:50.360Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:13:50.360Z] [LOG] [API] Checking server status...
[2025-10-26T22:13:50.371Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:13:50.362Z"
}
[2025-10-26T22:13:50.371Z] [LOG] [API] WebGPU available
[2025-10-26T22:13:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:13:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:13:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:13:50.397Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:13:56.444Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:13:56.444Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:13:56.487Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:13:56.487Z] [LOG] [API] Checking server status...
[2025-10-26T22:13:56.495Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:13:56.489Z"
}
[2025-10-26T22:13:56.495Z] [LOG] [API] WebGPU available
[2025-10-26T22:13:56.500Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:13:56.500Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:13:56.500Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:13:56.523Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:14:00.135Z] [LOG] [Boot] awakenAgent() called
[2025-10-26T22:14:00.135Z] [LOG] [Boot] Goal from input: Create tools to make tools to make tools
[2025-10-26T22:14:00.135Z] [LOG] [Boot] Awakening agent with goal: Create tools to make tools to make tools
[2025-10-26T22:14:00.144Z] [LOG] [Boot] Saving genesis boot state to VFS: {
  "timestamp": "2025-10-26T22:14:00.143Z",
  "goal": "Create tools to make tools to make tools",
  "bootMode": "meta",
  "consensusType": "arena",
  "selectedModels": [
    {
      "id": "gpt-oss:120b",
      "name": "gpt-oss:120b",
      "provider": "ollama",
      "hostType": "ollama-proxy",
      "queryMethod": "proxy",
      "keySource": "none",
      "keyId": null
    }
  ],
  "deploymentMode": "cloud",
  "environment": {
    "hasProxy": true,
    "hasWebGPU": true,
    "hasServer": true
  }
}
[2025-10-26T22:14:00.148Z] [LOG] [Boot] Genesis boot state saved to VFS successfully
[2025-10-26T22:14:00.148Z] [LOG] [BootLoader] Initializing VFS and loading modules...
[2025-10-26T22:14:00.149Z] [LOG] [BootLoader] Fetching module manifest...
[2025-10-26T22:14:00.152Z] [LOG] [BootLoader] Manifest loaded: 1.0 with 13 load groups
[2025-10-26T22:14:00.152Z] [LOG] [BootLoader] Boot mode: meta - Loading 18 modules
[2025-10-26T22:14:00.152Z] [LOG] [BootLoader] Fetching config.json...
[2025-10-26T22:14:00.155Z] [LOG] [BootLoader] Fetching 18 module files from server...
[2025-10-26T22:14:00.172Z] [LOG] [BootLoader] Successfully fetched 18 modules
[2025-10-26T22:14:00.172Z] [LOG] [BootLoader] Initializing GitVFS...
[2025-10-26T22:14:00.175Z] [LOG] [BootLoader] Writing 18 modules to VFS...
[2025-10-26T22:14:00.183Z] [LOG] [BootLoader] All modules written to VFS
[2025-10-26T22:14:00.255Z] [LOG] [BootLoader] Committed modules to Git
[2025-10-26T22:14:00.255Z] [LOG] [BootLoader] VFS ready - all modules available in IndexedDB
[2025-10-26T22:14:00.255Z] [LOG] [Boot] Loading app-logic.js from VFS...
[2025-10-26T22:14:00.255Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/app-logic.js
[2025-10-26T22:14:00.256Z] [ERROR] [VFS] Error reading /upgrades/app-logic.js: Error: ENOENT: /upgrades/app-logic.js
Error: /upgrades/app-logic.js
    at t.exports._lookup (https://unpkg.com/@isomorphic-git/lightning-fs:1:4506)
    at t.exports.stat (https://unpkg.com/@isomorphic-git/lightning-fs:1:5657)
    at t.exports.readFile (https://unpkg.com/@isomorphic-git/lightning-fs:1:7861)
    at t.exports.readFile (https://unpkg.com/@isomorphic-git/lightning-fs:1:15142)
    at t.exports.readFile (https://unpkg.com/@isomorphic-git/lightning-fs:1:14030)
    at async Object.read (http://localhost:8080/boot.js:757:37)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:776:30)
[2025-10-26T22:14:00.256Z] [ERROR] [Boot] Failed to awaken agent: Error: ENOENT: /upgrades/app-logic.js
Error: /upgrades/app-logic.js
    at t.exports._lookup (https://unpkg.com/@isomorphic-git/lightning-fs:1:4506)
    at t.exports.stat (https://unpkg.com/@isomorphic-git/lightning-fs:1:5657)
    at t.exports.readFile (https://unpkg.com/@isomorphic-git/lightning-fs:1:7861)
    at t.exports.readFile (https://unpkg.com/@isomorphic-git/lightning-fs:1:15142)
    at t.exports.readFile (https://unpkg.com/@isomorphic-git/lightning-fs:1:14030)
    at async Object.read (http://localhost:8080/boot.js:757:37)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:776:30)
[2025-10-26T22:14:50.325Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:14:50.325Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:14:50.357Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:14:50.357Z] [LOG] [API] Checking server status...
[2025-10-26T22:14:50.368Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:14:50.359Z"
}
[2025-10-26T22:14:50.368Z] [LOG] [API] WebGPU available
[2025-10-26T22:14:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:14:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:14:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:14:50.408Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:15:50.351Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:15:50.351Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:15:50.466Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:15:50.466Z] [LOG] [API] Checking server status...
[2025-10-26T22:15:50.475Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:15:50.467Z"
}
[2025-10-26T22:15:50.476Z] [LOG] [API] WebGPU available
[2025-10-26T22:15:50.479Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:15:50.479Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:15:50.479Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:15:50.488Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:16:50.464Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:16:50.464Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:16:50.466Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:16:50.466Z] [LOG] [API] Checking server status...
[2025-10-26T22:16:50.476Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:16:50.467Z"
}
[2025-10-26T22:16:50.476Z] [LOG] [API] WebGPU available
[2025-10-26T22:16:50.479Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:16:50.479Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:16:50.479Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:16:50.500Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:17:50.354Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:17:50.354Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:17:50.357Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:17:50.357Z] [LOG] [API] Checking server status...
[2025-10-26T22:17:50.370Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:17:50.359Z"
}
[2025-10-26T22:17:50.370Z] [LOG] [API] WebGPU available
[2025-10-26T22:17:50.388Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:17:50.388Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:17:50.388Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:17:50.414Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:18:37.908Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:18:37.908Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:18:37.912Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:18:37.912Z] [LOG] [API] Checking server status...
[2025-10-26T22:18:37.919Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:18:37.914Z"
}
[2025-10-26T22:18:37.920Z] [LOG] [API] WebGPU available
[2025-10-26T22:18:37.923Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:18:37.923Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:18:37.923Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:18:37.945Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:18:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:18:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:18:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:18:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T22:18:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:18:50.412Z"
}
[2025-10-26T22:18:50.422Z] [LOG] [API] WebGPU available
[2025-10-26T22:18:50.429Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:18:50.429Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:18:50.429Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:18:50.439Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:18:53.516Z] [INFO] {"timestamp":"2025-10-26T22:18:53.516Z","level":"INFO","message":"[DIContainer] Registered module: Persona","details":{}}
[2025-10-26T22:18:53.516Z] [INFO] {"timestamp":"2025-10-26T22:18:53.516Z","level":"INFO","message":"[CoreLogic] Loading module manifest...","details":{}}
[2025-10-26T22:18:53.516Z] [LOG] [VFS] Reading from IndexedDB: /module-manifest.json
[2025-10-26T22:18:53.516Z] [LOG] [VFS] Loaded: /module-manifest.json (10966 bytes)
[2025-10-26T22:18:53.516Z] [INFO] {"timestamp":"2025-10-26T22:18:53.516Z","level":"INFO","message":"[CoreLogic] Manifest version 1.0 loaded","details":{}}
[2025-10-26T22:18:53.516Z] [INFO] {"timestamp":"2025-10-26T22:18:53.516Z","level":"INFO","message":"[CoreLogic] Boot mode: meta, loading modules from preset","details":{}}
[2025-10-26T22:18:53.516Z] [INFO] {"timestamp":"2025-10-26T22:18:53.516Z","level":"INFO","message":"[CoreLogic] Loading 19 modules from 'meta' preset","details":{}}
[2025-10-26T22:18:53.516Z] [INFO] {"timestamp":"2025-10-26T22:18:53.516Z","level":"INFO","message":"[CoreLogic] Loading and registering all application modules...","details":{}}
[2025-10-26T22:18:53.516Z] [INFO] {"timestamp":"2025-10-26T22:18:53.516Z","level":"INFO","message":"[CoreLogic] Total module files to load: 19","details":{}}
[2025-10-26T22:18:53.516Z] [INFO] {"timestamp":"2025-10-26T22:18:53.516Z","level":"INFO","message":"[CoreLogic] Module list includes agent-logic-pure: true","details":{}}
[2025-10-26T22:18:53.516Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/utils.js
[2025-10-26T22:18:53.516Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/event-bus.js
[2025-10-26T22:18:53.516Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/di-container.js
[2025-10-26T22:18:53.516Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/storage-indexeddb.js
[2025-10-26T22:18:53.516Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/audit-logger.js
[2025-10-26T22:18:53.516Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/state-manager.js
[2025-10-26T22:18:53.516Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/api-client.js
[2025-10-26T22:18:53.516Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/tool-runner.js
[2025-10-26T22:18:53.517Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/app-logic.js
[2025-10-26T22:18:53.517Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/agent-logic-pure.js
[2025-10-26T22:18:53.517Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/context-manager.js
[2025-10-26T22:18:53.517Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/streaming-response-handler.js
[2025-10-26T22:18:53.517Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/reflection-store.js
[2025-10-26T22:18:53.517Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/performance-monitor.js
[2025-10-26T22:18:53.517Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-fsm.js
[2025-10-26T22:18:53.517Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-tools.js
[2025-10-26T22:18:53.517Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/meta-tool-creator.js
[2025-10-26T22:18:53.517Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/model-arena.js
[2025-10-26T22:18:53.517Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/ui-manager.js
[2025-10-26T22:18:53.518Z] [LOG] [VFS] Loaded: /upgrades/utils.js (23523 bytes)
[2025-10-26T22:18:53.518Z] [LOG] [VFS] Loaded: /upgrades/event-bus.js (10766 bytes)
[2025-10-26T22:18:53.518Z] [LOG] [VFS] Loaded: /upgrades/di-container.js (17553 bytes)
[2025-10-26T22:18:53.518Z] [LOG] [VFS] Loaded: /upgrades/storage-indexeddb.js (13962 bytes)
[2025-10-26T22:18:53.518Z] [LOG] [VFS] Loaded: /upgrades/audit-logger.js (20795 bytes)
[2025-10-26T22:18:53.518Z] [LOG] [VFS] Loaded: /upgrades/state-manager.js (35019 bytes)
[2025-10-26T22:18:53.518Z] [LOG] [VFS] Loaded: /upgrades/api-client.js (21290 bytes)
[2025-10-26T22:18:53.519Z] [LOG] [VFS] Loaded: /upgrades/tool-runner.js (40773 bytes)
[2025-10-26T22:18:53.519Z] [LOG] [VFS] Loaded: /upgrades/app-logic.js (28614 bytes)
[2025-10-26T22:18:53.519Z] [LOG] [VFS] Loaded: /upgrades/agent-logic-pure.js (6050 bytes)
[2025-10-26T22:18:53.519Z] [LOG] [VFS] Loaded: /upgrades/context-manager.js (20606 bytes)
[2025-10-26T22:18:53.519Z] [LOG] [VFS] Loaded: /upgrades/streaming-response-handler.js (14028 bytes)
[2025-10-26T22:18:53.519Z] [LOG] [VFS] Loaded: /upgrades/reflection-store.js (25016 bytes)
[2025-10-26T22:18:53.519Z] [LOG] [VFS] Loaded: /upgrades/performance-monitor.js (22524 bytes)
[2025-10-26T22:18:53.520Z] [LOG] [VFS] Loaded: /upgrades/sentinel-fsm.js (40706 bytes)
[2025-10-26T22:18:53.520Z] [LOG] [VFS] Loaded: /upgrades/sentinel-tools.js (28407 bytes)
[2025-10-26T22:18:53.520Z] [LOG] [VFS] Loaded: /upgrades/meta-tool-creator.js (22986 bytes)
[2025-10-26T22:18:53.520Z] [LOG] [VFS] Loaded: /upgrades/model-arena.js (34140 bytes)
[2025-10-26T22:18:53.521Z] [LOG] [VFS] Loaded: /upgrades/ui-manager.js (113293 bytes)
[2025-10-26T22:18:53.521Z] [INFO] {"timestamp":"2025-10-26T22:18:53.521Z","level":"INFO","message":"[CoreLogic] Module contents loaded: 19 items","details":{}}
[2025-10-26T22:18:53.521Z] [INFO] {"timestamp":"2025-10-26T22:18:53.521Z","level":"INFO","message":"[CoreLogic] agent-logic-pure content length: 6050","details":{}}
[2025-10-26T22:18:53.522Z] [LOG] [CoreLogic] Loaded module from /upgrades/utils.js: Utils
[2025-10-26T22:18:53.522Z] [INFO] {"timestamp":"2025-10-26T22:18:53.522Z","level":"INFO","message":"[DIContainer] Registered module: Utils","details":{}}
[2025-10-26T22:18:53.522Z] [INFO] {"timestamp":"2025-10-26T22:18:53.522Z","level":"INFO","message":"[CoreLogic] Registered module: Utils from /upgrades/utils.js","details":{}}
[2025-10-26T22:18:53.523Z] [INFO] {"timestamp":"2025-10-26T22:18:53.523Z","level":"INFO","message":"[DIContainer] Registered module: EventBus","details":{}}
[2025-10-26T22:18:53.523Z] [INFO] {"timestamp":"2025-10-26T22:18:53.523Z","level":"INFO","message":"[CoreLogic] Registered module: EventBus from /upgrades/event-bus.js","details":{}}
[2025-10-26T22:18:53.524Z] [LOG] [CoreLogic] Loaded module from /upgrades/di-container.js: DIContainer
[2025-10-26T22:18:53.524Z] [INFO] {"timestamp":"2025-10-26T22:18:53.524Z","level":"INFO","message":"[DIContainer] Registered module: DIContainer","details":{}}
[2025-10-26T22:18:53.524Z] [INFO] {"timestamp":"2025-10-26T22:18:53.524Z","level":"INFO","message":"[CoreLogic] Registered module: DIContainer from /upgrades/di-container.js","details":{}}
[2025-10-26T22:18:53.525Z] [INFO] {"timestamp":"2025-10-26T22:18:53.525Z","level":"INFO","message":"[DIContainer] Registered module: Storage","details":{}}
[2025-10-26T22:18:53.525Z] [INFO] {"timestamp":"2025-10-26T22:18:53.525Z","level":"INFO","message":"[CoreLogic] Registered module: Storage from /upgrades/storage-indexeddb.js","details":{}}
[2025-10-26T22:18:53.526Z] [INFO] {"timestamp":"2025-10-26T22:18:53.526Z","level":"INFO","message":"[DIContainer] Registered module: AuditLogger","details":{}}
[2025-10-26T22:18:53.526Z] [INFO] {"timestamp":"2025-10-26T22:18:53.526Z","level":"INFO","message":"[CoreLogic] Registered module: AuditLogger from /upgrades/audit-logger.js","details":{}}
[2025-10-26T22:18:53.527Z] [LOG] [CoreLogic] Loaded module from /upgrades/state-manager.js: StateManager
[2025-10-26T22:18:53.527Z] [INFO] {"timestamp":"2025-10-26T22:18:53.527Z","level":"INFO","message":"[DIContainer] Registered module: StateManager","details":{}}
[2025-10-26T22:18:53.527Z] [INFO] {"timestamp":"2025-10-26T22:18:53.527Z","level":"INFO","message":"[CoreLogic] Registered module: StateManager from /upgrades/state-manager.js","details":{}}
[2025-10-26T22:18:53.528Z] [INFO] {"timestamp":"2025-10-26T22:18:53.528Z","level":"INFO","message":"[DIContainer] Registered module: ApiClient","details":{}}
[2025-10-26T22:18:53.528Z] [INFO] {"timestamp":"2025-10-26T22:18:53.528Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T22:18:53.530Z] [INFO] {"timestamp":"2025-10-26T22:18:53.530Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T22:18:53.530Z] [INFO] {"timestamp":"2025-10-26T22:18:53.530Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T22:18:53.531Z] [LOG] [CoreLogic] Loaded module from /upgrades/app-logic.js: NO_METADATA
[2025-10-26T22:18:53.531Z] [ERROR] {"timestamp":"2025-10-26T22:18:53.531Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/app-logic.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T22:18:53.531Z] [WARN] {"timestamp":"2025-10-26T22:18:53.531Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/app-logic.js missing metadata. Module:","details":{}}
[2025-10-26T22:18:53.531Z] [INFO] {"timestamp":"2025-10-26T22:18:53.531Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T22:18:53.532Z] [INFO] {"timestamp":"2025-10-26T22:18:53.532Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T22:18:53.532Z] [INFO] {"timestamp":"2025-10-26T22:18:53.532Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T22:18:53.532Z] [INFO] {"timestamp":"2025-10-26T22:18:53.532Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T22:18:53.534Z] [INFO] {"timestamp":"2025-10-26T22:18:53.534Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T22:18:53.534Z] [INFO] {"timestamp":"2025-10-26T22:18:53.534Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T22:18:53.535Z] [INFO] {"timestamp":"2025-10-26T22:18:53.535Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T22:18:53.535Z] [INFO] {"timestamp":"2025-10-26T22:18:53.535Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T22:18:53.536Z] [INFO] {"timestamp":"2025-10-26T22:18:53.536Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T22:18:53.536Z] [INFO] {"timestamp":"2025-10-26T22:18:53.536Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T22:18:53.537Z] [INFO] {"timestamp":"2025-10-26T22:18:53.537Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T22:18:53.537Z] [INFO] {"timestamp":"2025-10-26T22:18:53.537Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T22:18:53.539Z] [INFO] {"timestamp":"2025-10-26T22:18:53.539Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T22:18:53.539Z] [INFO] {"timestamp":"2025-10-26T22:18:53.539Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T22:18:53.540Z] [INFO] {"timestamp":"2025-10-26T22:18:53.540Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T22:18:53.540Z] [INFO] {"timestamp":"2025-10-26T22:18:53.540Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T22:18:53.542Z] [INFO] {"timestamp":"2025-10-26T22:18:53.542Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T22:18:53.542Z] [INFO] {"timestamp":"2025-10-26T22:18:53.542Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T22:18:53.543Z] [INFO] {"timestamp":"2025-10-26T22:18:53.543Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T22:18:53.543Z] [INFO] {"timestamp":"2025-10-26T22:18:53.543Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T22:18:53.546Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T22:18:53.546Z] [INFO] {"timestamp":"2025-10-26T22:18:53.546Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T22:18:53.546Z] [INFO] {"timestamp":"2025-10-26T22:18:53.546Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T22:18:53.546Z] [INFO] {"timestamp":"2025-10-26T22:18:53.546Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T22:18:53.546Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Service not found: CycleLogic
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Error: [DIContainer] Service not found: CycleLogic
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:61:23)
    at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:40)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:18:53.546Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:18:53.546Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Service not found: CycleLogic
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Error: [DIContainer] Service not found: CycleLogic
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:61:23)
    at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:40)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:18:53.546Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:19:06.590Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:19:06.591Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:19:06.595Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:19:06.595Z] [LOG] [API] Checking server status...
[2025-10-26T22:19:06.603Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:19:06.597Z"
}
[2025-10-26T22:19:06.604Z] [LOG] [API] WebGPU available
[2025-10-26T22:19:06.610Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:19:06.610Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:19:06.610Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:19:06.633Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:19:09.794Z] [INFO] {"timestamp":"2025-10-26T22:19:09.794Z","level":"INFO","message":"[DIContainer] Registered module: Persona","details":{}}
[2025-10-26T22:19:09.794Z] [INFO] {"timestamp":"2025-10-26T22:19:09.794Z","level":"INFO","message":"[CoreLogic] Loading module manifest...","details":{}}
[2025-10-26T22:19:09.794Z] [LOG] [VFS] Reading from IndexedDB: /module-manifest.json
[2025-10-26T22:19:09.794Z] [LOG] [VFS] Loaded: /module-manifest.json (10966 bytes)
[2025-10-26T22:19:09.794Z] [INFO] {"timestamp":"2025-10-26T22:19:09.794Z","level":"INFO","message":"[CoreLogic] Manifest version 1.0 loaded","details":{}}
[2025-10-26T22:19:09.794Z] [INFO] {"timestamp":"2025-10-26T22:19:09.794Z","level":"INFO","message":"[CoreLogic] Boot mode: meta, loading modules from preset","details":{}}
[2025-10-26T22:19:09.794Z] [INFO] {"timestamp":"2025-10-26T22:19:09.794Z","level":"INFO","message":"[CoreLogic] Loading 19 modules from 'meta' preset","details":{}}
[2025-10-26T22:19:09.794Z] [INFO] {"timestamp":"2025-10-26T22:19:09.794Z","level":"INFO","message":"[CoreLogic] Loading and registering all application modules...","details":{}}
[2025-10-26T22:19:09.794Z] [INFO] {"timestamp":"2025-10-26T22:19:09.794Z","level":"INFO","message":"[CoreLogic] Total module files to load: 19","details":{}}
[2025-10-26T22:19:09.794Z] [INFO] {"timestamp":"2025-10-26T22:19:09.794Z","level":"INFO","message":"[CoreLogic] Module list includes agent-logic-pure: true","details":{}}
[2025-10-26T22:19:09.794Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/utils.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/event-bus.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/di-container.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/storage-indexeddb.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/audit-logger.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/state-manager.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/api-client.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/tool-runner.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/app-logic.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/agent-logic-pure.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/context-manager.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/streaming-response-handler.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/reflection-store.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/performance-monitor.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-fsm.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-tools.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/meta-tool-creator.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/model-arena.js
[2025-10-26T22:19:09.795Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/ui-manager.js
[2025-10-26T22:19:09.796Z] [LOG] [VFS] Loaded: /upgrades/utils.js (23523 bytes)
[2025-10-26T22:19:09.796Z] [LOG] [VFS] Loaded: /upgrades/event-bus.js (10766 bytes)
[2025-10-26T22:19:09.796Z] [LOG] [VFS] Loaded: /upgrades/di-container.js (17553 bytes)
[2025-10-26T22:19:09.796Z] [LOG] [VFS] Loaded: /upgrades/storage-indexeddb.js (13962 bytes)
[2025-10-26T22:19:09.796Z] [LOG] [VFS] Loaded: /upgrades/audit-logger.js (20795 bytes)
[2025-10-26T22:19:09.797Z] [LOG] [VFS] Loaded: /upgrades/state-manager.js (35019 bytes)
[2025-10-26T22:19:09.797Z] [LOG] [VFS] Loaded: /upgrades/api-client.js (21290 bytes)
[2025-10-26T22:19:09.797Z] [LOG] [VFS] Loaded: /upgrades/tool-runner.js (40773 bytes)
[2025-10-26T22:19:09.798Z] [LOG] [VFS] Loaded: /upgrades/app-logic.js (28614 bytes)
[2025-10-26T22:19:09.798Z] [LOG] [VFS] Loaded: /upgrades/agent-logic-pure.js (6050 bytes)
[2025-10-26T22:19:09.798Z] [LOG] [VFS] Loaded: /upgrades/context-manager.js (20606 bytes)
[2025-10-26T22:19:09.798Z] [LOG] [VFS] Loaded: /upgrades/streaming-response-handler.js (14028 bytes)
[2025-10-26T22:19:09.798Z] [LOG] [VFS] Loaded: /upgrades/reflection-store.js (25016 bytes)
[2025-10-26T22:19:09.798Z] [LOG] [VFS] Loaded: /upgrades/performance-monitor.js (22524 bytes)
[2025-10-26T22:19:09.799Z] [LOG] [VFS] Loaded: /upgrades/sentinel-fsm.js (40706 bytes)
[2025-10-26T22:19:09.799Z] [LOG] [VFS] Loaded: /upgrades/sentinel-tools.js (28407 bytes)
[2025-10-26T22:19:09.799Z] [LOG] [VFS] Loaded: /upgrades/meta-tool-creator.js (22986 bytes)
[2025-10-26T22:19:09.799Z] [LOG] [VFS] Loaded: /upgrades/model-arena.js (34140 bytes)
[2025-10-26T22:19:09.799Z] [LOG] [VFS] Loaded: /upgrades/ui-manager.js (113293 bytes)
[2025-10-26T22:19:09.800Z] [INFO] {"timestamp":"2025-10-26T22:19:09.799Z","level":"INFO","message":"[CoreLogic] Module contents loaded: 19 items","details":{}}
[2025-10-26T22:19:09.800Z] [INFO] {"timestamp":"2025-10-26T22:19:09.800Z","level":"INFO","message":"[CoreLogic] agent-logic-pure content length: 6050","details":{}}
[2025-10-26T22:19:09.800Z] [LOG] [CoreLogic] Loaded module from /upgrades/utils.js: Utils
[2025-10-26T22:19:09.801Z] [INFO] {"timestamp":"2025-10-26T22:19:09.800Z","level":"INFO","message":"[DIContainer] Registered module: Utils","details":{}}
[2025-10-26T22:19:09.801Z] [INFO] {"timestamp":"2025-10-26T22:19:09.801Z","level":"INFO","message":"[CoreLogic] Registered module: Utils from /upgrades/utils.js","details":{}}
[2025-10-26T22:19:09.802Z] [INFO] {"timestamp":"2025-10-26T22:19:09.802Z","level":"INFO","message":"[DIContainer] Registered module: EventBus","details":{}}
[2025-10-26T22:19:09.802Z] [INFO] {"timestamp":"2025-10-26T22:19:09.802Z","level":"INFO","message":"[CoreLogic] Registered module: EventBus from /upgrades/event-bus.js","details":{}}
[2025-10-26T22:19:09.803Z] [LOG] [CoreLogic] Loaded module from /upgrades/di-container.js: DIContainer
[2025-10-26T22:19:09.803Z] [INFO] {"timestamp":"2025-10-26T22:19:09.803Z","level":"INFO","message":"[DIContainer] Registered module: DIContainer","details":{}}
[2025-10-26T22:19:09.803Z] [INFO] {"timestamp":"2025-10-26T22:19:09.803Z","level":"INFO","message":"[CoreLogic] Registered module: DIContainer from /upgrades/di-container.js","details":{}}
[2025-10-26T22:19:09.804Z] [INFO] {"timestamp":"2025-10-26T22:19:09.804Z","level":"INFO","message":"[DIContainer] Registered module: Storage","details":{}}
[2025-10-26T22:19:09.804Z] [INFO] {"timestamp":"2025-10-26T22:19:09.804Z","level":"INFO","message":"[CoreLogic] Registered module: Storage from /upgrades/storage-indexeddb.js","details":{}}
[2025-10-26T22:19:09.805Z] [INFO] {"timestamp":"2025-10-26T22:19:09.805Z","level":"INFO","message":"[DIContainer] Registered module: AuditLogger","details":{}}
[2025-10-26T22:19:09.805Z] [INFO] {"timestamp":"2025-10-26T22:19:09.805Z","level":"INFO","message":"[CoreLogic] Registered module: AuditLogger from /upgrades/audit-logger.js","details":{}}
[2025-10-26T22:19:09.806Z] [LOG] [CoreLogic] Loaded module from /upgrades/state-manager.js: StateManager
[2025-10-26T22:19:09.806Z] [INFO] {"timestamp":"2025-10-26T22:19:09.806Z","level":"INFO","message":"[DIContainer] Registered module: StateManager","details":{}}
[2025-10-26T22:19:09.806Z] [INFO] {"timestamp":"2025-10-26T22:19:09.806Z","level":"INFO","message":"[CoreLogic] Registered module: StateManager from /upgrades/state-manager.js","details":{}}
[2025-10-26T22:19:09.808Z] [INFO] {"timestamp":"2025-10-26T22:19:09.808Z","level":"INFO","message":"[DIContainer] Registered module: ApiClient","details":{}}
[2025-10-26T22:19:09.808Z] [INFO] {"timestamp":"2025-10-26T22:19:09.808Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T22:19:09.810Z] [INFO] {"timestamp":"2025-10-26T22:19:09.810Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T22:19:09.810Z] [INFO] {"timestamp":"2025-10-26T22:19:09.810Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T22:19:09.810Z] [LOG] [CoreLogic] Loaded module from /upgrades/app-logic.js: NO_METADATA
[2025-10-26T22:19:09.810Z] [ERROR] {"timestamp":"2025-10-26T22:19:09.810Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/app-logic.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T22:19:09.811Z] [WARN] {"timestamp":"2025-10-26T22:19:09.810Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/app-logic.js missing metadata. Module:","details":{}}
[2025-10-26T22:19:09.811Z] [INFO] {"timestamp":"2025-10-26T22:19:09.811Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T22:19:09.811Z] [INFO] {"timestamp":"2025-10-26T22:19:09.811Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T22:19:09.811Z] [INFO] {"timestamp":"2025-10-26T22:19:09.811Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T22:19:09.811Z] [INFO] {"timestamp":"2025-10-26T22:19:09.811Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T22:19:09.813Z] [INFO] {"timestamp":"2025-10-26T22:19:09.813Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T22:19:09.813Z] [INFO] {"timestamp":"2025-10-26T22:19:09.813Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T22:19:09.814Z] [INFO] {"timestamp":"2025-10-26T22:19:09.814Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T22:19:09.814Z] [INFO] {"timestamp":"2025-10-26T22:19:09.814Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T22:19:09.815Z] [INFO] {"timestamp":"2025-10-26T22:19:09.815Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T22:19:09.815Z] [INFO] {"timestamp":"2025-10-26T22:19:09.815Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T22:19:09.817Z] [INFO] {"timestamp":"2025-10-26T22:19:09.816Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T22:19:09.817Z] [INFO] {"timestamp":"2025-10-26T22:19:09.817Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T22:19:09.819Z] [INFO] {"timestamp":"2025-10-26T22:19:09.819Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T22:19:09.819Z] [INFO] {"timestamp":"2025-10-26T22:19:09.819Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T22:19:09.820Z] [INFO] {"timestamp":"2025-10-26T22:19:09.820Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T22:19:09.821Z] [INFO] {"timestamp":"2025-10-26T22:19:09.820Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T22:19:09.822Z] [INFO] {"timestamp":"2025-10-26T22:19:09.822Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T22:19:09.822Z] [INFO] {"timestamp":"2025-10-26T22:19:09.822Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T22:19:09.824Z] [INFO] {"timestamp":"2025-10-26T22:19:09.824Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T22:19:09.824Z] [INFO] {"timestamp":"2025-10-26T22:19:09.824Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T22:19:09.827Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T22:19:09.827Z] [INFO] {"timestamp":"2025-10-26T22:19:09.827Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T22:19:09.827Z] [INFO] {"timestamp":"2025-10-26T22:19:09.827Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T22:19:09.827Z] [INFO] {"timestamp":"2025-10-26T22:19:09.827Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T22:19:09.827Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Service not found: CycleLogic
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Error: [DIContainer] Service not found: CycleLogic
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:61:23)
    at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:40)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:19:09.827Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:19:09.827Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Service not found: CycleLogic
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Error: [DIContainer] Service not found: CycleLogic
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:61:23)
    at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:40)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:19:09.827Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:19:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:19:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:19:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:19:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T22:19:50.376Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:19:50.369Z"
}
[2025-10-26T22:19:50.376Z] [LOG] [API] WebGPU available
[2025-10-26T22:19:50.395Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:19:50.395Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:19:50.395Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:19:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:20:50.344Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:20:50.344Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:20:50.411Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:20:50.411Z] [LOG] [API] Checking server status...
[2025-10-26T22:20:50.422Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:20:50.412Z"
}
[2025-10-26T22:20:50.422Z] [LOG] [API] WebGPU available
[2025-10-26T22:20:50.430Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:20:50.430Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:20:50.430Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:20:50.443Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:21:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:21:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:21:50.363Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:21:50.363Z] [LOG] [API] Checking server status...
[2025-10-26T22:21:50.373Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:21:50.365Z"
}
[2025-10-26T22:21:50.373Z] [LOG] [API] WebGPU available
[2025-10-26T22:21:50.391Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:21:50.391Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:21:50.391Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:21:50.405Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:22:50.562Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:22:50.562Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:22:50.565Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:22:50.565Z] [LOG] [API] Checking server status...
[2025-10-26T22:22:50.574Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:22:50.566Z"
}
[2025-10-26T22:22:50.574Z] [LOG] [API] WebGPU available
[2025-10-26T22:22:50.582Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:22:50.582Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:22:50.582Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:22:50.597Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:23:50.368Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:23:50.368Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:23:50.371Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:23:50.371Z] [LOG] [API] Checking server status...
[2025-10-26T22:23:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:23:50.373Z"
}
[2025-10-26T22:23:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T22:23:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:23:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:23:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:23:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:24:50.278Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:24:50.279Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:24:50.301Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:24:50.301Z] [LOG] [API] Checking server status...
[2025-10-26T22:24:50.311Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:24:50.303Z"
}
[2025-10-26T22:24:50.311Z] [LOG] [API] WebGPU available
[2025-10-26T22:24:50.317Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:24:50.317Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:24:50.318Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:24:50.345Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:25:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:25:50.364Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:25:50.366Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:25:50.366Z] [LOG] [API] Checking server status...
[2025-10-26T22:25:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:25:50.368Z"
}
[2025-10-26T22:25:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T22:25:50.396Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:25:50.396Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:25:50.396Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:25:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:26:51.541Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:26:51.541Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:26:51.543Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:26:51.543Z] [LOG] [API] Checking server status...
[2025-10-26T22:26:51.565Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:26:51.547Z"
}
[2025-10-26T22:26:51.565Z] [LOG] [API] WebGPU available
[2025-10-26T22:26:51.569Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:26:51.569Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:26:51.569Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:26:51.575Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:27:01.148Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:27:01.183Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:27:01.195Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:27:01.195Z] [LOG] [API] Checking server status...
[2025-10-26T22:27:01.204Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:27:01.197Z"
}
[2025-10-26T22:27:01.204Z] [LOG] [API] WebGPU available
[2025-10-26T22:27:01.211Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:27:01.211Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:27:01.211Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:27:01.230Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:27:06.496Z] [INFO] {"timestamp":"2025-10-26T22:27:06.496Z","level":"INFO","message":"[CoreLogic] Manifest version 1.0 loaded","details":{}}
[2025-10-26T22:27:06.496Z] [INFO] {"timestamp":"2025-10-26T22:27:06.496Z","level":"INFO","message":"[CoreLogic] Boot mode: meta, loading modules from preset","details":{}}
[2025-10-26T22:27:06.496Z] [INFO] {"timestamp":"2025-10-26T22:27:06.496Z","level":"INFO","message":"[CoreLogic] Loading 20 modules from 'meta' preset","details":{}}
[2025-10-26T22:27:06.496Z] [INFO] {"timestamp":"2025-10-26T22:27:06.496Z","level":"INFO","message":"[CoreLogic] Loading and registering all application modules...","details":{}}
[2025-10-26T22:27:06.496Z] [INFO] {"timestamp":"2025-10-26T22:27:06.496Z","level":"INFO","message":"[CoreLogic] Total module files to load: 20","details":{}}
[2025-10-26T22:27:06.496Z] [INFO] {"timestamp":"2025-10-26T22:27:06.496Z","level":"INFO","message":"[CoreLogic] Module list includes agent-logic-pure: true","details":{}}
[2025-10-26T22:27:06.496Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/utils.js
[2025-10-26T22:27:06.496Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/event-bus.js
[2025-10-26T22:27:06.496Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/di-container.js
[2025-10-26T22:27:06.496Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/storage-indexeddb.js
[2025-10-26T22:27:06.496Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/audit-logger.js
[2025-10-26T22:27:06.496Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/state-manager.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/api-client.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/tool-runner.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/agent-logic-pure.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/agent-cycle.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/context-manager.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/streaming-response-handler.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/reflection-store.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/performance-monitor.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-fsm.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-tools.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/meta-tool-creator.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/model-arena.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/ui-manager.js
[2025-10-26T22:27:06.497Z] [LOG] [VFS] Reading from IndexedDB: /personas/CodeRefactorerPersona.js
[2025-10-26T22:27:06.498Z] [LOG] [VFS] Loaded: /upgrades/utils.js (23523 bytes)
[2025-10-26T22:27:06.498Z] [LOG] [VFS] Loaded: /upgrades/event-bus.js (10766 bytes)
[2025-10-26T22:27:06.498Z] [LOG] [VFS] Loaded: /upgrades/di-container.js (17553 bytes)
[2025-10-26T22:27:06.498Z] [LOG] [VFS] Loaded: /upgrades/storage-indexeddb.js (13962 bytes)
[2025-10-26T22:27:06.498Z] [LOG] [VFS] Loaded: /upgrades/audit-logger.js (20795 bytes)
[2025-10-26T22:27:06.498Z] [LOG] [VFS] Loaded: /upgrades/state-manager.js (35019 bytes)
[2025-10-26T22:27:06.498Z] [LOG] [VFS] Loaded: /upgrades/api-client.js (21290 bytes)
[2025-10-26T22:27:06.498Z] [LOG] [VFS] Loaded: /upgrades/tool-runner.js (40773 bytes)
[2025-10-26T22:27:06.499Z] [LOG] [VFS] Loaded: /upgrades/agent-logic-pure.js (6050 bytes)
[2025-10-26T22:27:06.499Z] [LOG] [VFS] Loaded: /upgrades/agent-cycle.js (14891 bytes)
[2025-10-26T22:27:06.499Z] [LOG] [VFS] Loaded: /upgrades/context-manager.js (20606 bytes)
[2025-10-26T22:27:06.499Z] [LOG] [VFS] Loaded: /upgrades/streaming-response-handler.js (14028 bytes)
[2025-10-26T22:27:06.499Z] [LOG] [VFS] Loaded: /upgrades/reflection-store.js (25016 bytes)
[2025-10-26T22:27:06.499Z] [LOG] [VFS] Loaded: /upgrades/performance-monitor.js (22524 bytes)
[2025-10-26T22:27:06.499Z] [LOG] [VFS] Loaded: /upgrades/sentinel-fsm.js (40706 bytes)
[2025-10-26T22:27:06.499Z] [LOG] [VFS] Loaded: /upgrades/sentinel-tools.js (28407 bytes)
[2025-10-26T22:27:06.499Z] [LOG] [VFS] Loaded: /upgrades/meta-tool-creator.js (22986 bytes)
[2025-10-26T22:27:06.499Z] [LOG] [VFS] Loaded: /upgrades/model-arena.js (34140 bytes)
[2025-10-26T22:27:06.500Z] [LOG] [VFS] Loaded: /upgrades/ui-manager.js (113293 bytes)
[2025-10-26T22:27:06.500Z] [LOG] [VFS] Loaded: /personas/CodeRefactorerPersona.js (5488 bytes)
[2025-10-26T22:27:06.500Z] [INFO] {"timestamp":"2025-10-26T22:27:06.500Z","level":"INFO","message":"[CoreLogic] Module contents loaded: 20 items","details":{}}
[2025-10-26T22:27:06.500Z] [INFO] {"timestamp":"2025-10-26T22:27:06.500Z","level":"INFO","message":"[CoreLogic] agent-logic-pure content length: 6050","details":{}}
[2025-10-26T22:27:06.501Z] [LOG] [CoreLogic] Loaded module from /upgrades/utils.js: Utils
[2025-10-26T22:27:06.501Z] [INFO] {"timestamp":"2025-10-26T22:27:06.501Z","level":"INFO","message":"[DIContainer] Registered module: Utils","details":{}}
[2025-10-26T22:27:06.501Z] [INFO] {"timestamp":"2025-10-26T22:27:06.501Z","level":"INFO","message":"[CoreLogic] Registered module: Utils from /upgrades/utils.js","details":{}}
[2025-10-26T22:27:06.502Z] [INFO] {"timestamp":"2025-10-26T22:27:06.502Z","level":"INFO","message":"[DIContainer] Registered module: EventBus","details":{}}
[2025-10-26T22:27:06.502Z] [INFO] {"timestamp":"2025-10-26T22:27:06.502Z","level":"INFO","message":"[CoreLogic] Registered module: EventBus from /upgrades/event-bus.js","details":{}}
[2025-10-26T22:27:06.503Z] [LOG] [CoreLogic] Loaded module from /upgrades/di-container.js: DIContainer
[2025-10-26T22:27:06.503Z] [INFO] {"timestamp":"2025-10-26T22:27:06.503Z","level":"INFO","message":"[DIContainer] Registered module: DIContainer","details":{}}
[2025-10-26T22:27:06.503Z] [INFO] {"timestamp":"2025-10-26T22:27:06.503Z","level":"INFO","message":"[CoreLogic] Registered module: DIContainer from /upgrades/di-container.js","details":{}}
[2025-10-26T22:27:06.504Z] [INFO] {"timestamp":"2025-10-26T22:27:06.504Z","level":"INFO","message":"[DIContainer] Registered module: Storage","details":{}}
[2025-10-26T22:27:06.504Z] [INFO] {"timestamp":"2025-10-26T22:27:06.504Z","level":"INFO","message":"[CoreLogic] Registered module: Storage from /upgrades/storage-indexeddb.js","details":{}}
[2025-10-26T22:27:06.506Z] [INFO] {"timestamp":"2025-10-26T22:27:06.505Z","level":"INFO","message":"[DIContainer] Registered module: AuditLogger","details":{}}
[2025-10-26T22:27:06.506Z] [INFO] {"timestamp":"2025-10-26T22:27:06.506Z","level":"INFO","message":"[CoreLogic] Registered module: AuditLogger from /upgrades/audit-logger.js","details":{}}
[2025-10-26T22:27:06.507Z] [LOG] [CoreLogic] Loaded module from /upgrades/state-manager.js: StateManager
[2025-10-26T22:27:06.507Z] [INFO] {"timestamp":"2025-10-26T22:27:06.507Z","level":"INFO","message":"[DIContainer] Registered module: StateManager","details":{}}
[2025-10-26T22:27:06.507Z] [INFO] {"timestamp":"2025-10-26T22:27:06.507Z","level":"INFO","message":"[CoreLogic] Registered module: StateManager from /upgrades/state-manager.js","details":{}}
[2025-10-26T22:27:06.508Z] [INFO] {"timestamp":"2025-10-26T22:27:06.508Z","level":"INFO","message":"[DIContainer] Registered module: ApiClient","details":{}}
[2025-10-26T22:27:06.508Z] [INFO] {"timestamp":"2025-10-26T22:27:06.508Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T22:27:06.509Z] [INFO] {"timestamp":"2025-10-26T22:27:06.509Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T22:27:06.509Z] [INFO] {"timestamp":"2025-10-26T22:27:06.509Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T22:27:06.509Z] [INFO] {"timestamp":"2025-10-26T22:27:06.509Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T22:27:06.511Z] [INFO] {"timestamp":"2025-10-26T22:27:06.511Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T22:27:06.511Z] [INFO] {"timestamp":"2025-10-26T22:27:06.511Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T22:27:06.511Z] [INFO] {"timestamp":"2025-10-26T22:27:06.511Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T22:27:06.513Z] [INFO] {"timestamp":"2025-10-26T22:27:06.513Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T22:27:06.513Z] [INFO] {"timestamp":"2025-10-26T22:27:06.513Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T22:27:06.515Z] [INFO] {"timestamp":"2025-10-26T22:27:06.515Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T22:27:06.515Z] [INFO] {"timestamp":"2025-10-26T22:27:06.515Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T22:27:06.516Z] [INFO] {"timestamp":"2025-10-26T22:27:06.516Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T22:27:06.516Z] [INFO] {"timestamp":"2025-10-26T22:27:06.516Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T22:27:06.518Z] [INFO] {"timestamp":"2025-10-26T22:27:06.518Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T22:27:06.518Z] [INFO] {"timestamp":"2025-10-26T22:27:06.518Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T22:27:06.520Z] [INFO] {"timestamp":"2025-10-26T22:27:06.520Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T22:27:06.520Z] [INFO] {"timestamp":"2025-10-26T22:27:06.520Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T22:27:06.522Z] [INFO] {"timestamp":"2025-10-26T22:27:06.522Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T22:27:06.522Z] [INFO] {"timestamp":"2025-10-26T22:27:06.522Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T22:27:06.527Z] [INFO] {"timestamp":"2025-10-26T22:27:06.527Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T22:27:06.527Z] [INFO] {"timestamp":"2025-10-26T22:27:06.527Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T22:27:06.533Z] [INFO] {"timestamp":"2025-10-26T22:27:06.533Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T22:27:06.533Z] [INFO] {"timestamp":"2025-10-26T22:27:06.533Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T22:27:06.535Z] [INFO] {"timestamp":"2025-10-26T22:27:06.535Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T22:27:06.535Z] [INFO] {"timestamp":"2025-10-26T22:27:06.535Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T22:27:06.537Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T22:27:06.538Z] [INFO] {"timestamp":"2025-10-26T22:27:06.538Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T22:27:06.538Z] [INFO] {"timestamp":"2025-10-26T22:27:06.538Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T22:27:06.539Z] [WARN] [CoreLogic] Failed to import ESM module at /personas/CodeRefactorerPersona.js; falling back to legacy evaluator. TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
[2025-10-26T22:27:06.539Z] [INFO] {"timestamp":"2025-10-26T22:27:06.539Z","level":"INFO","message":"[DIContainer] Registered module: code-refactorer-persona","details":{}}
[2025-10-26T22:27:06.539Z] [INFO] {"timestamp":"2025-10-26T22:27:06.539Z","level":"INFO","message":"[CoreLogic] Registered module: code-refactorer-persona from /personas/CodeRefactorerPersona.js","details":{}}
[2025-10-26T22:27:06.539Z] [INFO] {"timestamp":"2025-10-26T22:27:06.539Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T22:27:06.540Z] [INFO] {"timestamp":"2025-10-26T22:27:06.540Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T22:27:07.002Z] [INFO] {"timestamp":"2025-10-26T22:27:07.001Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T22:27:07.002Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Failed to resolve dependency 'StateManager' for module 'CycleLogic'.
Dependency chain: CycleLogic  StateManager
Original error: [DIContainer] Failed to resolve dependency 'StateHelpersPure' for module 'StateManager'.
Dependency chain: StateManager  StateHelpersPure
Original error: [DIContainer] Service not found: StateHelpersPure
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'StateManager' for module 'CycleLogic'.
Dependency chain: CycleLogic  StateManager
Original error: [DIContainer] Failed to resolve dependency 'StateHelpersPure' for module 'StateManager'.
Dependency chain: StateManager  StateHelpersPure
Original error: [DIContainer] Service not found: StateHelpersPure
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:787:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:787:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:807:9)
[2025-10-26T22:27:07.002Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Failed to resolve dependency 'StateManager' for module 'CycleLogic'.
Dependency chain: CycleLogic  StateManager
Original error: [DIContainer] Failed to resolve dependency 'StateHelpersPure' for module 'StateManager'.
Dependency chain: StateManager  StateHelpersPure
Original error: [DIContainer] Service not found: StateHelpersPure
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'StateManager' for module 'CycleLogic'.
Dependency chain: CycleLogic  StateManager
Original error: [DIContainer] Failed to resolve dependency 'StateHelpersPure' for module 'StateManager'.
Dependency chain: StateManager  StateHelpersPure
Original error: [DIContainer] Service not found: StateHelpersPure
Available services: config, Persona, Utils, EventBus, DIContainer, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:787:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:787:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:807:9)
[2025-10-26T22:27:37.611Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:27:37.611Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:27:37.615Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:27:37.615Z] [LOG] [API] Checking server status...
[2025-10-26T22:27:37.626Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:27:37.618Z"
}
[2025-10-26T22:27:37.627Z] [LOG] [API] WebGPU available
[2025-10-26T22:27:37.636Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:27:37.636Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:27:37.636Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:27:37.660Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:27:50.325Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:27:50.326Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:27:50.330Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:27:50.330Z] [LOG] [API] Checking server status...
[2025-10-26T22:27:50.341Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:27:50.333Z"
}
[2025-10-26T22:27:50.341Z] [LOG] [API] WebGPU available
[2025-10-26T22:27:50.362Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:27:50.362Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:27:50.362Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:27:50.398Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:27:51.964Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:27:51.964Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:27:51.968Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:27:51.968Z] [LOG] [API] Checking server status...
[2025-10-26T22:27:51.975Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:27:51.970Z"
}
[2025-10-26T22:27:51.975Z] [LOG] [API] WebGPU available
[2025-10-26T22:27:51.986Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:27:51.986Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:27:51.986Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:27:52.014Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:28:50.442Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:28:50.442Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:28:50.445Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:28:50.445Z] [LOG] [API] Checking server status...
[2025-10-26T22:28:50.456Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:28:50.446Z"
}
[2025-10-26T22:28:50.456Z] [LOG] [API] WebGPU available
[2025-10-26T22:28:50.464Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:28:50.464Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:28:50.464Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:28:50.478Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:29:50.376Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:29:50.376Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:29:50.379Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:29:50.379Z] [LOG] [API] Checking server status...
[2025-10-26T22:29:50.389Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:29:50.381Z"
}
[2025-10-26T22:29:50.389Z] [LOG] [API] WebGPU available
[2025-10-26T22:29:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:29:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:29:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:29:50.431Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:30:26.354Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:30:26.354Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:30:26.357Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:30:26.358Z] [LOG] [API] Checking server status...
[2025-10-26T22:30:26.364Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:30:26.359Z"
}
[2025-10-26T22:30:26.364Z] [LOG] [API] WebGPU available
[2025-10-26T22:30:26.371Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:30:26.371Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:30:26.371Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:30:26.398Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:30:27.580Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/event-bus.js
[2025-10-26T22:30:27.580Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/di-container.js
[2025-10-26T22:30:27.580Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/state-helpers-pure.js
[2025-10-26T22:30:27.580Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/storage-indexeddb.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/audit-logger.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/state-manager.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/api-client.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/tool-runner.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/agent-logic-pure.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/agent-cycle.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/context-manager.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/streaming-response-handler.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/reflection-store.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/performance-monitor.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-fsm.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-tools.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/meta-tool-creator.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/model-arena.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/ui-manager.js
[2025-10-26T22:30:27.581Z] [LOG] [VFS] Reading from IndexedDB: /personas/CodeRefactorerPersona.js
[2025-10-26T22:30:27.582Z] [LOG] [VFS] Loaded: /upgrades/utils.js (23523 bytes)
[2025-10-26T22:30:27.582Z] [LOG] [VFS] Loaded: /upgrades/event-bus.js (10766 bytes)
[2025-10-26T22:30:27.582Z] [LOG] [VFS] Loaded: /upgrades/di-container.js (17553 bytes)
[2025-10-26T22:30:27.582Z] [LOG] [VFS] Loaded: /upgrades/state-helpers-pure.js (5784 bytes)
[2025-10-26T22:30:27.582Z] [LOG] [VFS] Loaded: /upgrades/storage-indexeddb.js (13962 bytes)
[2025-10-26T22:30:27.582Z] [LOG] [VFS] Loaded: /upgrades/audit-logger.js (20795 bytes)
[2025-10-26T22:30:27.582Z] [LOG] [VFS] Loaded: /upgrades/state-manager.js (35019 bytes)
[2025-10-26T22:30:27.582Z] [LOG] [VFS] Loaded: /upgrades/api-client.js (21290 bytes)
[2025-10-26T22:30:27.583Z] [LOG] [VFS] Loaded: /upgrades/tool-runner.js (40773 bytes)
[2025-10-26T22:30:27.583Z] [LOG] [VFS] Loaded: /upgrades/agent-logic-pure.js (6050 bytes)
[2025-10-26T22:30:27.583Z] [LOG] [VFS] Loaded: /upgrades/agent-cycle.js (14891 bytes)
[2025-10-26T22:30:27.583Z] [LOG] [VFS] Loaded: /upgrades/context-manager.js (20606 bytes)
[2025-10-26T22:30:27.583Z] [LOG] [VFS] Loaded: /upgrades/streaming-response-handler.js (14028 bytes)
[2025-10-26T22:30:27.583Z] [LOG] [VFS] Loaded: /upgrades/reflection-store.js (25016 bytes)
[2025-10-26T22:30:27.583Z] [LOG] [VFS] Loaded: /upgrades/performance-monitor.js (22524 bytes)
[2025-10-26T22:30:27.583Z] [LOG] [VFS] Loaded: /upgrades/sentinel-fsm.js (40706 bytes)
[2025-10-26T22:30:27.583Z] [LOG] [VFS] Loaded: /upgrades/sentinel-tools.js (28407 bytes)
[2025-10-26T22:30:27.584Z] [LOG] [VFS] Loaded: /upgrades/meta-tool-creator.js (22986 bytes)
[2025-10-26T22:30:27.584Z] [LOG] [VFS] Loaded: /upgrades/model-arena.js (34140 bytes)
[2025-10-26T22:30:27.584Z] [LOG] [VFS] Loaded: /upgrades/ui-manager.js (113293 bytes)
[2025-10-26T22:30:27.584Z] [LOG] [VFS] Loaded: /personas/CodeRefactorerPersona.js (5488 bytes)
[2025-10-26T22:30:27.584Z] [INFO] {"timestamp":"2025-10-26T22:30:27.584Z","level":"INFO","message":"[CoreLogic] Module contents loaded: 21 items","details":{}}
[2025-10-26T22:30:27.584Z] [INFO] {"timestamp":"2025-10-26T22:30:27.584Z","level":"INFO","message":"[CoreLogic] agent-logic-pure content length: 6050","details":{}}
[2025-10-26T22:30:27.585Z] [LOG] [CoreLogic] Loaded module from /upgrades/utils.js: Utils
[2025-10-26T22:30:27.585Z] [INFO] {"timestamp":"2025-10-26T22:30:27.585Z","level":"INFO","message":"[DIContainer] Registered module: Utils","details":{}}
[2025-10-26T22:30:27.585Z] [INFO] {"timestamp":"2025-10-26T22:30:27.585Z","level":"INFO","message":"[CoreLogic] Registered module: Utils from /upgrades/utils.js","details":{}}
[2025-10-26T22:30:27.587Z] [INFO] {"timestamp":"2025-10-26T22:30:27.587Z","level":"INFO","message":"[DIContainer] Registered module: EventBus","details":{}}
[2025-10-26T22:30:27.587Z] [INFO] {"timestamp":"2025-10-26T22:30:27.587Z","level":"INFO","message":"[CoreLogic] Registered module: EventBus from /upgrades/event-bus.js","details":{}}
[2025-10-26T22:30:27.587Z] [LOG] [CoreLogic] Loaded module from /upgrades/di-container.js: DIContainer
[2025-10-26T22:30:27.587Z] [INFO] {"timestamp":"2025-10-26T22:30:27.587Z","level":"INFO","message":"[DIContainer] Registered module: DIContainer","details":{}}
[2025-10-26T22:30:27.587Z] [INFO] {"timestamp":"2025-10-26T22:30:27.587Z","level":"INFO","message":"[CoreLogic] Registered module: DIContainer from /upgrades/di-container.js","details":{}}
[2025-10-26T22:30:27.588Z] [INFO] {"timestamp":"2025-10-26T22:30:27.588Z","level":"INFO","message":"[DIContainer] Registered module: StateHelpersPure","details":{}}
[2025-10-26T22:30:27.589Z] [INFO] {"timestamp":"2025-10-26T22:30:27.589Z","level":"INFO","message":"[CoreLogic] Registered module: StateHelpersPure from /upgrades/state-helpers-pure.js","details":{}}
[2025-10-26T22:30:27.590Z] [INFO] {"timestamp":"2025-10-26T22:30:27.590Z","level":"INFO","message":"[DIContainer] Registered module: Storage","details":{}}
[2025-10-26T22:30:27.590Z] [INFO] {"timestamp":"2025-10-26T22:30:27.590Z","level":"INFO","message":"[CoreLogic] Registered module: Storage from /upgrades/storage-indexeddb.js","details":{}}
[2025-10-26T22:30:27.591Z] [INFO] {"timestamp":"2025-10-26T22:30:27.591Z","level":"INFO","message":"[DIContainer] Registered module: AuditLogger","details":{}}
[2025-10-26T22:30:27.591Z] [INFO] {"timestamp":"2025-10-26T22:30:27.591Z","level":"INFO","message":"[CoreLogic] Registered module: AuditLogger from /upgrades/audit-logger.js","details":{}}
[2025-10-26T22:30:27.592Z] [LOG] [CoreLogic] Loaded module from /upgrades/state-manager.js: StateManager
[2025-10-26T22:30:27.592Z] [INFO] {"timestamp":"2025-10-26T22:30:27.592Z","level":"INFO","message":"[DIContainer] Registered module: StateManager","details":{}}
[2025-10-26T22:30:27.592Z] [INFO] {"timestamp":"2025-10-26T22:30:27.592Z","level":"INFO","message":"[CoreLogic] Registered module: StateManager from /upgrades/state-manager.js","details":{}}
[2025-10-26T22:30:27.594Z] [INFO] {"timestamp":"2025-10-26T22:30:27.594Z","level":"INFO","message":"[DIContainer] Registered module: ApiClient","details":{}}
[2025-10-26T22:30:27.594Z] [INFO] {"timestamp":"2025-10-26T22:30:27.594Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T22:30:27.596Z] [INFO] {"timestamp":"2025-10-26T22:30:27.596Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T22:30:27.596Z] [INFO] {"timestamp":"2025-10-26T22:30:27.596Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T22:30:27.596Z] [INFO] {"timestamp":"2025-10-26T22:30:27.596Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T22:30:27.597Z] [INFO] {"timestamp":"2025-10-26T22:30:27.597Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T22:30:27.597Z] [INFO] {"timestamp":"2025-10-26T22:30:27.597Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T22:30:27.597Z] [INFO] {"timestamp":"2025-10-26T22:30:27.597Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T22:30:27.598Z] [INFO] {"timestamp":"2025-10-26T22:30:27.598Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T22:30:27.598Z] [INFO] {"timestamp":"2025-10-26T22:30:27.598Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T22:30:27.599Z] [INFO] {"timestamp":"2025-10-26T22:30:27.599Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T22:30:27.599Z] [INFO] {"timestamp":"2025-10-26T22:30:27.599Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T22:30:27.600Z] [INFO] {"timestamp":"2025-10-26T22:30:27.600Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T22:30:27.600Z] [INFO] {"timestamp":"2025-10-26T22:30:27.600Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T22:30:27.602Z] [INFO] {"timestamp":"2025-10-26T22:30:27.602Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T22:30:27.602Z] [INFO] {"timestamp":"2025-10-26T22:30:27.602Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T22:30:27.603Z] [INFO] {"timestamp":"2025-10-26T22:30:27.603Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T22:30:27.603Z] [INFO] {"timestamp":"2025-10-26T22:30:27.603Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T22:30:27.605Z] [INFO] {"timestamp":"2025-10-26T22:30:27.605Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T22:30:27.605Z] [INFO] {"timestamp":"2025-10-26T22:30:27.605Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T22:30:27.607Z] [INFO] {"timestamp":"2025-10-26T22:30:27.606Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T22:30:27.607Z] [INFO] {"timestamp":"2025-10-26T22:30:27.607Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T22:30:27.608Z] [INFO] {"timestamp":"2025-10-26T22:30:27.608Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T22:30:27.608Z] [INFO] {"timestamp":"2025-10-26T22:30:27.608Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T22:30:27.609Z] [INFO] {"timestamp":"2025-10-26T22:30:27.609Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T22:30:27.609Z] [INFO] {"timestamp":"2025-10-26T22:30:27.609Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T22:30:27.612Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T22:30:27.612Z] [INFO] {"timestamp":"2025-10-26T22:30:27.612Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T22:30:27.612Z] [INFO] {"timestamp":"2025-10-26T22:30:27.612Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T22:30:27.613Z] [WARN] [CoreLogic] Failed to import ESM module at /personas/CodeRefactorerPersona.js; falling back to legacy evaluator. TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
[2025-10-26T22:30:27.614Z] [INFO] {"timestamp":"2025-10-26T22:30:27.613Z","level":"INFO","message":"[DIContainer] Registered module: code-refactorer-persona","details":{}}
[2025-10-26T22:30:27.614Z] [INFO] {"timestamp":"2025-10-26T22:30:27.614Z","level":"INFO","message":"[CoreLogic] Registered module: code-refactorer-persona from /personas/CodeRefactorerPersona.js","details":{}}
[2025-10-26T22:30:27.614Z] [INFO] {"timestamp":"2025-10-26T22:30:27.614Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T22:30:27.615Z] [INFO] {"timestamp":"2025-10-26T22:30:27.615Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T22:30:28.086Z] [INFO] {"timestamp":"2025-10-26T22:30:28.086Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T22:30:28.087Z] [INFO] {"timestamp":"2025-10-26T22:30:28.087Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-26T22:30:28.087Z] [INFO] {"timestamp":"2025-10-26T22:30:28.087Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-26T22:30:28.088Z] [WARN] {"timestamp":"2025-10-26T22:30:28.088Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-26T22:30:28.088Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Failed to resolve dependency 'ApiClient' for module 'CycleLogic'.
Dependency chain: CycleLogic  ApiClient
Original error: [DIContainer] Failed to resolve dependency 'RateLimiter' for module 'ApiClient'.
Dependency chain: ApiClient  RateLimiter
Original error: [DIContainer] Service not found: RateLimiter
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'ApiClient' for module 'CycleLogic'.
Dependency chain: CycleLogic  ApiClient
Original error: [DIContainer] Failed to resolve dependency 'RateLimiter' for module 'ApiClient'.
Dependency chain: ApiClient  RateLimiter
Original error: [DIContainer] Service not found: RateLimiter
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:30:28.088Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Failed to resolve dependency 'ApiClient' for module 'CycleLogic'.
Dependency chain: CycleLogic  ApiClient
Original error: [DIContainer] Failed to resolve dependency 'RateLimiter' for module 'ApiClient'.
Dependency chain: ApiClient  RateLimiter
Original error: [DIContainer] Service not found: RateLimiter
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'ApiClient' for module 'CycleLogic'.
Dependency chain: CycleLogic  ApiClient
Original error: [DIContainer] Failed to resolve dependency 'RateLimiter' for module 'ApiClient'.
Dependency chain: ApiClient  RateLimiter
Original error: [DIContainer] Service not found: RateLimiter
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:30:35.026Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:30:35.026Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:30:35.034Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:30:35.034Z] [LOG] [API] Checking server status...
[2025-10-26T22:30:35.040Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:30:35.035Z"
}
[2025-10-26T22:30:35.040Z] [LOG] [API] WebGPU available
[2025-10-26T22:30:35.051Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:30:35.051Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:30:35.051Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:30:35.081Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:30:36.617Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/event-bus.js
[2025-10-26T22:30:36.617Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/di-container.js
[2025-10-26T22:30:36.617Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/state-helpers-pure.js
[2025-10-26T22:30:36.617Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/storage-indexeddb.js
[2025-10-26T22:30:36.617Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/audit-logger.js
[2025-10-26T22:30:36.617Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/state-manager.js
[2025-10-26T22:30:36.617Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/api-client.js
[2025-10-26T22:30:36.617Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/tool-runner.js
[2025-10-26T22:30:36.617Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/agent-logic-pure.js
[2025-10-26T22:30:36.617Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/agent-cycle.js
[2025-10-26T22:30:36.617Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/context-manager.js
[2025-10-26T22:30:36.618Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/streaming-response-handler.js
[2025-10-26T22:30:36.618Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/reflection-store.js
[2025-10-26T22:30:36.618Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/performance-monitor.js
[2025-10-26T22:30:36.618Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-fsm.js
[2025-10-26T22:30:36.618Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-tools.js
[2025-10-26T22:30:36.618Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/meta-tool-creator.js
[2025-10-26T22:30:36.618Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/model-arena.js
[2025-10-26T22:30:36.618Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/ui-manager.js
[2025-10-26T22:30:36.618Z] [LOG] [VFS] Reading from IndexedDB: /personas/CodeRefactorerPersona.js
[2025-10-26T22:30:36.619Z] [LOG] [VFS] Loaded: /upgrades/utils.js (23523 bytes)
[2025-10-26T22:30:36.619Z] [LOG] [VFS] Loaded: /upgrades/event-bus.js (10766 bytes)
[2025-10-26T22:30:36.619Z] [LOG] [VFS] Loaded: /upgrades/di-container.js (17553 bytes)
[2025-10-26T22:30:36.619Z] [LOG] [VFS] Loaded: /upgrades/state-helpers-pure.js (5784 bytes)
[2025-10-26T22:30:36.619Z] [LOG] [VFS] Loaded: /upgrades/storage-indexeddb.js (13962 bytes)
[2025-10-26T22:30:36.619Z] [LOG] [VFS] Loaded: /upgrades/audit-logger.js (20795 bytes)
[2025-10-26T22:30:36.619Z] [LOG] [VFS] Loaded: /upgrades/state-manager.js (35019 bytes)
[2025-10-26T22:30:36.620Z] [LOG] [VFS] Loaded: /upgrades/api-client.js (21290 bytes)
[2025-10-26T22:30:36.620Z] [LOG] [VFS] Loaded: /upgrades/tool-runner.js (40773 bytes)
[2025-10-26T22:30:36.620Z] [LOG] [VFS] Loaded: /upgrades/agent-logic-pure.js (6050 bytes)
[2025-10-26T22:30:36.620Z] [LOG] [VFS] Loaded: /upgrades/agent-cycle.js (14891 bytes)
[2025-10-26T22:30:36.620Z] [LOG] [VFS] Loaded: /upgrades/context-manager.js (20606 bytes)
[2025-10-26T22:30:36.620Z] [LOG] [VFS] Loaded: /upgrades/streaming-response-handler.js (14028 bytes)
[2025-10-26T22:30:36.620Z] [LOG] [VFS] Loaded: /upgrades/reflection-store.js (25016 bytes)
[2025-10-26T22:30:36.621Z] [LOG] [VFS] Loaded: /upgrades/performance-monitor.js (22524 bytes)
[2025-10-26T22:30:36.621Z] [LOG] [VFS] Loaded: /upgrades/sentinel-fsm.js (40706 bytes)
[2025-10-26T22:30:36.621Z] [LOG] [VFS] Loaded: /upgrades/sentinel-tools.js (28407 bytes)
[2025-10-26T22:30:36.621Z] [LOG] [VFS] Loaded: /upgrades/meta-tool-creator.js (22986 bytes)
[2025-10-26T22:30:36.621Z] [LOG] [VFS] Loaded: /upgrades/model-arena.js (34140 bytes)
[2025-10-26T22:30:36.621Z] [LOG] [VFS] Loaded: /upgrades/ui-manager.js (113293 bytes)
[2025-10-26T22:30:36.622Z] [LOG] [VFS] Loaded: /personas/CodeRefactorerPersona.js (5488 bytes)
[2025-10-26T22:30:36.622Z] [INFO] {"timestamp":"2025-10-26T22:30:36.622Z","level":"INFO","message":"[CoreLogic] Module contents loaded: 21 items","details":{}}
[2025-10-26T22:30:36.622Z] [INFO] {"timestamp":"2025-10-26T22:30:36.622Z","level":"INFO","message":"[CoreLogic] agent-logic-pure content length: 6050","details":{}}
[2025-10-26T22:30:36.623Z] [LOG] [CoreLogic] Loaded module from /upgrades/utils.js: Utils
[2025-10-26T22:30:36.623Z] [INFO] {"timestamp":"2025-10-26T22:30:36.623Z","level":"INFO","message":"[DIContainer] Registered module: Utils","details":{}}
[2025-10-26T22:30:36.623Z] [INFO] {"timestamp":"2025-10-26T22:30:36.623Z","level":"INFO","message":"[CoreLogic] Registered module: Utils from /upgrades/utils.js","details":{}}
[2025-10-26T22:30:36.625Z] [INFO] {"timestamp":"2025-10-26T22:30:36.625Z","level":"INFO","message":"[DIContainer] Registered module: EventBus","details":{}}
[2025-10-26T22:30:36.625Z] [INFO] {"timestamp":"2025-10-26T22:30:36.625Z","level":"INFO","message":"[CoreLogic] Registered module: EventBus from /upgrades/event-bus.js","details":{}}
[2025-10-26T22:30:36.626Z] [LOG] [CoreLogic] Loaded module from /upgrades/di-container.js: DIContainer
[2025-10-26T22:30:36.626Z] [INFO] {"timestamp":"2025-10-26T22:30:36.626Z","level":"INFO","message":"[DIContainer] Registered module: DIContainer","details":{}}
[2025-10-26T22:30:36.626Z] [INFO] {"timestamp":"2025-10-26T22:30:36.626Z","level":"INFO","message":"[CoreLogic] Registered module: DIContainer from /upgrades/di-container.js","details":{}}
[2025-10-26T22:30:36.627Z] [INFO] {"timestamp":"2025-10-26T22:30:36.627Z","level":"INFO","message":"[DIContainer] Registered module: StateHelpersPure","details":{}}
[2025-10-26T22:30:36.627Z] [INFO] {"timestamp":"2025-10-26T22:30:36.627Z","level":"INFO","message":"[CoreLogic] Registered module: StateHelpersPure from /upgrades/state-helpers-pure.js","details":{}}
[2025-10-26T22:30:36.628Z] [INFO] {"timestamp":"2025-10-26T22:30:36.628Z","level":"INFO","message":"[DIContainer] Registered module: Storage","details":{}}
[2025-10-26T22:30:36.628Z] [INFO] {"timestamp":"2025-10-26T22:30:36.628Z","level":"INFO","message":"[CoreLogic] Registered module: Storage from /upgrades/storage-indexeddb.js","details":{}}
[2025-10-26T22:30:36.629Z] [INFO] {"timestamp":"2025-10-26T22:30:36.629Z","level":"INFO","message":"[DIContainer] Registered module: AuditLogger","details":{}}
[2025-10-26T22:30:36.629Z] [INFO] {"timestamp":"2025-10-26T22:30:36.629Z","level":"INFO","message":"[CoreLogic] Registered module: AuditLogger from /upgrades/audit-logger.js","details":{}}
[2025-10-26T22:30:36.630Z] [LOG] [CoreLogic] Loaded module from /upgrades/state-manager.js: StateManager
[2025-10-26T22:30:36.630Z] [INFO] {"timestamp":"2025-10-26T22:30:36.630Z","level":"INFO","message":"[DIContainer] Registered module: StateManager","details":{}}
[2025-10-26T22:30:36.631Z] [INFO] {"timestamp":"2025-10-26T22:30:36.631Z","level":"INFO","message":"[CoreLogic] Registered module: StateManager from /upgrades/state-manager.js","details":{}}
[2025-10-26T22:30:36.632Z] [INFO] {"timestamp":"2025-10-26T22:30:36.632Z","level":"INFO","message":"[DIContainer] Registered module: ApiClient","details":{}}
[2025-10-26T22:30:36.632Z] [INFO] {"timestamp":"2025-10-26T22:30:36.632Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T22:30:36.634Z] [INFO] {"timestamp":"2025-10-26T22:30:36.634Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T22:30:36.634Z] [INFO] {"timestamp":"2025-10-26T22:30:36.634Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T22:30:36.634Z] [INFO] {"timestamp":"2025-10-26T22:30:36.634Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T22:30:36.635Z] [INFO] {"timestamp":"2025-10-26T22:30:36.635Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T22:30:36.635Z] [INFO] {"timestamp":"2025-10-26T22:30:36.635Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T22:30:36.635Z] [INFO] {"timestamp":"2025-10-26T22:30:36.635Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T22:30:36.636Z] [INFO] {"timestamp":"2025-10-26T22:30:36.636Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T22:30:36.636Z] [INFO] {"timestamp":"2025-10-26T22:30:36.636Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T22:30:36.638Z] [INFO] {"timestamp":"2025-10-26T22:30:36.638Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T22:30:36.638Z] [INFO] {"timestamp":"2025-10-26T22:30:36.638Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T22:30:36.639Z] [INFO] {"timestamp":"2025-10-26T22:30:36.639Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T22:30:36.639Z] [INFO] {"timestamp":"2025-10-26T22:30:36.639Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T22:30:36.641Z] [INFO] {"timestamp":"2025-10-26T22:30:36.641Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T22:30:36.641Z] [INFO] {"timestamp":"2025-10-26T22:30:36.641Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T22:30:36.642Z] [INFO] {"timestamp":"2025-10-26T22:30:36.642Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T22:30:36.642Z] [INFO] {"timestamp":"2025-10-26T22:30:36.642Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T22:30:36.644Z] [INFO] {"timestamp":"2025-10-26T22:30:36.644Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T22:30:36.644Z] [INFO] {"timestamp":"2025-10-26T22:30:36.644Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T22:30:36.646Z] [INFO] {"timestamp":"2025-10-26T22:30:36.646Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T22:30:36.646Z] [INFO] {"timestamp":"2025-10-26T22:30:36.646Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T22:30:36.648Z] [INFO] {"timestamp":"2025-10-26T22:30:36.648Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T22:30:36.648Z] [INFO] {"timestamp":"2025-10-26T22:30:36.648Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T22:30:36.650Z] [INFO] {"timestamp":"2025-10-26T22:30:36.649Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T22:30:36.650Z] [INFO] {"timestamp":"2025-10-26T22:30:36.650Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T22:30:36.652Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T22:30:36.652Z] [INFO] {"timestamp":"2025-10-26T22:30:36.652Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T22:30:36.652Z] [INFO] {"timestamp":"2025-10-26T22:30:36.652Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T22:30:36.653Z] [WARN] [CoreLogic] Failed to import ESM module at /personas/CodeRefactorerPersona.js; falling back to legacy evaluator. TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
[2025-10-26T22:30:36.654Z] [INFO] {"timestamp":"2025-10-26T22:30:36.654Z","level":"INFO","message":"[DIContainer] Registered module: code-refactorer-persona","details":{}}
[2025-10-26T22:30:36.654Z] [INFO] {"timestamp":"2025-10-26T22:30:36.654Z","level":"INFO","message":"[CoreLogic] Registered module: code-refactorer-persona from /personas/CodeRefactorerPersona.js","details":{}}
[2025-10-26T22:30:36.654Z] [INFO] {"timestamp":"2025-10-26T22:30:36.654Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T22:30:36.655Z] [INFO] {"timestamp":"2025-10-26T22:30:36.655Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T22:30:37.124Z] [INFO] {"timestamp":"2025-10-26T22:30:37.124Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T22:30:37.125Z] [INFO] {"timestamp":"2025-10-26T22:30:37.125Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-26T22:30:37.125Z] [INFO] {"timestamp":"2025-10-26T22:30:37.125Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-26T22:30:37.125Z] [WARN] {"timestamp":"2025-10-26T22:30:37.125Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-26T22:30:37.126Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Failed to resolve dependency 'ApiClient' for module 'CycleLogic'.
Dependency chain: CycleLogic  ApiClient
Original error: [DIContainer] Failed to resolve dependency 'RateLimiter' for module 'ApiClient'.
Dependency chain: ApiClient  RateLimiter
Original error: [DIContainer] Service not found: RateLimiter
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'ApiClient' for module 'CycleLogic'.
Dependency chain: CycleLogic  ApiClient
Original error: [DIContainer] Failed to resolve dependency 'RateLimiter' for module 'ApiClient'.
Dependency chain: ApiClient  RateLimiter
Original error: [DIContainer] Service not found: RateLimiter
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:30:37.126Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Failed to resolve dependency 'ApiClient' for module 'CycleLogic'.
Dependency chain: CycleLogic  ApiClient
Original error: [DIContainer] Failed to resolve dependency 'RateLimiter' for module 'ApiClient'.
Dependency chain: ApiClient  RateLimiter
Original error: [DIContainer] Service not found: RateLimiter
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'ApiClient' for module 'CycleLogic'.
Dependency chain: CycleLogic  ApiClient
Original error: [DIContainer] Failed to resolve dependency 'RateLimiter' for module 'ApiClient'.
Dependency chain: ApiClient  RateLimiter
Original error: [DIContainer] Service not found: RateLimiter
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:30:50.323Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:30:50.323Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:30:50.331Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:30:50.331Z] [LOG] [API] Checking server status...
[2025-10-26T22:30:50.342Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:30:50.333Z"
}
[2025-10-26T22:30:50.342Z] [LOG] [API] WebGPU available
[2025-10-26T22:30:50.346Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:30:50.346Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:30:50.347Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:30:50.370Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:31:50.377Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:31:50.377Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:31:50.414Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:31:50.414Z] [LOG] [API] Checking server status...
[2025-10-26T22:31:50.425Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:31:50.415Z"
}
[2025-10-26T22:31:50.425Z] [LOG] [API] WebGPU available
[2025-10-26T22:31:50.438Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:31:50.438Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:31:50.438Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:31:50.451Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:31:52.306Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:31:52.307Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:31:52.349Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:31:52.349Z] [LOG] [API] Checking server status...
[2025-10-26T22:31:52.356Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:31:52.351Z"
}
[2025-10-26T22:31:52.356Z] [LOG] [API] WebGPU available
[2025-10-26T22:31:52.364Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:31:52.364Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:31:52.364Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:31:52.385Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/api-client.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/tool-runner.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/agent-logic-pure.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/agent-cycle.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/context-manager.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/streaming-response-handler.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/reflection-store.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/performance-monitor.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-fsm.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/sentinel-tools.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/meta-tool-creator.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/model-arena.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /upgrades/ui-manager.js
[2025-10-26T22:31:54.835Z] [LOG] [VFS] Reading from IndexedDB: /personas/CodeRefactorerPersona.js
[2025-10-26T22:31:54.836Z] [LOG] [VFS] Loaded: /upgrades/utils.js (23523 bytes)
[2025-10-26T22:31:54.836Z] [LOG] [VFS] Loaded: /upgrades/event-bus.js (10766 bytes)
[2025-10-26T22:31:54.836Z] [LOG] [VFS] Loaded: /upgrades/di-container.js (17553 bytes)
[2025-10-26T22:31:54.837Z] [LOG] [VFS] Loaded: /upgrades/state-helpers-pure.js (5784 bytes)
[2025-10-26T22:31:54.837Z] [LOG] [VFS] Loaded: /upgrades/storage-indexeddb.js (13962 bytes)
[2025-10-26T22:31:54.838Z] [LOG] [VFS] Loaded: /upgrades/audit-logger.js (20795 bytes)
[2025-10-26T22:31:54.838Z] [LOG] [VFS] Loaded: /upgrades/rate-limiter.js (17640 bytes)
[2025-10-26T22:31:54.838Z] [LOG] [VFS] Loaded: /upgrades/state-manager.js (35019 bytes)
[2025-10-26T22:31:54.838Z] [LOG] [VFS] Loaded: /upgrades/api-client.js (21290 bytes)
[2025-10-26T22:31:54.838Z] [LOG] [VFS] Loaded: /upgrades/tool-runner.js (40773 bytes)
[2025-10-26T22:31:54.838Z] [LOG] [VFS] Loaded: /upgrades/agent-logic-pure.js (6050 bytes)
[2025-10-26T22:31:54.838Z] [LOG] [VFS] Loaded: /upgrades/agent-cycle.js (14891 bytes)
[2025-10-26T22:31:54.839Z] [LOG] [VFS] Loaded: /upgrades/context-manager.js (20606 bytes)
[2025-10-26T22:31:54.839Z] [LOG] [VFS] Loaded: /upgrades/streaming-response-handler.js (14028 bytes)
[2025-10-26T22:31:54.839Z] [LOG] [VFS] Loaded: /upgrades/reflection-store.js (25016 bytes)
[2025-10-26T22:31:54.839Z] [LOG] [VFS] Loaded: /upgrades/performance-monitor.js (22524 bytes)
[2025-10-26T22:31:54.839Z] [LOG] [VFS] Loaded: /upgrades/sentinel-fsm.js (40706 bytes)
[2025-10-26T22:31:54.839Z] [LOG] [VFS] Loaded: /upgrades/sentinel-tools.js (28407 bytes)
[2025-10-26T22:31:54.839Z] [LOG] [VFS] Loaded: /upgrades/meta-tool-creator.js (22986 bytes)
[2025-10-26T22:31:54.840Z] [LOG] [VFS] Loaded: /upgrades/model-arena.js (34140 bytes)
[2025-10-26T22:31:54.840Z] [LOG] [VFS] Loaded: /upgrades/ui-manager.js (113293 bytes)
[2025-10-26T22:31:54.840Z] [LOG] [VFS] Loaded: /personas/CodeRefactorerPersona.js (5488 bytes)
[2025-10-26T22:31:54.841Z] [INFO] {"timestamp":"2025-10-26T22:31:54.841Z","level":"INFO","message":"[CoreLogic] Module contents loaded: 22 items","details":{}}
[2025-10-26T22:31:54.841Z] [INFO] {"timestamp":"2025-10-26T22:31:54.841Z","level":"INFO","message":"[CoreLogic] agent-logic-pure content length: 6050","details":{}}
[2025-10-26T22:31:54.842Z] [LOG] [CoreLogic] Loaded module from /upgrades/utils.js: Utils
[2025-10-26T22:31:54.842Z] [INFO] {"timestamp":"2025-10-26T22:31:54.842Z","level":"INFO","message":"[DIContainer] Registered module: Utils","details":{}}
[2025-10-26T22:31:54.842Z] [INFO] {"timestamp":"2025-10-26T22:31:54.842Z","level":"INFO","message":"[CoreLogic] Registered module: Utils from /upgrades/utils.js","details":{}}
[2025-10-26T22:31:54.843Z] [INFO] {"timestamp":"2025-10-26T22:31:54.843Z","level":"INFO","message":"[DIContainer] Registered module: EventBus","details":{}}
[2025-10-26T22:31:54.843Z] [INFO] {"timestamp":"2025-10-26T22:31:54.843Z","level":"INFO","message":"[CoreLogic] Registered module: EventBus from /upgrades/event-bus.js","details":{}}
[2025-10-26T22:31:54.844Z] [LOG] [CoreLogic] Loaded module from /upgrades/di-container.js: DIContainer
[2025-10-26T22:31:54.844Z] [INFO] {"timestamp":"2025-10-26T22:31:54.844Z","level":"INFO","message":"[DIContainer] Registered module: DIContainer","details":{}}
[2025-10-26T22:31:54.844Z] [INFO] {"timestamp":"2025-10-26T22:31:54.844Z","level":"INFO","message":"[CoreLogic] Registered module: DIContainer from /upgrades/di-container.js","details":{}}
[2025-10-26T22:31:54.845Z] [INFO] {"timestamp":"2025-10-26T22:31:54.845Z","level":"INFO","message":"[DIContainer] Registered module: StateHelpersPure","details":{}}
[2025-10-26T22:31:54.845Z] [INFO] {"timestamp":"2025-10-26T22:31:54.845Z","level":"INFO","message":"[CoreLogic] Registered module: StateHelpersPure from /upgrades/state-helpers-pure.js","details":{}}
[2025-10-26T22:31:54.846Z] [INFO] {"timestamp":"2025-10-26T22:31:54.846Z","level":"INFO","message":"[DIContainer] Registered module: Storage","details":{}}
[2025-10-26T22:31:54.846Z] [INFO] {"timestamp":"2025-10-26T22:31:54.846Z","level":"INFO","message":"[CoreLogic] Registered module: Storage from /upgrades/storage-indexeddb.js","details":{}}
[2025-10-26T22:31:54.847Z] [INFO] {"timestamp":"2025-10-26T22:31:54.847Z","level":"INFO","message":"[DIContainer] Registered module: AuditLogger","details":{}}
[2025-10-26T22:31:54.847Z] [INFO] {"timestamp":"2025-10-26T22:31:54.847Z","level":"INFO","message":"[CoreLogic] Registered module: AuditLogger from /upgrades/audit-logger.js","details":{}}
[2025-10-26T22:31:54.849Z] [INFO] {"timestamp":"2025-10-26T22:31:54.849Z","level":"INFO","message":"[DIContainer] Registered module: RateLimiter","details":{}}
[2025-10-26T22:31:54.849Z] [INFO] {"timestamp":"2025-10-26T22:31:54.849Z","level":"INFO","message":"[CoreLogic] Registered module: RateLimiter from /upgrades/rate-limiter.js","details":{}}
[2025-10-26T22:31:54.850Z] [LOG] [CoreLogic] Loaded module from /upgrades/state-manager.js: StateManager
[2025-10-26T22:31:54.850Z] [INFO] {"timestamp":"2025-10-26T22:31:54.850Z","level":"INFO","message":"[DIContainer] Registered module: StateManager","details":{}}
[2025-10-26T22:31:54.850Z] [INFO] {"timestamp":"2025-10-26T22:31:54.850Z","level":"INFO","message":"[CoreLogic] Registered module: StateManager from /upgrades/state-manager.js","details":{}}
[2025-10-26T22:31:54.851Z] [INFO] {"timestamp":"2025-10-26T22:31:54.851Z","level":"INFO","message":"[DIContainer] Registered module: ApiClient","details":{}}
[2025-10-26T22:31:54.851Z] [INFO] {"timestamp":"2025-10-26T22:31:54.851Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T22:31:54.853Z] [INFO] {"timestamp":"2025-10-26T22:31:54.853Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T22:31:54.853Z] [INFO] {"timestamp":"2025-10-26T22:31:54.853Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T22:31:54.853Z] [INFO] {"timestamp":"2025-10-26T22:31:54.853Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T22:31:54.854Z] [INFO] {"timestamp":"2025-10-26T22:31:54.854Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T22:31:54.854Z] [INFO] {"timestamp":"2025-10-26T22:31:54.854Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T22:31:54.854Z] [INFO] {"timestamp":"2025-10-26T22:31:54.854Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T22:31:54.855Z] [INFO] {"timestamp":"2025-10-26T22:31:54.855Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T22:31:54.855Z] [INFO] {"timestamp":"2025-10-26T22:31:54.855Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T22:31:54.856Z] [INFO] {"timestamp":"2025-10-26T22:31:54.856Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T22:31:54.856Z] [INFO] {"timestamp":"2025-10-26T22:31:54.856Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T22:31:54.857Z] [INFO] {"timestamp":"2025-10-26T22:31:54.857Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T22:31:54.857Z] [INFO] {"timestamp":"2025-10-26T22:31:54.857Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T22:31:54.858Z] [INFO] {"timestamp":"2025-10-26T22:31:54.858Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T22:31:54.858Z] [INFO] {"timestamp":"2025-10-26T22:31:54.858Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T22:31:54.859Z] [INFO] {"timestamp":"2025-10-26T22:31:54.859Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T22:31:54.859Z] [INFO] {"timestamp":"2025-10-26T22:31:54.859Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T22:31:54.861Z] [INFO] {"timestamp":"2025-10-26T22:31:54.861Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T22:31:54.861Z] [INFO] {"timestamp":"2025-10-26T22:31:54.861Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T22:31:54.863Z] [INFO] {"timestamp":"2025-10-26T22:31:54.863Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T22:31:54.863Z] [INFO] {"timestamp":"2025-10-26T22:31:54.863Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T22:31:54.864Z] [INFO] {"timestamp":"2025-10-26T22:31:54.864Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T22:31:54.864Z] [INFO] {"timestamp":"2025-10-26T22:31:54.864Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T22:31:54.866Z] [INFO] {"timestamp":"2025-10-26T22:31:54.866Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T22:31:54.866Z] [INFO] {"timestamp":"2025-10-26T22:31:54.866Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T22:31:54.868Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T22:31:54.869Z] [INFO] {"timestamp":"2025-10-26T22:31:54.869Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T22:31:54.869Z] [INFO] {"timestamp":"2025-10-26T22:31:54.869Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T22:31:54.870Z] [WARN] [CoreLogic] Failed to import ESM module at /personas/CodeRefactorerPersona.js; falling back to legacy evaluator. TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
[2025-10-26T22:31:54.870Z] [INFO] {"timestamp":"2025-10-26T22:31:54.870Z","level":"INFO","message":"[DIContainer] Registered module: code-refactorer-persona","details":{}}
[2025-10-26T22:31:54.870Z] [INFO] {"timestamp":"2025-10-26T22:31:54.870Z","level":"INFO","message":"[CoreLogic] Registered module: code-refactorer-persona from /personas/CodeRefactorerPersona.js","details":{}}
[2025-10-26T22:31:54.870Z] [INFO] {"timestamp":"2025-10-26T22:31:54.870Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T22:31:54.871Z] [INFO] {"timestamp":"2025-10-26T22:31:54.871Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T22:31:55.342Z] [INFO] {"timestamp":"2025-10-26T22:31:55.342Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T22:31:55.343Z] [INFO] {"timestamp":"2025-10-26T22:31:55.343Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-26T22:31:55.343Z] [INFO] {"timestamp":"2025-10-26T22:31:55.343Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-26T22:31:55.344Z] [WARN] {"timestamp":"2025-10-26T22:31:55.344Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-26T22:31:55.344Z] [INFO] {"timestamp":"2025-10-26T22:31:55.344Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-26T22:31:55.344Z] [INFO] {"timestamp":"2025-10-26T22:31:55.344Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-26T22:31:55.344Z] [WARN] {"timestamp":"2025-10-26T22:31:55.344Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-26T22:31:55.345Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Failed to resolve dependency 'HybridLLMProvider' for module 'CycleLogic'.
Dependency chain: CycleLogic  HybridLLMProvider
Original error: [DIContainer] Service not found: HybridLLMProvider
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, RateLimiter, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'HybridLLMProvider' for module 'CycleLogic'.
Dependency chain: CycleLogic  HybridLLMProvider
Original error: [DIContainer] Service not found: HybridLLMProvider
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, RateLimiter, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:31:55.345Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Failed to resolve dependency 'HybridLLMProvider' for module 'CycleLogic'.
Dependency chain: CycleLogic  HybridLLMProvider
Original error: [DIContainer] Service not found: HybridLLMProvider
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, RateLimiter, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'HybridLLMProvider' for module 'CycleLogic'.
Dependency chain: CycleLogic  HybridLLMProvider
Original error: [DIContainer] Service not found: HybridLLMProvider
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, Storage, AuditLogger, RateLimiter, StateManager, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:32:50.343Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:32:50.343Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:32:50.347Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:32:50.347Z] [LOG] [API] Checking server status...
[2025-10-26T22:32:50.357Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:32:50.348Z"
}
[2025-10-26T22:32:50.357Z] [LOG] [API] WebGPU available
[2025-10-26T22:32:50.374Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:32:50.374Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:32:50.374Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:32:50.396Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:33:50.294Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:33:50.294Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:33:50.337Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:33:50.337Z] [LOG] [API] Checking server status...
[2025-10-26T22:33:50.347Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:33:50.338Z"
}
[2025-10-26T22:33:50.347Z] [LOG] [API] WebGPU available
[2025-10-26T22:33:50.357Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:33:50.357Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:33:50.357Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:33:50.374Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:34:50.494Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:34:50.494Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:34:50.496Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:34:50.496Z] [LOG] [API] Checking server status...
[2025-10-26T22:34:50.505Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:34:50.498Z"
}
[2025-10-26T22:34:50.506Z] [LOG] [API] WebGPU available
[2025-10-26T22:34:50.512Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:34:50.512Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:34:50.512Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:34:50.526Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:35:50.352Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:35:50.352Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:35:50.356Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:35:50.356Z] [LOG] [API] Checking server status...
[2025-10-26T22:35:50.366Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:35:50.357Z"
}
[2025-10-26T22:35:50.366Z] [LOG] [API] WebGPU available
[2025-10-26T22:35:50.383Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:35:50.383Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:35:50.383Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:35:50.402Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:36:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:36:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:36:50.401Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:36:50.401Z] [LOG] [API] Checking server status...
[2025-10-26T22:36:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:36:50.406Z"
}
[2025-10-26T22:36:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T22:36:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:36:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:36:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:36:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:36:50.358Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:36:50.359Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:36:50.407Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:36:50.407Z] [LOG] [API] Checking server status...
[2025-10-26T22:36:50.412Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:36:50.408Z"
}
[2025-10-26T22:36:50.412Z] [LOG] [API] WebGPU available
[2025-10-26T22:36:50.421Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:36:50.421Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:36:50.421Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:36:50.449Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:37:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:37:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:37:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:37:50.421Z] [LOG] [API] Checking server status...
[2025-10-26T22:37:50.429Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:37:50.423Z"
}
[2025-10-26T22:37:50.430Z] [LOG] [API] WebGPU available
[2025-10-26T22:37:50.434Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:37:50.434Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:37:50.434Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:37:50.448Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:37:50.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:37:50.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:37:50.421Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:37:50.422Z] [LOG] [API] Checking server status...
[2025-10-26T22:37:50.431Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:37:50.424Z"
}
[2025-10-26T22:37:50.431Z] [LOG] [API] WebGPU available
[2025-10-26T22:37:50.436Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:37:50.437Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:37:50.437Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:37:50.449Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:38:50.380Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:38:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:38:50.385Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:38:50.385Z] [LOG] [API] Checking server status...
[2025-10-26T22:38:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:38:50.389Z"
}
[2025-10-26T22:38:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T22:38:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:38:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:38:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:38:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:38:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:38:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:38:50.377Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:38:50.377Z] [LOG] [API] Checking server status...
[2025-10-26T22:38:50.385Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:38:50.380Z"
}
[2025-10-26T22:38:50.385Z] [LOG] [API] WebGPU available
[2025-10-26T22:38:50.389Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:38:50.389Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:38:50.389Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:38:50.411Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:39:50.353Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:39:50.353Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:39:50.381Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:39:50.381Z] [LOG] [API] Checking server status...
[2025-10-26T22:39:50.390Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:39:50.383Z"
}
[2025-10-26T22:39:50.391Z] [LOG] [API] WebGPU available
[2025-10-26T22:39:50.400Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:39:50.400Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:39:50.400Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:39:50.432Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:39:50.363Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:39:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:39:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:39:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T22:39:50.400Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:39:50.391Z"
}
[2025-10-26T22:39:50.400Z] [LOG] [API] WebGPU available
[2025-10-26T22:39:50.409Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:39:50.410Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:39:50.410Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:39:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:40:50.427Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:40:50.427Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:40:50.430Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:40:50.430Z] [LOG] [API] Checking server status...
[2025-10-26T22:40:50.440Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:40:50.431Z"
}
[2025-10-26T22:40:50.440Z] [LOG] [API] WebGPU available
[2025-10-26T22:40:50.444Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:40:50.444Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:40:50.444Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:40:50.460Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:40:50.393Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:40:50.393Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:40:50.396Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:40:50.396Z] [LOG] [API] Checking server status...
[2025-10-26T22:40:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:40:50.397Z"
}
[2025-10-26T22:40:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T22:40:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:40:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:40:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:40:50.421Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:41:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:41:50.379Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:41:50.383Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:41:50.383Z] [LOG] [API] Checking server status...
[2025-10-26T22:41:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:41:50.386Z"
}
[2025-10-26T22:41:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T22:41:50.413Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:41:50.413Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:41:50.413Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:41:50.427Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:41:50.313Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:41:50.313Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:41:50.362Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:41:50.362Z] [LOG] [API] Checking server status...
[2025-10-26T22:41:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:41:50.364Z"
}
[2025-10-26T22:41:50.394Z] [LOG] [API] WebGPU available
[2025-10-26T22:41:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:41:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:41:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:41:50.425Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:42:50.373Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:42:50.373Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:42:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:42:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T22:42:50.393Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:42:50.385Z"
}
[2025-10-26T22:42:50.393Z] [LOG] [API] WebGPU available
[2025-10-26T22:42:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:42:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:42:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:42:50.420Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:42:50.341Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:42:50.370Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:42:50.395Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:42:50.395Z] [LOG] [API] Checking server status...
[2025-10-26T22:42:50.407Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:42:50.398Z"
}
[2025-10-26T22:42:50.407Z] [LOG] [API] WebGPU available
[2025-10-26T22:42:50.411Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:42:50.412Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:42:50.412Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:42:50.442Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:43:50.283Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:43:50.283Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:43:50.287Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:43:50.287Z] [LOG] [API] Checking server status...
[2025-10-26T22:43:50.297Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:43:50.289Z"
}
[2025-10-26T22:43:50.297Z] [LOG] [API] WebGPU available
[2025-10-26T22:43:50.303Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:43:50.303Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:43:50.303Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:43:50.317Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:43:50.309Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:43:50.309Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:43:50.312Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:43:50.312Z] [LOG] [API] Checking server status...
[2025-10-26T22:43:50.327Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:43:50.315Z"
}
[2025-10-26T22:43:50.327Z] [LOG] [API] WebGPU available
[2025-10-26T22:43:50.345Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:43:50.345Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:43:50.345Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:43:50.360Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:44:20.774Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:44:20.774Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:44:20.777Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:44:20.778Z] [LOG] [API] Checking server status...
[2025-10-26T22:44:20.821Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:44:20.781Z"
}
[2025-10-26T22:44:20.821Z] [LOG] [API] WebGPU available
[2025-10-26T22:44:20.826Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:44:20.826Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:44:20.826Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:44:20.860Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:44:23.450Z] [LOG] [VFS] Loaded: /upgrades/model-registry.js (10966 bytes)
[2025-10-26T22:44:23.450Z] [LOG] [VFS] Loaded: /upgrades/state-manager.js (35019 bytes)
[2025-10-26T22:44:23.450Z] [LOG] [VFS] Loaded: /upgrades/local-llm.js (22963 bytes)
[2025-10-26T22:44:23.450Z] [LOG] [VFS] Loaded: /upgrades/hybrid-llm-provider.js (25130 bytes)
[2025-10-26T22:44:23.450Z] [LOG] [VFS] Loaded: /upgrades/api-client.js (21290 bytes)
[2025-10-26T22:44:23.451Z] [LOG] [VFS] Loaded: /upgrades/tool-runner.js (40773 bytes)
[2025-10-26T22:44:23.451Z] [LOG] [VFS] Loaded: /upgrades/agent-logic-pure.js (6050 bytes)
[2025-10-26T22:44:23.451Z] [LOG] [VFS] Loaded: /upgrades/agent-cycle.js (14891 bytes)
[2025-10-26T22:44:23.451Z] [LOG] [VFS] Loaded: /upgrades/context-manager.js (20606 bytes)
[2025-10-26T22:44:23.451Z] [LOG] [VFS] Loaded: /upgrades/streaming-response-handler.js (14028 bytes)
[2025-10-26T22:44:23.451Z] [LOG] [VFS] Loaded: /upgrades/reflection-store.js (25016 bytes)
[2025-10-26T22:44:23.451Z] [LOG] [VFS] Loaded: /upgrades/performance-monitor.js (22524 bytes)
[2025-10-26T22:44:23.451Z] [LOG] [VFS] Loaded: /upgrades/verification-manager.js (22605 bytes)
[2025-10-26T22:44:23.451Z] [LOG] [VFS] Loaded: /upgrades/sentinel-fsm.js (40706 bytes)
[2025-10-26T22:44:23.452Z] [LOG] [VFS] Loaded: /upgrades/sentinel-tools.js (28407 bytes)
[2025-10-26T22:44:23.452Z] [LOG] [VFS] Loaded: /upgrades/meta-tool-creator.js (22986 bytes)
[2025-10-26T22:44:23.452Z] [LOG] [VFS] Loaded: /upgrades/model-arena.js (34140 bytes)
[2025-10-26T22:44:23.452Z] [LOG] [VFS] Loaded: /upgrades/ui-manager.js (113293 bytes)
[2025-10-26T22:44:23.452Z] [LOG] [VFS] Loaded: /personas/CodeRefactorerPersona.js (5488 bytes)
[2025-10-26T22:44:23.452Z] [INFO] {"timestamp":"2025-10-26T22:44:23.452Z","level":"INFO","message":"[CoreLogic] Module contents loaded: 28 items","details":{}}
[2025-10-26T22:44:23.452Z] [INFO] {"timestamp":"2025-10-26T22:44:23.452Z","level":"INFO","message":"[CoreLogic] agent-logic-pure content length: 6050","details":{}}
[2025-10-26T22:44:23.453Z] [LOG] [CoreLogic] Loaded module from /upgrades/utils.js: Utils
[2025-10-26T22:44:23.453Z] [INFO] {"timestamp":"2025-10-26T22:44:23.453Z","level":"INFO","message":"[DIContainer] Registered module: Utils","details":{}}
[2025-10-26T22:44:23.453Z] [INFO] {"timestamp":"2025-10-26T22:44:23.453Z","level":"INFO","message":"[CoreLogic] Registered module: Utils from /upgrades/utils.js","details":{}}
[2025-10-26T22:44:23.455Z] [INFO] {"timestamp":"2025-10-26T22:44:23.455Z","level":"INFO","message":"[DIContainer] Registered module: EventBus","details":{}}
[2025-10-26T22:44:23.455Z] [INFO] {"timestamp":"2025-10-26T22:44:23.455Z","level":"INFO","message":"[CoreLogic] Registered module: EventBus from /upgrades/event-bus.js","details":{}}
[2025-10-26T22:44:23.455Z] [LOG] [CoreLogic] Loaded module from /upgrades/di-container.js: DIContainer
[2025-10-26T22:44:23.456Z] [INFO] {"timestamp":"2025-10-26T22:44:23.455Z","level":"INFO","message":"[DIContainer] Registered module: DIContainer","details":{}}
[2025-10-26T22:44:23.456Z] [INFO] {"timestamp":"2025-10-26T22:44:23.456Z","level":"INFO","message":"[CoreLogic] Registered module: DIContainer from /upgrades/di-container.js","details":{}}
[2025-10-26T22:44:23.456Z] [INFO] {"timestamp":"2025-10-26T22:44:23.456Z","level":"INFO","message":"[DIContainer] Registered module: StateHelpersPure","details":{}}
[2025-10-26T22:44:23.456Z] [INFO] {"timestamp":"2025-10-26T22:44:23.456Z","level":"INFO","message":"[CoreLogic] Registered module: StateHelpersPure from /upgrades/state-helpers-pure.js","details":{}}
[2025-10-26T22:44:23.458Z] [INFO] {"timestamp":"2025-10-26T22:44:23.457Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunnerPureHelpers","details":{}}
[2025-10-26T22:44:23.458Z] [INFO] {"timestamp":"2025-10-26T22:44:23.458Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunnerPureHelpers from /upgrades/tool-runner-pure-helpers.js","details":{}}
[2025-10-26T22:44:23.459Z] [INFO] {"timestamp":"2025-10-26T22:44:23.459Z","level":"INFO","message":"[DIContainer] Registered module: Config","details":{}}
[2025-10-26T22:44:23.459Z] [INFO] {"timestamp":"2025-10-26T22:44:23.459Z","level":"INFO","message":"[CoreLogic] Registered module: Config from /upgrades/config.js","details":{}}
[2025-10-26T22:44:23.460Z] [INFO] {"timestamp":"2025-10-26T22:44:23.460Z","level":"INFO","message":"[DIContainer] Registered module: Storage","details":{}}
[2025-10-26T22:44:23.460Z] [INFO] {"timestamp":"2025-10-26T22:44:23.460Z","level":"INFO","message":"[CoreLogic] Registered module: Storage from /upgrades/storage-indexeddb.js","details":{}}
[2025-10-26T22:44:23.461Z] [INFO] {"timestamp":"2025-10-26T22:44:23.461Z","level":"INFO","message":"[DIContainer] Registered module: AuditLogger","details":{}}
[2025-10-26T22:44:23.461Z] [INFO] {"timestamp":"2025-10-26T22:44:23.461Z","level":"INFO","message":"[CoreLogic] Registered module: AuditLogger from /upgrades/audit-logger.js","details":{}}
[2025-10-26T22:44:23.463Z] [INFO] {"timestamp":"2025-10-26T22:44:23.462Z","level":"INFO","message":"[DIContainer] Registered module: RateLimiter","details":{}}
[2025-10-26T22:44:23.463Z] [INFO] {"timestamp":"2025-10-26T22:44:23.463Z","level":"INFO","message":"[CoreLogic] Registered module: RateLimiter from /upgrades/rate-limiter.js","details":{}}
[2025-10-26T22:44:23.464Z] [INFO] {"timestamp":"2025-10-26T22:44:23.464Z","level":"INFO","message":"[DIContainer] Registered module: ModelRegistry","details":{}}
[2025-10-26T22:44:23.464Z] [INFO] {"timestamp":"2025-10-26T22:44:23.464Z","level":"INFO","message":"[CoreLogic] Registered module: ModelRegistry from /upgrades/model-registry.js","details":{}}
[2025-10-26T22:44:23.465Z] [LOG] [CoreLogic] Loaded module from /upgrades/state-manager.js: StateManager
[2025-10-26T22:44:23.465Z] [INFO] {"timestamp":"2025-10-26T22:44:23.465Z","level":"INFO","message":"[DIContainer] Registered module: StateManager","details":{}}
[2025-10-26T22:44:23.465Z] [INFO] {"timestamp":"2025-10-26T22:44:23.465Z","level":"INFO","message":"[CoreLogic] Registered module: StateManager from /upgrades/state-manager.js","details":{}}
[2025-10-26T22:44:23.466Z] [INFO] {"timestamp":"2025-10-26T22:44:23.466Z","level":"INFO","message":"[DIContainer] Registered module: LocalLLM","details":{}}
[2025-10-26T22:44:23.466Z] [INFO] {"timestamp":"2025-10-26T22:44:23.466Z","level":"INFO","message":"[CoreLogic] Registered module: LocalLLM from /upgrades/local-llm.js","details":{}}
[2025-10-26T22:44:23.467Z] [INFO] {"timestamp":"2025-10-26T22:44:23.467Z","level":"INFO","message":"[DIContainer] Registered module: HybridLLMProvider","details":{}}
[2025-10-26T22:44:23.467Z] [INFO] {"timestamp":"2025-10-26T22:44:23.467Z","level":"INFO","message":"[CoreLogic] Registered module: HybridLLMProvider from /upgrades/hybrid-llm-provider.js","details":{}}
[2025-10-26T22:44:23.469Z] [INFO] {"timestamp":"2025-10-26T22:44:23.468Z","level":"INFO","message":"[DIContainer] Registered module: ApiClient","details":{}}
[2025-10-26T22:44:23.469Z] [INFO] {"timestamp":"2025-10-26T22:44:23.469Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T22:44:23.471Z] [INFO] {"timestamp":"2025-10-26T22:44:23.471Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T22:44:23.471Z] [INFO] {"timestamp":"2025-10-26T22:44:23.471Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T22:44:23.471Z] [INFO] {"timestamp":"2025-10-26T22:44:23.471Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T22:44:23.472Z] [INFO] {"timestamp":"2025-10-26T22:44:23.472Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T22:44:23.472Z] [INFO] {"timestamp":"2025-10-26T22:44:23.472Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T22:44:23.472Z] [INFO] {"timestamp":"2025-10-26T22:44:23.472Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T22:44:23.473Z] [INFO] {"timestamp":"2025-10-26T22:44:23.473Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T22:44:23.473Z] [INFO] {"timestamp":"2025-10-26T22:44:23.473Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T22:44:23.474Z] [INFO] {"timestamp":"2025-10-26T22:44:23.474Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T22:44:23.474Z] [INFO] {"timestamp":"2025-10-26T22:44:23.474Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T22:44:23.475Z] [INFO] {"timestamp":"2025-10-26T22:44:23.475Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T22:44:23.475Z] [INFO] {"timestamp":"2025-10-26T22:44:23.475Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T22:44:23.476Z] [INFO] {"timestamp":"2025-10-26T22:44:23.476Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T22:44:23.476Z] [INFO] {"timestamp":"2025-10-26T22:44:23.476Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T22:44:23.478Z] [INFO] {"timestamp":"2025-10-26T22:44:23.478Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T22:44:23.478Z] [INFO] {"timestamp":"2025-10-26T22:44:23.478Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T22:44:23.479Z] [INFO] {"timestamp":"2025-10-26T22:44:23.479Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-26T22:44:23.479Z] [INFO] {"timestamp":"2025-10-26T22:44:23.479Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-26T22:44:23.481Z] [INFO] {"timestamp":"2025-10-26T22:44:23.481Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T22:44:23.481Z] [INFO] {"timestamp":"2025-10-26T22:44:23.481Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T22:44:23.484Z] [INFO] {"timestamp":"2025-10-26T22:44:23.483Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T22:44:23.484Z] [INFO] {"timestamp":"2025-10-26T22:44:23.484Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T22:44:23.486Z] [INFO] {"timestamp":"2025-10-26T22:44:23.486Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T22:44:23.486Z] [INFO] {"timestamp":"2025-10-26T22:44:23.486Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T22:44:23.489Z] [INFO] {"timestamp":"2025-10-26T22:44:23.489Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T22:44:23.489Z] [INFO] {"timestamp":"2025-10-26T22:44:23.489Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T22:44:23.491Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T22:44:23.492Z] [INFO] {"timestamp":"2025-10-26T22:44:23.491Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T22:44:23.492Z] [INFO] {"timestamp":"2025-10-26T22:44:23.492Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T22:44:23.493Z] [WARN] [CoreLogic] Failed to import ESM module at /personas/CodeRefactorerPersona.js; falling back to legacy evaluator. TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
[2025-10-26T22:44:23.493Z] [INFO] {"timestamp":"2025-10-26T22:44:23.493Z","level":"INFO","message":"[DIContainer] Registered module: code-refactorer-persona","details":{}}
[2025-10-26T22:44:23.493Z] [INFO] {"timestamp":"2025-10-26T22:44:23.493Z","level":"INFO","message":"[CoreLogic] Registered module: code-refactorer-persona from /personas/CodeRefactorerPersona.js","details":{}}
[2025-10-26T22:44:23.493Z] [INFO] {"timestamp":"2025-10-26T22:44:23.493Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T22:44:23.494Z] [INFO] {"timestamp":"2025-10-26T22:44:23.494Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T22:44:23.954Z] [INFO] {"timestamp":"2025-10-26T22:44:23.954Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T22:44:23.955Z] [INFO] {"timestamp":"2025-10-26T22:44:23.955Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-26T22:44:23.956Z] [INFO] {"timestamp":"2025-10-26T22:44:23.955Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-26T22:44:23.956Z] [WARN] {"timestamp":"2025-10-26T22:44:23.956Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-26T22:44:23.956Z] [INFO] {"timestamp":"2025-10-26T22:44:23.956Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-26T22:44:23.956Z] [INFO] {"timestamp":"2025-10-26T22:44:23.956Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-26T22:44:23.957Z] [WARN] {"timestamp":"2025-10-26T22:44:23.957Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-26T22:44:23.958Z] [INFO] {"timestamp":"2025-10-26T22:44:23.958Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-26T22:44:23.958Z] [WARN] {"timestamp":"2025-10-26T22:44:23.958Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-26T22:44:23.959Z] [INFO] {"timestamp":"2025-10-26T22:44:23.959Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-26T22:44:23.959Z] [INFO] {"timestamp":"2025-10-26T22:44:23.959Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-26T22:44:23.959Z] [INFO] {"timestamp":"2025-10-26T22:44:23.959Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-26T22:44:23.960Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Failed to resolve dependency 'DiffGenerator' for module 'UI'.
Dependency chain: UI  DiffGenerator
Original error: [DIContainer] Service not found: DiffGenerator
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, VerificationManager, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'DiffGenerator' for module 'UI'.
Dependency chain: UI  DiffGenerator
Original error: [DIContainer] Service not found: DiffGenerator
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, VerificationManager, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:300:16)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:44:23.961Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Failed to resolve dependency 'DiffGenerator' for module 'UI'.
Dependency chain: UI  DiffGenerator
Original error: [DIContainer] Service not found: DiffGenerator
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, VerificationManager, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'DiffGenerator' for module 'UI'.
Dependency chain: UI  DiffGenerator
Original error: [DIContainer] Service not found: DiffGenerator
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, VerificationManager, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:300:16)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:44:50.293Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:44:50.293Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:44:50.297Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:44:50.297Z] [LOG] [API] Checking server status...
[2025-10-26T22:44:50.307Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:44:50.299Z"
}
[2025-10-26T22:44:50.307Z] [LOG] [API] WebGPU available
[2025-10-26T22:44:50.325Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:44:50.326Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:44:50.326Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:44:50.344Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:45:50.343Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:45:50.343Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:45:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:45:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T22:45:50.397Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:45:50.387Z"
}
[2025-10-26T22:45:50.397Z] [LOG] [API] WebGPU available
[2025-10-26T22:45:50.404Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:45:50.404Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:45:50.404Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:45:50.413Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:46:50.468Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:46:50.468Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:46:50.470Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:46:50.470Z] [LOG] [API] Checking server status...
[2025-10-26T22:46:50.480Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:46:50.472Z"
}
[2025-10-26T22:46:50.480Z] [LOG] [API] WebGPU available
[2025-10-26T22:46:50.487Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:46:50.487Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:46:50.487Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:46:50.501Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:47:02.881Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:47:02.881Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:47:02.885Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:47:02.885Z] [LOG] [API] Checking server status...
[2025-10-26T22:47:02.892Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:47:02.887Z"
}
[2025-10-26T22:47:02.892Z] [LOG] [API] WebGPU available
[2025-10-26T22:47:02.896Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:47:02.896Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:47:02.896Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:47:02.907Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:47:06.317Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:47:06.318Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:47:06.321Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:47:06.321Z] [LOG] [API] Checking server status...
[2025-10-26T22:47:06.328Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:47:06.325Z"
}
[2025-10-26T22:47:06.329Z] [LOG] [API] WebGPU available
[2025-10-26T22:47:06.332Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:47:06.332Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:47:06.332Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:47:06.351Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:47:11.205Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:47:11.205Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:47:11.216Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:47:11.216Z] [LOG] [API] Checking server status...
[2025-10-26T22:47:11.229Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:47:11.221Z"
}
[2025-10-26T22:47:11.229Z] [LOG] [API] WebGPU available
[2025-10-26T22:47:11.235Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:47:11.235Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:47:11.235Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:47:11.263Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:47:12.388Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:47:12.388Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:47:12.955Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:47:12.955Z] [LOG] [API] Checking server status...
[2025-10-26T22:47:12.965Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:47:12.957Z"
}
[2025-10-26T22:47:12.965Z] [LOG] [API] WebGPU available
[2025-10-26T22:47:12.974Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:47:12.974Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:47:12.974Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:47:12.996Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:47:14.291Z] [LOG] [VFS] Loaded: /upgrades/hybrid-llm-provider.js (25130 bytes)
[2025-10-26T22:47:14.291Z] [LOG] [VFS] Loaded: /upgrades/api-client.js (21290 bytes)
[2025-10-26T22:47:14.291Z] [LOG] [VFS] Loaded: /upgrades/tool-runner.js (40773 bytes)
[2025-10-26T22:47:14.291Z] [LOG] [VFS] Loaded: /utils/diff-generator.js (1320 bytes)
[2025-10-26T22:47:14.292Z] [LOG] [VFS] Loaded: /upgrades/agent-logic-pure.js (6050 bytes)
[2025-10-26T22:47:14.292Z] [LOG] [VFS] Loaded: /upgrades/agent-cycle.js (14891 bytes)
[2025-10-26T22:47:14.292Z] [LOG] [VFS] Loaded: /upgrades/context-manager.js (20606 bytes)
[2025-10-26T22:47:14.292Z] [LOG] [VFS] Loaded: /upgrades/streaming-response-handler.js (14028 bytes)
[2025-10-26T22:47:14.292Z] [LOG] [VFS] Loaded: /upgrades/reflection-store.js (25016 bytes)
[2025-10-26T22:47:14.292Z] [LOG] [VFS] Loaded: /upgrades/performance-monitor.js (22524 bytes)
[2025-10-26T22:47:14.292Z] [LOG] [VFS] Loaded: /upgrades/verification-manager.js (22605 bytes)
[2025-10-26T22:47:14.292Z] [LOG] [VFS] Loaded: /upgrades/sentinel-fsm.js (40706 bytes)
[2025-10-26T22:47:14.292Z] [LOG] [VFS] Loaded: /upgrades/sentinel-tools.js (28407 bytes)
[2025-10-26T22:47:14.293Z] [LOG] [VFS] Loaded: /upgrades/meta-tool-creator.js (22986 bytes)
[2025-10-26T22:47:14.293Z] [LOG] [VFS] Loaded: /upgrades/model-arena.js (34140 bytes)
[2025-10-26T22:47:14.293Z] [LOG] [VFS] Loaded: /upgrades/ui-manager.js (113293 bytes)
[2025-10-26T22:47:14.293Z] [LOG] [VFS] Loaded: /personas/CodeRefactorerPersona.js (5488 bytes)
[2025-10-26T22:47:14.293Z] [INFO] {"timestamp":"2025-10-26T22:47:14.293Z","level":"INFO","message":"[CoreLogic] Module contents loaded: 29 items","details":{}}
[2025-10-26T22:47:14.293Z] [INFO] {"timestamp":"2025-10-26T22:47:14.293Z","level":"INFO","message":"[CoreLogic] agent-logic-pure content length: 6050","details":{}}
[2025-10-26T22:47:14.294Z] [LOG] [CoreLogic] Loaded module from /upgrades/utils.js: Utils
[2025-10-26T22:47:14.294Z] [INFO] {"timestamp":"2025-10-26T22:47:14.294Z","level":"INFO","message":"[DIContainer] Registered module: Utils","details":{}}
[2025-10-26T22:47:14.294Z] [INFO] {"timestamp":"2025-10-26T22:47:14.294Z","level":"INFO","message":"[CoreLogic] Registered module: Utils from /upgrades/utils.js","details":{}}
[2025-10-26T22:47:14.296Z] [INFO] {"timestamp":"2025-10-26T22:47:14.296Z","level":"INFO","message":"[DIContainer] Registered module: EventBus","details":{}}
[2025-10-26T22:47:14.296Z] [INFO] {"timestamp":"2025-10-26T22:47:14.296Z","level":"INFO","message":"[CoreLogic] Registered module: EventBus from /upgrades/event-bus.js","details":{}}
[2025-10-26T22:47:14.297Z] [LOG] [CoreLogic] Loaded module from /upgrades/di-container.js: DIContainer
[2025-10-26T22:47:14.297Z] [INFO] {"timestamp":"2025-10-26T22:47:14.297Z","level":"INFO","message":"[DIContainer] Registered module: DIContainer","details":{}}
[2025-10-26T22:47:14.297Z] [INFO] {"timestamp":"2025-10-26T22:47:14.297Z","level":"INFO","message":"[CoreLogic] Registered module: DIContainer from /upgrades/di-container.js","details":{}}
[2025-10-26T22:47:14.298Z] [INFO] {"timestamp":"2025-10-26T22:47:14.298Z","level":"INFO","message":"[DIContainer] Registered module: StateHelpersPure","details":{}}
[2025-10-26T22:47:14.298Z] [INFO] {"timestamp":"2025-10-26T22:47:14.298Z","level":"INFO","message":"[CoreLogic] Registered module: StateHelpersPure from /upgrades/state-helpers-pure.js","details":{}}
[2025-10-26T22:47:14.299Z] [INFO] {"timestamp":"2025-10-26T22:47:14.299Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunnerPureHelpers","details":{}}
[2025-10-26T22:47:14.299Z] [INFO] {"timestamp":"2025-10-26T22:47:14.299Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunnerPureHelpers from /upgrades/tool-runner-pure-helpers.js","details":{}}
[2025-10-26T22:47:14.300Z] [INFO] {"timestamp":"2025-10-26T22:47:14.300Z","level":"INFO","message":"[DIContainer] Registered module: Config","details":{}}
[2025-10-26T22:47:14.300Z] [INFO] {"timestamp":"2025-10-26T22:47:14.300Z","level":"INFO","message":"[CoreLogic] Registered module: Config from /upgrades/config.js","details":{}}
[2025-10-26T22:47:14.301Z] [INFO] {"timestamp":"2025-10-26T22:47:14.301Z","level":"INFO","message":"[DIContainer] Registered module: Storage","details":{}}
[2025-10-26T22:47:14.301Z] [INFO] {"timestamp":"2025-10-26T22:47:14.301Z","level":"INFO","message":"[CoreLogic] Registered module: Storage from /upgrades/storage-indexeddb.js","details":{}}
[2025-10-26T22:47:14.302Z] [INFO] {"timestamp":"2025-10-26T22:47:14.302Z","level":"INFO","message":"[DIContainer] Registered module: AuditLogger","details":{}}
[2025-10-26T22:47:14.302Z] [INFO] {"timestamp":"2025-10-26T22:47:14.302Z","level":"INFO","message":"[CoreLogic] Registered module: AuditLogger from /upgrades/audit-logger.js","details":{}}
[2025-10-26T22:47:14.303Z] [INFO] {"timestamp":"2025-10-26T22:47:14.303Z","level":"INFO","message":"[DIContainer] Registered module: RateLimiter","details":{}}
[2025-10-26T22:47:14.303Z] [INFO] {"timestamp":"2025-10-26T22:47:14.303Z","level":"INFO","message":"[CoreLogic] Registered module: RateLimiter from /upgrades/rate-limiter.js","details":{}}
[2025-10-26T22:47:14.304Z] [INFO] {"timestamp":"2025-10-26T22:47:14.304Z","level":"INFO","message":"[DIContainer] Registered module: ModelRegistry","details":{}}
[2025-10-26T22:47:14.304Z] [INFO] {"timestamp":"2025-10-26T22:47:14.304Z","level":"INFO","message":"[CoreLogic] Registered module: ModelRegistry from /upgrades/model-registry.js","details":{}}
[2025-10-26T22:47:14.305Z] [LOG] [CoreLogic] Loaded module from /upgrades/state-manager.js: StateManager
[2025-10-26T22:47:14.305Z] [INFO] {"timestamp":"2025-10-26T22:47:14.305Z","level":"INFO","message":"[DIContainer] Registered module: StateManager","details":{}}
[2025-10-26T22:47:14.305Z] [INFO] {"timestamp":"2025-10-26T22:47:14.305Z","level":"INFO","message":"[CoreLogic] Registered module: StateManager from /upgrades/state-manager.js","details":{}}
[2025-10-26T22:47:14.307Z] [INFO] {"timestamp":"2025-10-26T22:47:14.307Z","level":"INFO","message":"[DIContainer] Registered module: LocalLLM","details":{}}
[2025-10-26T22:47:14.307Z] [INFO] {"timestamp":"2025-10-26T22:47:14.307Z","level":"INFO","message":"[CoreLogic] Registered module: LocalLLM from /upgrades/local-llm.js","details":{}}
[2025-10-26T22:47:14.308Z] [INFO] {"timestamp":"2025-10-26T22:47:14.308Z","level":"INFO","message":"[DIContainer] Registered module: HybridLLMProvider","details":{}}
[2025-10-26T22:47:14.309Z] [INFO] {"timestamp":"2025-10-26T22:47:14.308Z","level":"INFO","message":"[CoreLogic] Registered module: HybridLLMProvider from /upgrades/hybrid-llm-provider.js","details":{}}
[2025-10-26T22:47:14.310Z] [INFO] {"timestamp":"2025-10-26T22:47:14.310Z","level":"INFO","message":"[DIContainer] Registered module: ApiClient","details":{}}
[2025-10-26T22:47:14.310Z] [INFO] {"timestamp":"2025-10-26T22:47:14.310Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T22:47:14.312Z] [INFO] {"timestamp":"2025-10-26T22:47:14.312Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T22:47:14.312Z] [INFO] {"timestamp":"2025-10-26T22:47:14.312Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T22:47:14.313Z] [INFO] {"timestamp":"2025-10-26T22:47:14.313Z","level":"INFO","message":"[DIContainer] Registered module: DiffGenerator","details":{}}
[2025-10-26T22:47:14.313Z] [INFO] {"timestamp":"2025-10-26T22:47:14.313Z","level":"INFO","message":"[CoreLogic] Registered module: DiffGenerator from /utils/diff-generator.js","details":{}}
[2025-10-26T22:47:14.313Z] [INFO] {"timestamp":"2025-10-26T22:47:14.313Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T22:47:14.314Z] [INFO] {"timestamp":"2025-10-26T22:47:14.314Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T22:47:14.314Z] [INFO] {"timestamp":"2025-10-26T22:47:14.314Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T22:47:14.314Z] [INFO] {"timestamp":"2025-10-26T22:47:14.314Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T22:47:14.315Z] [INFO] {"timestamp":"2025-10-26T22:47:14.315Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T22:47:14.315Z] [INFO] {"timestamp":"2025-10-26T22:47:14.315Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T22:47:14.316Z] [INFO] {"timestamp":"2025-10-26T22:47:14.316Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T22:47:14.316Z] [INFO] {"timestamp":"2025-10-26T22:47:14.316Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T22:47:14.317Z] [INFO] {"timestamp":"2025-10-26T22:47:14.317Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T22:47:14.317Z] [INFO] {"timestamp":"2025-10-26T22:47:14.317Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T22:47:14.318Z] [INFO] {"timestamp":"2025-10-26T22:47:14.318Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T22:47:14.318Z] [INFO] {"timestamp":"2025-10-26T22:47:14.318Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T22:47:14.319Z] [INFO] {"timestamp":"2025-10-26T22:47:14.319Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T22:47:14.319Z] [INFO] {"timestamp":"2025-10-26T22:47:14.319Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T22:47:14.320Z] [INFO] {"timestamp":"2025-10-26T22:47:14.320Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-26T22:47:14.320Z] [INFO] {"timestamp":"2025-10-26T22:47:14.320Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-26T22:47:14.322Z] [INFO] {"timestamp":"2025-10-26T22:47:14.322Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T22:47:14.322Z] [INFO] {"timestamp":"2025-10-26T22:47:14.322Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T22:47:14.324Z] [INFO] {"timestamp":"2025-10-26T22:47:14.324Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T22:47:14.324Z] [INFO] {"timestamp":"2025-10-26T22:47:14.324Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T22:47:14.325Z] [INFO] {"timestamp":"2025-10-26T22:47:14.325Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T22:47:14.325Z] [INFO] {"timestamp":"2025-10-26T22:47:14.325Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T22:47:14.327Z] [INFO] {"timestamp":"2025-10-26T22:47:14.327Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T22:47:14.327Z] [INFO] {"timestamp":"2025-10-26T22:47:14.327Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T22:47:14.330Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T22:47:14.330Z] [INFO] {"timestamp":"2025-10-26T22:47:14.330Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T22:47:14.330Z] [INFO] {"timestamp":"2025-10-26T22:47:14.330Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T22:47:14.331Z] [WARN] [CoreLogic] Failed to import ESM module at /personas/CodeRefactorerPersona.js; falling back to legacy evaluator. TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
TypeError: Failed to resolve module specifier "./base-persona.js". Invalid relative url or base scheme isn't hierarchical.
[2025-10-26T22:47:14.331Z] [INFO] {"timestamp":"2025-10-26T22:47:14.331Z","level":"INFO","message":"[DIContainer] Registered module: code-refactorer-persona","details":{}}
[2025-10-26T22:47:14.331Z] [INFO] {"timestamp":"2025-10-26T22:47:14.331Z","level":"INFO","message":"[CoreLogic] Registered module: code-refactorer-persona from /personas/CodeRefactorerPersona.js","details":{}}
[2025-10-26T22:47:14.331Z] [INFO] {"timestamp":"2025-10-26T22:47:14.331Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T22:47:14.332Z] [INFO] {"timestamp":"2025-10-26T22:47:14.332Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T22:47:14.795Z] [INFO] {"timestamp":"2025-10-26T22:47:14.795Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T22:47:14.796Z] [INFO] {"timestamp":"2025-10-26T22:47:14.796Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-26T22:47:14.797Z] [INFO] {"timestamp":"2025-10-26T22:47:14.797Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-26T22:47:14.797Z] [WARN] {"timestamp":"2025-10-26T22:47:14.797Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-26T22:47:14.798Z] [INFO] {"timestamp":"2025-10-26T22:47:14.798Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-26T22:47:14.798Z] [INFO] {"timestamp":"2025-10-26T22:47:14.798Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-26T22:47:14.799Z] [WARN] {"timestamp":"2025-10-26T22:47:14.799Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-26T22:47:14.800Z] [INFO] {"timestamp":"2025-10-26T22:47:14.800Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-26T22:47:14.800Z] [WARN] {"timestamp":"2025-10-26T22:47:14.800Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-26T22:47:14.801Z] [INFO] {"timestamp":"2025-10-26T22:47:14.800Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-26T22:47:14.801Z] [INFO] {"timestamp":"2025-10-26T22:47:14.801Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-26T22:47:14.801Z] [INFO] {"timestamp":"2025-10-26T22:47:14.801Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-26T22:47:14.802Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Failed to resolve dependency 'VFSExplorer' for module 'UI'.
Dependency chain: UI  VFSExplorer
Original error: [DIContainer] Service not found: VFSExplorer
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, VerificationManager, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'VFSExplorer' for module 'UI'.
Dependency chain: UI  VFSExplorer
Original error: [DIContainer] Service not found: VFSExplorer
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, VerificationManager, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:300:16)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:47:14.803Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Failed to resolve dependency 'VFSExplorer' for module 'UI'.
Dependency chain: UI  VFSExplorer
Original error: [DIContainer] Service not found: VFSExplorer
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, VerificationManager, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'VFSExplorer' for module 'UI'.
Dependency chain: UI  VFSExplorer
Original error: [DIContainer] Service not found: VFSExplorer
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, VerificationManager, SentinelFSM, SentinelTools, MetaToolCreator, ModelArena, UI, code-refactorer-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:300:16)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:47:50.379Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:47:50.380Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:47:50.388Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:47:50.388Z] [LOG] [API] Checking server status...
[2025-10-26T22:47:50.399Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:47:50.391Z"
}
[2025-10-26T22:47:50.399Z] [LOG] [API] WebGPU available
[2025-10-26T22:47:50.406Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:47:50.406Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:47:50.406Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:47:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:48:50.360Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:48:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:48:50.466Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:48:50.466Z] [LOG] [API] Checking server status...
[2025-10-26T22:48:50.476Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:48:50.467Z"
}
[2025-10-26T22:48:50.476Z] [LOG] [API] WebGPU available
[2025-10-26T22:48:50.479Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:48:50.479Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:48:50.479Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:48:50.497Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:49:50.306Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:49:50.307Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:49:50.310Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:49:50.310Z] [LOG] [API] Checking server status...
[2025-10-26T22:49:50.321Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:49:50.312Z"
}
[2025-10-26T22:49:50.321Z] [LOG] [API] WebGPU available
[2025-10-26T22:49:50.343Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:49:50.343Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:49:50.343Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:49:50.358Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:50:50.355Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:50:50.355Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:50:50.402Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:50:50.402Z] [LOG] [API] Checking server status...
[2025-10-26T22:50:50.415Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:50:50.404Z"
}
[2025-10-26T22:50:50.415Z] [LOG] [API] WebGPU available
[2025-10-26T22:50:50.428Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:50:50.428Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:50:50.428Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:50:50.449Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:51:50.342Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:51:50.342Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:51:50.345Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:51:50.345Z] [LOG] [API] Checking server status...
[2025-10-26T22:51:50.355Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:51:50.347Z"
}
[2025-10-26T22:51:50.355Z] [LOG] [API] WebGPU available
[2025-10-26T22:51:50.373Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:51:50.373Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:51:50.373Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:51:50.397Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:52:50.529Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:52:50.529Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:52:50.531Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:52:50.531Z] [LOG] [API] Checking server status...
[2025-10-26T22:52:50.541Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:52:50.532Z"
}
[2025-10-26T22:52:50.541Z] [LOG] [API] WebGPU available
[2025-10-26T22:52:50.548Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:52:50.548Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:52:50.548Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:52:50.563Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:53:50.350Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:53:50.350Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:53:50.353Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:53:50.353Z] [LOG] [API] Checking server status...
[2025-10-26T22:53:50.364Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:53:50.354Z"
}
[2025-10-26T22:53:50.364Z] [LOG] [API] WebGPU available
[2025-10-26T22:53:50.381Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:53:50.381Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:53:50.381Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:53:50.399Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:54:50.372Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:54:50.372Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:54:50.450Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:54:50.450Z] [LOG] [API] Checking server status...
[2025-10-26T22:54:50.459Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:54:50.451Z"
}
[2025-10-26T22:54:50.459Z] [LOG] [API] WebGPU available
[2025-10-26T22:54:50.466Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:54:50.466Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:54:50.466Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:54:50.481Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:55:50.366Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:55:50.367Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:55:50.370Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:55:50.370Z] [LOG] [API] Checking server status...
[2025-10-26T22:55:50.382Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:55:50.372Z"
}
[2025-10-26T22:55:50.382Z] [LOG] [API] WebGPU available
[2025-10-26T22:55:50.397Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:55:50.397Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:55:50.397Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:55:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:56:50.359Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:56:50.360Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:56:50.433Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:56:50.433Z] [LOG] [API] Checking server status...
[2025-10-26T22:56:50.441Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:56:50.434Z"
}
[2025-10-26T22:56:50.441Z] [LOG] [API] WebGPU available
[2025-10-26T22:56:50.444Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:56:50.444Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:56:50.444Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:56:50.458Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:57:15.785Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:57:15.786Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:57:15.825Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:57:15.825Z] [LOG] [API] Checking server status...
[2025-10-26T22:57:15.831Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:57:15.828Z"
}
[2025-10-26T22:57:15.831Z] [LOG] [API] WebGPU available
[2025-10-26T22:57:15.837Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:57:15.837Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:57:15.837Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:57:15.852Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:57:19.617Z] [LOG] [CoreLogic] Loaded module from /upgrades/state-manager.js: StateManager
[2025-10-26T22:57:19.617Z] [INFO] {"timestamp":"2025-10-26T22:57:19.617Z","level":"INFO","message":"[DIContainer] Registered module: StateManager","details":{}}
[2025-10-26T22:57:19.617Z] [INFO] {"timestamp":"2025-10-26T22:57:19.617Z","level":"INFO","message":"[CoreLogic] Registered module: StateManager from /upgrades/state-manager.js","details":{}}
[2025-10-26T22:57:19.619Z] [INFO] {"timestamp":"2025-10-26T22:57:19.619Z","level":"INFO","message":"[DIContainer] Registered module: LocalLLM","details":{}}
[2025-10-26T22:57:19.619Z] [INFO] {"timestamp":"2025-10-26T22:57:19.619Z","level":"INFO","message":"[CoreLogic] Registered module: LocalLLM from /upgrades/local-llm.js","details":{}}
[2025-10-26T22:57:19.620Z] [INFO] {"timestamp":"2025-10-26T22:57:19.620Z","level":"INFO","message":"[DIContainer] Registered module: HybridLLMProvider","details":{}}
[2025-10-26T22:57:19.620Z] [INFO] {"timestamp":"2025-10-26T22:57:19.620Z","level":"INFO","message":"[CoreLogic] Registered module: HybridLLMProvider from /upgrades/hybrid-llm-provider.js","details":{}}
[2025-10-26T22:57:19.621Z] [INFO] {"timestamp":"2025-10-26T22:57:19.621Z","level":"INFO","message":"[DIContainer] Registered module: ApiClient","details":{}}
[2025-10-26T22:57:19.621Z] [INFO] {"timestamp":"2025-10-26T22:57:19.621Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T22:57:19.623Z] [INFO] {"timestamp":"2025-10-26T22:57:19.623Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T22:57:19.623Z] [INFO] {"timestamp":"2025-10-26T22:57:19.623Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T22:57:19.624Z] [INFO] {"timestamp":"2025-10-26T22:57:19.624Z","level":"INFO","message":"[DIContainer] Registered module: DiffGenerator","details":{}}
[2025-10-26T22:57:19.624Z] [INFO] {"timestamp":"2025-10-26T22:57:19.624Z","level":"INFO","message":"[CoreLogic] Registered module: DiffGenerator from /utils/diff-generator.js","details":{}}
[2025-10-26T22:57:19.624Z] [INFO] {"timestamp":"2025-10-26T22:57:19.624Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T22:57:19.624Z] [INFO] {"timestamp":"2025-10-26T22:57:19.624Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T22:57:19.625Z] [INFO] {"timestamp":"2025-10-26T22:57:19.624Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T22:57:19.625Z] [INFO] {"timestamp":"2025-10-26T22:57:19.625Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T22:57:19.626Z] [INFO] {"timestamp":"2025-10-26T22:57:19.625Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T22:57:19.626Z] [INFO] {"timestamp":"2025-10-26T22:57:19.626Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T22:57:19.627Z] [INFO] {"timestamp":"2025-10-26T22:57:19.627Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T22:57:19.627Z] [INFO] {"timestamp":"2025-10-26T22:57:19.627Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T22:57:19.628Z] [INFO] {"timestamp":"2025-10-26T22:57:19.628Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T22:57:19.628Z] [INFO] {"timestamp":"2025-10-26T22:57:19.628Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T22:57:19.629Z] [INFO] {"timestamp":"2025-10-26T22:57:19.629Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T22:57:19.629Z] [INFO] {"timestamp":"2025-10-26T22:57:19.629Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T22:57:19.630Z] [INFO] {"timestamp":"2025-10-26T22:57:19.630Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T22:57:19.630Z] [INFO] {"timestamp":"2025-10-26T22:57:19.630Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T22:57:19.631Z] [INFO] {"timestamp":"2025-10-26T22:57:19.631Z","level":"INFO","message":"[DIContainer] Registered module: ToastNotifications","details":{}}
[2025-10-26T22:57:19.631Z] [INFO] {"timestamp":"2025-10-26T22:57:19.631Z","level":"INFO","message":"[CoreLogic] Registered module: ToastNotifications from /upgrades/toast-notifications.js","details":{}}
[2025-10-26T22:57:19.632Z] [INFO] {"timestamp":"2025-10-26T22:57:19.632Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-26T22:57:19.632Z] [INFO] {"timestamp":"2025-10-26T22:57:19.632Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-26T22:57:19.634Z] [INFO] {"timestamp":"2025-10-26T22:57:19.633Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-26T22:57:19.634Z] [INFO] {"timestamp":"2025-10-26T22:57:19.634Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-26T22:57:19.635Z] [INFO] {"timestamp":"2025-10-26T22:57:19.635Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-26T22:57:19.635Z] [INFO] {"timestamp":"2025-10-26T22:57:19.635Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-26T22:57:19.636Z] [INFO] {"timestamp":"2025-10-26T22:57:19.636Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-26T22:57:19.636Z] [INFO] {"timestamp":"2025-10-26T22:57:19.636Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-26T22:57:19.637Z] [INFO] {"timestamp":"2025-10-26T22:57:19.637Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-26T22:57:19.637Z] [INFO] {"timestamp":"2025-10-26T22:57:19.637Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-26T22:57:19.638Z] [INFO] {"timestamp":"2025-10-26T22:57:19.638Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-26T22:57:19.638Z] [INFO] {"timestamp":"2025-10-26T22:57:19.638Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-26T22:57:19.639Z] [INFO] {"timestamp":"2025-10-26T22:57:19.639Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-26T22:57:19.640Z] [INFO] {"timestamp":"2025-10-26T22:57:19.639Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-26T22:57:19.641Z] [INFO] {"timestamp":"2025-10-26T22:57:19.641Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-26T22:57:19.641Z] [INFO] {"timestamp":"2025-10-26T22:57:19.641Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-26T22:57:19.642Z] [INFO] {"timestamp":"2025-10-26T22:57:19.642Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-26T22:57:19.642Z] [INFO] {"timestamp":"2025-10-26T22:57:19.642Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-26T22:57:19.643Z] [INFO] {"timestamp":"2025-10-26T22:57:19.643Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-26T22:57:19.643Z] [INFO] {"timestamp":"2025-10-26T22:57:19.643Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-26T22:57:19.644Z] [INFO] {"timestamp":"2025-10-26T22:57:19.644Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-26T22:57:19.644Z] [INFO] {"timestamp":"2025-10-26T22:57:19.644Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-26T22:57:19.645Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-26T22:57:19.645Z] [ERROR] {"timestamp":"2025-10-26T22:57:19.645Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T22:57:19.645Z] [WARN] {"timestamp":"2025-10-26T22:57:19.645Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-26T22:57:19.645Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-26T22:57:19.645Z] [ERROR] {"timestamp":"2025-10-26T22:57:19.645Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T22:57:19.645Z] [WARN] {"timestamp":"2025-10-26T22:57:19.645Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-26T22:57:19.646Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-26T22:57:19.646Z] [ERROR] {"timestamp":"2025-10-26T22:57:19.646Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T22:57:19.646Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:57:19.646Z] [WARN] {"timestamp":"2025-10-26T22:57:19.646Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-26T22:57:19.647Z] [INFO] {"timestamp":"2025-10-26T22:57:19.647Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-26T22:57:19.647Z] [INFO] {"timestamp":"2025-10-26T22:57:19.647Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-26T22:57:19.648Z] [INFO] {"timestamp":"2025-10-26T22:57:19.648Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-26T22:57:19.648Z] [INFO] {"timestamp":"2025-10-26T22:57:19.648Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-26T22:57:19.650Z] [INFO] {"timestamp":"2025-10-26T22:57:19.650Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T22:57:19.650Z] [INFO] {"timestamp":"2025-10-26T22:57:19.650Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T22:57:19.651Z] [INFO] {"timestamp":"2025-10-26T22:57:19.651Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T22:57:19.651Z] [INFO] {"timestamp":"2025-10-26T22:57:19.651Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T22:57:19.652Z] [INFO] {"timestamp":"2025-10-26T22:57:19.652Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-26T22:57:19.652Z] [INFO] {"timestamp":"2025-10-26T22:57:19.652Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-26T22:57:19.653Z] [INFO] {"timestamp":"2025-10-26T22:57:19.653Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T22:57:19.653Z] [INFO] {"timestamp":"2025-10-26T22:57:19.653Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T22:57:19.654Z] [INFO] {"timestamp":"2025-10-26T22:57:19.654Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T22:57:19.654Z] [INFO] {"timestamp":"2025-10-26T22:57:19.654Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T22:57:19.657Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T22:57:19.657Z] [INFO] {"timestamp":"2025-10-26T22:57:19.657Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T22:57:19.657Z] [INFO] {"timestamp":"2025-10-26T22:57:19.657Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T22:57:19.657Z] [LOG] [CoreLogic] Loaded module from /personas/CodeRefactorerPersona.js: NO_METADATA
[2025-10-26T22:57:19.657Z] [ERROR] {"timestamp":"2025-10-26T22:57:19.657Z","level":"ERROR","message":"[CoreLogic] Module load failed for /personas/CodeRefactorerPersona.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T22:57:19.657Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:57:19.657Z] [WARN] {"timestamp":"2025-10-26T22:57:19.657Z","level":"WARN","message":"[CoreLogic] Module at /personas/CodeRefactorerPersona.js missing metadata. Module:","details":{}}
[2025-10-26T22:57:19.657Z] [INFO] {"timestamp":"2025-10-26T22:57:19.657Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T22:57:19.658Z] [INFO] {"timestamp":"2025-10-26T22:57:19.658Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T22:57:20.106Z] [INFO] {"timestamp":"2025-10-26T22:57:20.106Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T22:57:20.107Z] [INFO] {"timestamp":"2025-10-26T22:57:20.107Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-26T22:57:20.107Z] [INFO] {"timestamp":"2025-10-26T22:57:20.107Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-26T22:57:20.108Z] [WARN] {"timestamp":"2025-10-26T22:57:20.108Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-26T22:57:20.108Z] [INFO] {"timestamp":"2025-10-26T22:57:20.108Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-26T22:57:20.108Z] [INFO] {"timestamp":"2025-10-26T22:57:20.108Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-26T22:57:20.108Z] [WARN] {"timestamp":"2025-10-26T22:57:20.108Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-26T22:57:20.109Z] [INFO] {"timestamp":"2025-10-26T22:57:20.109Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-26T22:57:20.110Z] [WARN] {"timestamp":"2025-10-26T22:57:20.110Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-26T22:57:20.110Z] [INFO] {"timestamp":"2025-10-26T22:57:20.110Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-26T22:57:20.110Z] [INFO] {"timestamp":"2025-10-26T22:57:20.110Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-26T22:57:20.111Z] [INFO] {"timestamp":"2025-10-26T22:57:20.111Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-26T22:57:20.112Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Failed to resolve dependency 'Persona' for module 'CycleLogic'.
Dependency chain: CycleLogic  Persona
Original error: [DIContainer] Failed to resolve dependency 'base-persona' for module 'Persona'.
Dependency chain: Persona  base-persona
Original error: [DIContainer] Service not found: base-persona
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'Persona' for module 'CycleLogic'.
Dependency chain: CycleLogic  Persona
Original error: [DIContainer] Failed to resolve dependency 'base-persona' for module 'Persona'.
Dependency chain: Persona  base-persona
Original error: [DIContainer] Service not found: base-persona
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:57:20.112Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:57:20.112Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Failed to resolve dependency 'Persona' for module 'CycleLogic'.
Dependency chain: CycleLogic  Persona
Original error: [DIContainer] Failed to resolve dependency 'base-persona' for module 'Persona'.
Dependency chain: Persona  base-persona
Original error: [DIContainer] Service not found: base-persona
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'Persona' for module 'CycleLogic'.
Dependency chain: CycleLogic  Persona
Original error: [DIContainer] Failed to resolve dependency 'base-persona' for module 'Persona'.
Dependency chain: Persona  base-persona
Original error: [DIContainer] Service not found: base-persona
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:57:20.112Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:57:50.334Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:57:50.335Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:57:50.338Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:57:50.338Z] [LOG] [API] Checking server status...
[2025-10-26T22:57:50.348Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:57:50.340Z"
}
[2025-10-26T22:57:50.348Z] [LOG] [API] WebGPU available
[2025-10-26T22:57:50.365Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:57:50.365Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:57:50.366Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:57:50.379Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:58:50.505Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:58:50.505Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:58:50.507Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:58:50.507Z] [LOG] [API] Checking server status...
[2025-10-26T22:58:50.517Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:58:50.509Z"
}
[2025-10-26T22:58:50.517Z] [LOG] [API] WebGPU available
[2025-10-26T22:58:50.521Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:58:50.521Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:58:50.521Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:58:50.541Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:59:20.157Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:59:20.158Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:59:20.194Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:59:20.194Z] [LOG] [API] Checking server status...
[2025-10-26T22:59:20.202Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:59:20.196Z"
}
[2025-10-26T22:59:20.202Z] [LOG] [API] WebGPU available
[2025-10-26T22:59:20.213Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:59:20.213Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:59:20.213Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:59:20.222Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T22:59:22.637Z] [LOG] [CoreLogic] Loaded module from /upgrades/state-manager.js: StateManager
[2025-10-26T22:59:22.637Z] [INFO] {"timestamp":"2025-10-26T22:59:22.637Z","level":"INFO","message":"[DIContainer] Registered module: StateManager","details":{}}
[2025-10-26T22:59:22.637Z] [INFO] {"timestamp":"2025-10-26T22:59:22.637Z","level":"INFO","message":"[CoreLogic] Registered module: StateManager from /upgrades/state-manager.js","details":{}}
[2025-10-26T22:59:22.639Z] [INFO] {"timestamp":"2025-10-26T22:59:22.639Z","level":"INFO","message":"[DIContainer] Registered module: LocalLLM","details":{}}
[2025-10-26T22:59:22.639Z] [INFO] {"timestamp":"2025-10-26T22:59:22.639Z","level":"INFO","message":"[CoreLogic] Registered module: LocalLLM from /upgrades/local-llm.js","details":{}}
[2025-10-26T22:59:22.640Z] [INFO] {"timestamp":"2025-10-26T22:59:22.640Z","level":"INFO","message":"[DIContainer] Registered module: HybridLLMProvider","details":{}}
[2025-10-26T22:59:22.640Z] [INFO] {"timestamp":"2025-10-26T22:59:22.640Z","level":"INFO","message":"[CoreLogic] Registered module: HybridLLMProvider from /upgrades/hybrid-llm-provider.js","details":{}}
[2025-10-26T22:59:22.641Z] [INFO] {"timestamp":"2025-10-26T22:59:22.641Z","level":"INFO","message":"[DIContainer] Registered module: ApiClient","details":{}}
[2025-10-26T22:59:22.641Z] [INFO] {"timestamp":"2025-10-26T22:59:22.641Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T22:59:22.643Z] [INFO] {"timestamp":"2025-10-26T22:59:22.643Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T22:59:22.643Z] [INFO] {"timestamp":"2025-10-26T22:59:22.643Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T22:59:22.644Z] [INFO] {"timestamp":"2025-10-26T22:59:22.644Z","level":"INFO","message":"[DIContainer] Registered module: DiffGenerator","details":{}}
[2025-10-26T22:59:22.644Z] [INFO] {"timestamp":"2025-10-26T22:59:22.644Z","level":"INFO","message":"[CoreLogic] Registered module: DiffGenerator from /utils/diff-generator.js","details":{}}
[2025-10-26T22:59:22.644Z] [INFO] {"timestamp":"2025-10-26T22:59:22.644Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T22:59:22.645Z] [INFO] {"timestamp":"2025-10-26T22:59:22.645Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T22:59:22.645Z] [INFO] {"timestamp":"2025-10-26T22:59:22.645Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T22:59:22.645Z] [INFO] {"timestamp":"2025-10-26T22:59:22.645Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T22:59:22.646Z] [INFO] {"timestamp":"2025-10-26T22:59:22.646Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T22:59:22.646Z] [INFO] {"timestamp":"2025-10-26T22:59:22.646Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T22:59:22.647Z] [INFO] {"timestamp":"2025-10-26T22:59:22.647Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T22:59:22.647Z] [INFO] {"timestamp":"2025-10-26T22:59:22.647Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T22:59:22.648Z] [INFO] {"timestamp":"2025-10-26T22:59:22.648Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T22:59:22.648Z] [INFO] {"timestamp":"2025-10-26T22:59:22.648Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T22:59:22.650Z] [INFO] {"timestamp":"2025-10-26T22:59:22.650Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T22:59:22.650Z] [INFO] {"timestamp":"2025-10-26T22:59:22.650Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T22:59:22.651Z] [INFO] {"timestamp":"2025-10-26T22:59:22.651Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T22:59:22.651Z] [INFO] {"timestamp":"2025-10-26T22:59:22.651Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T22:59:22.652Z] [INFO] {"timestamp":"2025-10-26T22:59:22.652Z","level":"INFO","message":"[DIContainer] Registered module: ToastNotifications","details":{}}
[2025-10-26T22:59:22.652Z] [INFO] {"timestamp":"2025-10-26T22:59:22.652Z","level":"INFO","message":"[CoreLogic] Registered module: ToastNotifications from /upgrades/toast-notifications.js","details":{}}
[2025-10-26T22:59:22.653Z] [INFO] {"timestamp":"2025-10-26T22:59:22.653Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-26T22:59:22.653Z] [INFO] {"timestamp":"2025-10-26T22:59:22.653Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-26T22:59:22.655Z] [INFO] {"timestamp":"2025-10-26T22:59:22.654Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-26T22:59:22.655Z] [INFO] {"timestamp":"2025-10-26T22:59:22.655Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-26T22:59:22.656Z] [INFO] {"timestamp":"2025-10-26T22:59:22.656Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-26T22:59:22.656Z] [INFO] {"timestamp":"2025-10-26T22:59:22.656Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-26T22:59:22.657Z] [INFO] {"timestamp":"2025-10-26T22:59:22.657Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-26T22:59:22.657Z] [INFO] {"timestamp":"2025-10-26T22:59:22.657Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-26T22:59:22.659Z] [INFO] {"timestamp":"2025-10-26T22:59:22.659Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-26T22:59:22.659Z] [INFO] {"timestamp":"2025-10-26T22:59:22.659Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-26T22:59:22.660Z] [INFO] {"timestamp":"2025-10-26T22:59:22.660Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-26T22:59:22.660Z] [INFO] {"timestamp":"2025-10-26T22:59:22.660Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-26T22:59:22.661Z] [INFO] {"timestamp":"2025-10-26T22:59:22.661Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-26T22:59:22.661Z] [INFO] {"timestamp":"2025-10-26T22:59:22.661Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-26T22:59:22.662Z] [INFO] {"timestamp":"2025-10-26T22:59:22.662Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-26T22:59:22.662Z] [INFO] {"timestamp":"2025-10-26T22:59:22.662Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-26T22:59:22.663Z] [INFO] {"timestamp":"2025-10-26T22:59:22.663Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-26T22:59:22.663Z] [INFO] {"timestamp":"2025-10-26T22:59:22.663Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-26T22:59:22.664Z] [INFO] {"timestamp":"2025-10-26T22:59:22.664Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-26T22:59:22.664Z] [INFO] {"timestamp":"2025-10-26T22:59:22.664Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-26T22:59:22.665Z] [INFO] {"timestamp":"2025-10-26T22:59:22.665Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-26T22:59:22.665Z] [INFO] {"timestamp":"2025-10-26T22:59:22.665Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-26T22:59:22.666Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-26T22:59:22.666Z] [ERROR] {"timestamp":"2025-10-26T22:59:22.666Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T22:59:22.666Z] [WARN] {"timestamp":"2025-10-26T22:59:22.666Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-26T22:59:22.666Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-26T22:59:22.666Z] [ERROR] {"timestamp":"2025-10-26T22:59:22.666Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T22:59:22.666Z] [WARN] {"timestamp":"2025-10-26T22:59:22.666Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-26T22:59:22.667Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-26T22:59:22.667Z] [ERROR] {"timestamp":"2025-10-26T22:59:22.667Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T22:59:22.667Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:59:22.667Z] [WARN] {"timestamp":"2025-10-26T22:59:22.667Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-26T22:59:22.668Z] [INFO] {"timestamp":"2025-10-26T22:59:22.668Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-26T22:59:22.668Z] [INFO] {"timestamp":"2025-10-26T22:59:22.668Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-26T22:59:22.669Z] [INFO] {"timestamp":"2025-10-26T22:59:22.669Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-26T22:59:22.669Z] [INFO] {"timestamp":"2025-10-26T22:59:22.669Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-26T22:59:22.671Z] [INFO] {"timestamp":"2025-10-26T22:59:22.671Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T22:59:22.671Z] [INFO] {"timestamp":"2025-10-26T22:59:22.671Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T22:59:22.672Z] [INFO] {"timestamp":"2025-10-26T22:59:22.672Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T22:59:22.672Z] [INFO] {"timestamp":"2025-10-26T22:59:22.672Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T22:59:22.673Z] [INFO] {"timestamp":"2025-10-26T22:59:22.673Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-26T22:59:22.673Z] [INFO] {"timestamp":"2025-10-26T22:59:22.673Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-26T22:59:22.674Z] [INFO] {"timestamp":"2025-10-26T22:59:22.674Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T22:59:22.675Z] [INFO] {"timestamp":"2025-10-26T22:59:22.674Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T22:59:22.676Z] [INFO] {"timestamp":"2025-10-26T22:59:22.676Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T22:59:22.676Z] [INFO] {"timestamp":"2025-10-26T22:59:22.676Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T22:59:22.678Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T22:59:22.679Z] [INFO] {"timestamp":"2025-10-26T22:59:22.678Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T22:59:22.679Z] [INFO] {"timestamp":"2025-10-26T22:59:22.679Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T22:59:22.679Z] [LOG] [CoreLogic] Loaded module from /personas/CodeRefactorerPersona.js: NO_METADATA
[2025-10-26T22:59:22.679Z] [ERROR] {"timestamp":"2025-10-26T22:59:22.679Z","level":"ERROR","message":"[CoreLogic] Module load failed for /personas/CodeRefactorerPersona.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T22:59:22.679Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:59:22.679Z] [WARN] {"timestamp":"2025-10-26T22:59:22.679Z","level":"WARN","message":"[CoreLogic] Module at /personas/CodeRefactorerPersona.js missing metadata. Module:","details":{}}
[2025-10-26T22:59:22.679Z] [INFO] {"timestamp":"2025-10-26T22:59:22.679Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T22:59:22.680Z] [INFO] {"timestamp":"2025-10-26T22:59:22.680Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T22:59:23.127Z] [INFO] {"timestamp":"2025-10-26T22:59:23.127Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T22:59:23.128Z] [INFO] {"timestamp":"2025-10-26T22:59:23.128Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-26T22:59:23.128Z] [INFO] {"timestamp":"2025-10-26T22:59:23.128Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-26T22:59:23.129Z] [WARN] {"timestamp":"2025-10-26T22:59:23.129Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-26T22:59:23.129Z] [INFO] {"timestamp":"2025-10-26T22:59:23.129Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-26T22:59:23.129Z] [INFO] {"timestamp":"2025-10-26T22:59:23.129Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-26T22:59:23.129Z] [WARN] {"timestamp":"2025-10-26T22:59:23.129Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-26T22:59:23.130Z] [INFO] {"timestamp":"2025-10-26T22:59:23.130Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-26T22:59:23.131Z] [WARN] {"timestamp":"2025-10-26T22:59:23.131Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-26T22:59:23.131Z] [INFO] {"timestamp":"2025-10-26T22:59:23.131Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-26T22:59:23.131Z] [INFO] {"timestamp":"2025-10-26T22:59:23.131Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-26T22:59:23.132Z] [INFO] {"timestamp":"2025-10-26T22:59:23.131Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-26T22:59:23.133Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Failed to resolve dependency 'Persona' for module 'CycleLogic'.
Dependency chain: CycleLogic  Persona
Original error: [DIContainer] Failed to resolve dependency 'base-persona' for module 'Persona'.
Dependency chain: Persona  base-persona
Original error: [DIContainer] Service not found: base-persona
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'Persona' for module 'CycleLogic'.
Dependency chain: CycleLogic  Persona
Original error: [DIContainer] Failed to resolve dependency 'base-persona' for module 'Persona'.
Dependency chain: Persona  base-persona
Original error: [DIContainer] Service not found: base-persona
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:59:23.133Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:59:23.133Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Failed to resolve dependency 'Persona' for module 'CycleLogic'.
Dependency chain: CycleLogic  Persona
Original error: [DIContainer] Failed to resolve dependency 'base-persona' for module 'Persona'.
Dependency chain: Persona  base-persona
Original error: [DIContainer] Service not found: base-persona
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'Persona' for module 'CycleLogic'.
Dependency chain: CycleLogic  Persona
Original error: [DIContainer] Failed to resolve dependency 'base-persona' for module 'Persona'.
Dependency chain: Persona  base-persona
Original error: [DIContainer] Service not found: base-persona
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:299:24)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T22:59:23.133Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T22:59:50.247Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T22:59:50.247Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T22:59:50.250Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T22:59:50.250Z] [LOG] [API] Checking server status...
[2025-10-26T22:59:50.261Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T22:59:50.252Z"
}
[2025-10-26T22:59:50.261Z] [LOG] [API] WebGPU available
[2025-10-26T22:59:50.278Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T22:59:50.279Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T22:59:50.279Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T22:59:50.301Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:00:50.277Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:00:50.277Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:00:50.303Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:00:50.303Z] [LOG] [API] Checking server status...
[2025-10-26T23:00:50.315Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:00:50.305Z"
}
[2025-10-26T23:00:50.315Z] [LOG] [API] WebGPU available
[2025-10-26T23:00:50.321Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:00:50.321Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:00:50.321Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:00:50.342Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:01:19.320Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:01:19.320Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:01:19.359Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:01:19.359Z] [LOG] [API] Checking server status...
[2025-10-26T23:01:19.369Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:01:19.361Z"
}
[2025-10-26T23:01:19.369Z] [LOG] [API] WebGPU available
[2025-10-26T23:01:19.381Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:01:19.381Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:01:19.381Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:01:19.401Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:01:23.196Z] [INFO] {"timestamp":"2025-10-26T23:01:23.196Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T23:01:23.198Z] [INFO] {"timestamp":"2025-10-26T23:01:23.198Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T23:01:23.198Z] [INFO] {"timestamp":"2025-10-26T23:01:23.198Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T23:01:23.199Z] [INFO] {"timestamp":"2025-10-26T23:01:23.199Z","level":"INFO","message":"[DIContainer] Registered module: DiffGenerator","details":{}}
[2025-10-26T23:01:23.199Z] [INFO] {"timestamp":"2025-10-26T23:01:23.199Z","level":"INFO","message":"[CoreLogic] Registered module: DiffGenerator from /utils/diff-generator.js","details":{}}
[2025-10-26T23:01:23.199Z] [INFO] {"timestamp":"2025-10-26T23:01:23.199Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T23:01:23.200Z] [INFO] {"timestamp":"2025-10-26T23:01:23.200Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T23:01:23.200Z] [INFO] {"timestamp":"2025-10-26T23:01:23.200Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T23:01:23.201Z] [INFO] {"timestamp":"2025-10-26T23:01:23.200Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T23:01:23.202Z] [INFO] {"timestamp":"2025-10-26T23:01:23.202Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T23:01:23.202Z] [INFO] {"timestamp":"2025-10-26T23:01:23.202Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T23:01:23.203Z] [INFO] {"timestamp":"2025-10-26T23:01:23.203Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T23:01:23.203Z] [INFO] {"timestamp":"2025-10-26T23:01:23.203Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T23:01:23.204Z] [INFO] {"timestamp":"2025-10-26T23:01:23.204Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T23:01:23.204Z] [INFO] {"timestamp":"2025-10-26T23:01:23.204Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T23:01:23.205Z] [INFO] {"timestamp":"2025-10-26T23:01:23.205Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T23:01:23.205Z] [INFO] {"timestamp":"2025-10-26T23:01:23.205Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T23:01:23.206Z] [INFO] {"timestamp":"2025-10-26T23:01:23.206Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T23:01:23.206Z] [INFO] {"timestamp":"2025-10-26T23:01:23.206Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T23:01:23.207Z] [INFO] {"timestamp":"2025-10-26T23:01:23.207Z","level":"INFO","message":"[DIContainer] Registered module: ToastNotifications","details":{}}
[2025-10-26T23:01:23.207Z] [INFO] {"timestamp":"2025-10-26T23:01:23.207Z","level":"INFO","message":"[CoreLogic] Registered module: ToastNotifications from /upgrades/toast-notifications.js","details":{}}
[2025-10-26T23:01:23.209Z] [INFO] {"timestamp":"2025-10-26T23:01:23.209Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-26T23:01:23.209Z] [INFO] {"timestamp":"2025-10-26T23:01:23.209Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-26T23:01:23.210Z] [INFO] {"timestamp":"2025-10-26T23:01:23.210Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-26T23:01:23.210Z] [INFO] {"timestamp":"2025-10-26T23:01:23.210Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-26T23:01:23.211Z] [INFO] {"timestamp":"2025-10-26T23:01:23.211Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-26T23:01:23.211Z] [INFO] {"timestamp":"2025-10-26T23:01:23.211Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-26T23:01:23.213Z] [INFO] {"timestamp":"2025-10-26T23:01:23.213Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-26T23:01:23.213Z] [INFO] {"timestamp":"2025-10-26T23:01:23.213Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-26T23:01:23.214Z] [INFO] {"timestamp":"2025-10-26T23:01:23.214Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-26T23:01:23.214Z] [INFO] {"timestamp":"2025-10-26T23:01:23.214Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-26T23:01:23.215Z] [INFO] {"timestamp":"2025-10-26T23:01:23.215Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-26T23:01:23.215Z] [INFO] {"timestamp":"2025-10-26T23:01:23.215Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-26T23:01:23.217Z] [INFO] {"timestamp":"2025-10-26T23:01:23.217Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-26T23:01:23.217Z] [INFO] {"timestamp":"2025-10-26T23:01:23.217Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-26T23:01:23.218Z] [INFO] {"timestamp":"2025-10-26T23:01:23.218Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-26T23:01:23.218Z] [INFO] {"timestamp":"2025-10-26T23:01:23.218Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-26T23:01:23.219Z] [INFO] {"timestamp":"2025-10-26T23:01:23.219Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-26T23:01:23.219Z] [INFO] {"timestamp":"2025-10-26T23:01:23.219Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-26T23:01:23.220Z] [INFO] {"timestamp":"2025-10-26T23:01:23.220Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-26T23:01:23.220Z] [INFO] {"timestamp":"2025-10-26T23:01:23.220Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-26T23:01:23.222Z] [INFO] {"timestamp":"2025-10-26T23:01:23.221Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-26T23:01:23.222Z] [INFO] {"timestamp":"2025-10-26T23:01:23.222Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-26T23:01:23.222Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-26T23:01:23.222Z] [ERROR] {"timestamp":"2025-10-26T23:01:23.222Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T23:01:23.222Z] [WARN] {"timestamp":"2025-10-26T23:01:23.222Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-26T23:01:23.223Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-26T23:01:23.223Z] [ERROR] {"timestamp":"2025-10-26T23:01:23.223Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T23:01:23.223Z] [WARN] {"timestamp":"2025-10-26T23:01:23.223Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-26T23:01:23.223Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-26T23:01:23.223Z] [ERROR] {"timestamp":"2025-10-26T23:01:23.223Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T23:01:23.223Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T23:01:23.223Z] [WARN] {"timestamp":"2025-10-26T23:01:23.223Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-26T23:01:23.224Z] [INFO] {"timestamp":"2025-10-26T23:01:23.224Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-26T23:01:23.224Z] [INFO] {"timestamp":"2025-10-26T23:01:23.224Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-26T23:01:23.226Z] [INFO] {"timestamp":"2025-10-26T23:01:23.225Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-26T23:01:23.226Z] [INFO] {"timestamp":"2025-10-26T23:01:23.226Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-26T23:01:23.228Z] [INFO] {"timestamp":"2025-10-26T23:01:23.228Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T23:01:23.228Z] [INFO] {"timestamp":"2025-10-26T23:01:23.228Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T23:01:23.230Z] [INFO] {"timestamp":"2025-10-26T23:01:23.230Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T23:01:23.230Z] [INFO] {"timestamp":"2025-10-26T23:01:23.230Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T23:01:23.231Z] [INFO] {"timestamp":"2025-10-26T23:01:23.231Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-26T23:01:23.231Z] [INFO] {"timestamp":"2025-10-26T23:01:23.231Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-26T23:01:23.232Z] [INFO] {"timestamp":"2025-10-26T23:01:23.232Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T23:01:23.232Z] [INFO] {"timestamp":"2025-10-26T23:01:23.232Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T23:01:23.234Z] [INFO] {"timestamp":"2025-10-26T23:01:23.234Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T23:01:23.234Z] [INFO] {"timestamp":"2025-10-26T23:01:23.234Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T23:01:23.237Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T23:01:23.237Z] [INFO] {"timestamp":"2025-10-26T23:01:23.237Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T23:01:23.237Z] [INFO] {"timestamp":"2025-10-26T23:01:23.237Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T23:01:23.238Z] [INFO] {"timestamp":"2025-10-26T23:01:23.238Z","level":"INFO","message":"[DIContainer] Registered module: base-persona","details":{}}
[2025-10-26T23:01:23.238Z] [INFO] {"timestamp":"2025-10-26T23:01:23.238Z","level":"INFO","message":"[CoreLogic] Registered module: base-persona from /personas/base-persona.js","details":{}}
[2025-10-26T23:01:23.238Z] [LOG] [CoreLogic] Loaded module from /personas/CodeRefactorerPersona.js: NO_METADATA
[2025-10-26T23:01:23.238Z] [ERROR] {"timestamp":"2025-10-26T23:01:23.238Z","level":"ERROR","message":"[CoreLogic] Module load failed for /personas/CodeRefactorerPersona.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T23:01:23.238Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T23:01:23.238Z] [WARN] {"timestamp":"2025-10-26T23:01:23.238Z","level":"WARN","message":"[CoreLogic] Module at /personas/CodeRefactorerPersona.js missing metadata. Module:","details":{}}
[2025-10-26T23:01:23.238Z] [INFO] {"timestamp":"2025-10-26T23:01:23.238Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T23:01:23.239Z] [INFO] {"timestamp":"2025-10-26T23:01:23.239Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T23:01:23.682Z] [INFO] {"timestamp":"2025-10-26T23:01:23.682Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T23:01:23.682Z] [INFO] {"timestamp":"2025-10-26T23:01:23.682Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-26T23:01:23.683Z] [INFO] {"timestamp":"2025-10-26T23:01:23.683Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-26T23:01:23.683Z] [WARN] {"timestamp":"2025-10-26T23:01:23.683Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-26T23:01:23.684Z] [INFO] {"timestamp":"2025-10-26T23:01:23.684Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-26T23:01:23.684Z] [INFO] {"timestamp":"2025-10-26T23:01:23.684Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-26T23:01:23.684Z] [WARN] {"timestamp":"2025-10-26T23:01:23.684Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-26T23:01:23.685Z] [INFO] {"timestamp":"2025-10-26T23:01:23.685Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-26T23:01:23.685Z] [WARN] {"timestamp":"2025-10-26T23:01:23.685Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-26T23:01:23.686Z] [INFO] {"timestamp":"2025-10-26T23:01:23.685Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-26T23:01:23.686Z] [INFO] {"timestamp":"2025-10-26T23:01:23.686Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-26T23:01:23.686Z] [INFO] {"timestamp":"2025-10-26T23:01:23.686Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-26T23:01:23.689Z] [INFO] {"timestamp":"2025-10-26T23:01:23.689Z","level":"INFO","message":"[ReflectionStore] Initializing reflection persistence","details":{}}
[2025-10-26T23:01:23.690Z] [INFO] {"timestamp":"2025-10-26T23:01:23.690Z","level":"INFO","message":"[ReflectionStore] Database opened successfully","details":{}}
[2025-10-26T23:01:23.690Z] [INFO] {"timestamp":"2025-10-26T23:01:23.690Z","level":"INFO","message":"[SelfTester] Initialized","details":{}}
[2025-10-26T23:01:23.690Z] [INFO] {"timestamp":"2025-10-26T23:01:23.690Z","level":"INFO","message":"[BrowserAPIs] Initializing web API integration...","details":{}}
[2025-10-26T23:01:23.691Z] [INFO] {"timestamp":"2025-10-26T23:01:23.691Z","level":"INFO","message":"[BrowserAPIs] Capabilities detected:","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-26T23:01:23.691Z] [INFO] {"timestamp":"2025-10-26T23:01:23.691Z","level":"INFO","message":"[EventBus] Emitting event: browser-apis:initialized","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-26T23:01:23.692Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Failed to resolve dependency 'AgentVisualizer' for module 'UI'.
Dependency chain: UI  AgentVisualizer
Original error: [DIContainer] Failed to resolve dependency 'SentinelFSM' for module 'AgentVisualizer'.
Dependency chain: AgentVisualizer  SentinelFSM
Original error: [DIContainer] Failed to resolve dependency 'GitVFS' for module 'SentinelFSM'.
Dependency chain: SentinelFSM  GitVFS
Original error: [DIContainer] Service not found: GitVFS
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI, base-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'AgentVisualizer' for module 'UI'.
Dependency chain: UI  AgentVisualizer
Original error: [DIContainer] Failed to resolve dependency 'SentinelFSM' for module 'AgentVisualizer'.
Dependency chain: AgentVisualizer  SentinelFSM
Original error: [DIContainer] Failed to resolve dependency 'GitVFS' for module 'SentinelFSM'.
Dependency chain: SentinelFSM  GitVFS
Original error: [DIContainer] Service not found: GitVFS
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI, base-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:300:16)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T23:01:23.692Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T23:01:23.692Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Failed to resolve dependency 'AgentVisualizer' for module 'UI'.
Dependency chain: UI  AgentVisualizer
Original error: [DIContainer] Failed to resolve dependency 'SentinelFSM' for module 'AgentVisualizer'.
Dependency chain: AgentVisualizer  SentinelFSM
Original error: [DIContainer] Failed to resolve dependency 'GitVFS' for module 'SentinelFSM'.
Dependency chain: SentinelFSM  GitVFS
Original error: [DIContainer] Service not found: GitVFS
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI, base-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'AgentVisualizer' for module 'UI'.
Dependency chain: UI  AgentVisualizer
Original error: [DIContainer] Failed to resolve dependency 'SentinelFSM' for module 'AgentVisualizer'.
Dependency chain: AgentVisualizer  SentinelFSM
Original error: [DIContainer] Failed to resolve dependency 'GitVFS' for module 'SentinelFSM'.
Dependency chain: SentinelFSM  GitVFS
Original error: [DIContainer] Service not found: GitVFS
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI, base-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:300:16)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T23:01:23.692Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T23:01:50.409Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:01:50.409Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:01:50.437Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:01:50.437Z] [LOG] [API] Checking server status...
[2025-10-26T23:01:50.448Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:01:50.438Z"
}
[2025-10-26T23:01:50.448Z] [LOG] [API] WebGPU available
[2025-10-26T23:01:50.451Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:01:50.451Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:01:50.451Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:01:50.498Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:02:50.362Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:02:50.363Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:02:50.367Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:02:50.367Z] [LOG] [API] Checking server status...
[2025-10-26T23:02:50.378Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:02:50.369Z"
}
[2025-10-26T23:02:50.378Z] [LOG] [API] WebGPU available
[2025-10-26T23:02:50.386Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:02:50.386Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:02:50.386Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:02:50.419Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:03:50.301Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:03:50.301Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:03:50.384Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:03:50.384Z] [LOG] [API] Checking server status...
[2025-10-26T23:03:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:03:50.386Z"
}
[2025-10-26T23:03:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T23:03:50.398Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:03:50.398Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:03:50.398Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:03:50.406Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:04:50.609Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:04:50.609Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:04:50.611Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:04:50.611Z] [LOG] [API] Checking server status...
[2025-10-26T23:04:50.624Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:04:50.613Z"
}
[2025-10-26T23:04:50.624Z] [LOG] [API] WebGPU available
[2025-10-26T23:04:50.632Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:04:50.632Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:04:50.632Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:04:50.647Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:05:50.262Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:05:50.262Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:05:50.265Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:05:50.265Z] [LOG] [API] Checking server status...
[2025-10-26T23:05:50.276Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:05:50.267Z"
}
[2025-10-26T23:05:50.276Z] [LOG] [API] WebGPU available
[2025-10-26T23:05:50.294Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:05:50.294Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:05:50.294Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:05:50.310Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:06:50.398Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:06:50.398Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:06:50.431Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:06:50.431Z] [LOG] [API] Checking server status...
[2025-10-26T23:06:50.442Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:06:50.433Z"
}
[2025-10-26T23:06:50.442Z] [LOG] [API] WebGPU available
[2025-10-26T23:06:50.449Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:06:50.449Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:06:50.449Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:06:50.461Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:07:50.335Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:07:50.335Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:07:50.338Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:07:50.338Z] [LOG] [API] Checking server status...
[2025-10-26T23:07:50.348Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:07:50.340Z"
}
[2025-10-26T23:07:50.348Z] [LOG] [API] WebGPU available
[2025-10-26T23:07:50.365Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:07:50.365Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:07:50.365Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:07:50.392Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:08:50.294Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:08:50.295Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:08:50.386Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:08:50.386Z] [LOG] [API] Checking server status...
[2025-10-26T23:08:50.395Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:08:50.387Z"
}
[2025-10-26T23:08:50.395Z] [LOG] [API] WebGPU available
[2025-10-26T23:08:50.402Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:08:50.402Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:08:50.402Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:08:50.416Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:09:50.308Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:09:50.309Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:09:50.312Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:09:50.312Z] [LOG] [API] Checking server status...
[2025-10-26T23:09:50.323Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:09:50.314Z"
}
[2025-10-26T23:09:50.323Z] [LOG] [API] WebGPU available
[2025-10-26T23:09:50.340Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:09:50.340Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:09:50.340Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:09:50.353Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:10:50.519Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:10:50.519Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:10:50.522Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:10:50.522Z] [LOG] [API] Checking server status...
[2025-10-26T23:10:50.532Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:10:50.523Z"
}
[2025-10-26T23:10:50.532Z] [LOG] [API] WebGPU available
[2025-10-26T23:10:50.540Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:10:50.540Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:10:50.540Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:10:50.554Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:11:00.616Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:11:00.616Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:11:00.657Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:11:00.657Z] [LOG] [API] Checking server status...
[2025-10-26T23:11:00.670Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:11:00.660Z"
}
[2025-10-26T23:11:00.670Z] [LOG] [API] WebGPU available
[2025-10-26T23:11:00.677Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:11:00.677Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:11:00.677Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:11:00.696Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
[2025-10-26T23:11:02.230Z] [INFO] {"timestamp":"2025-10-26T23:11:02.230Z","level":"INFO","message":"[CoreLogic] Registered module: ApiClient from /upgrades/api-client.js","details":{}}
[2025-10-26T23:11:02.232Z] [INFO] {"timestamp":"2025-10-26T23:11:02.232Z","level":"INFO","message":"[DIContainer] Registered module: ToolRunner","details":{}}
[2025-10-26T23:11:02.232Z] [INFO] {"timestamp":"2025-10-26T23:11:02.232Z","level":"INFO","message":"[CoreLogic] Registered module: ToolRunner from /upgrades/tool-runner.js","details":{}}
[2025-10-26T23:11:02.233Z] [INFO] {"timestamp":"2025-10-26T23:11:02.233Z","level":"INFO","message":"[DIContainer] Registered module: DiffGenerator","details":{}}
[2025-10-26T23:11:02.233Z] [INFO] {"timestamp":"2025-10-26T23:11:02.233Z","level":"INFO","message":"[CoreLogic] Registered module: DiffGenerator from /utils/diff-generator.js","details":{}}
[2025-10-26T23:11:02.233Z] [INFO] {"timestamp":"2025-10-26T23:11:02.233Z","level":"INFO","message":"[CoreLogic] Loading agent-logic-pure.js (size: 6050)","details":{}}
[2025-10-26T23:11:02.234Z] [INFO] {"timestamp":"2025-10-26T23:11:02.234Z","level":"INFO","message":"[CoreLogic] agent-logic-pure.js loaded:","details":{"hasModule":true,"hasMetadata":true,"moduleType":"object","moduleKeys":["metadata","factory"],"metadataId":"AgentLogicPureHelpers","factoryType":"function"}}
[2025-10-26T23:11:02.234Z] [INFO] {"timestamp":"2025-10-26T23:11:02.234Z","level":"INFO","message":"[DIContainer] Registered module: AgentLogicPureHelpers","details":{}}
[2025-10-26T23:11:02.234Z] [INFO] {"timestamp":"2025-10-26T23:11:02.234Z","level":"INFO","message":"[CoreLogic] Registered module: AgentLogicPureHelpers from /upgrades/agent-logic-pure.js","details":{}}
[2025-10-26T23:11:02.235Z] [INFO] {"timestamp":"2025-10-26T23:11:02.235Z","level":"INFO","message":"[DIContainer] Registered module: CycleLogic","details":{}}
[2025-10-26T23:11:02.235Z] [INFO] {"timestamp":"2025-10-26T23:11:02.235Z","level":"INFO","message":"[CoreLogic] Registered module: CycleLogic from /upgrades/agent-cycle.js","details":{}}
[2025-10-26T23:11:02.237Z] [INFO] {"timestamp":"2025-10-26T23:11:02.237Z","level":"INFO","message":"[DIContainer] Registered module: ContextManager","details":{}}
[2025-10-26T23:11:02.237Z] [INFO] {"timestamp":"2025-10-26T23:11:02.237Z","level":"INFO","message":"[CoreLogic] Registered module: ContextManager from /upgrades/context-manager.js","details":{}}
[2025-10-26T23:11:02.238Z] [INFO] {"timestamp":"2025-10-26T23:11:02.238Z","level":"INFO","message":"[DIContainer] Registered module: StreamingResponseHandler","details":{}}
[2025-10-26T23:11:02.238Z] [INFO] {"timestamp":"2025-10-26T23:11:02.238Z","level":"INFO","message":"[CoreLogic] Registered module: StreamingResponseHandler from /upgrades/streaming-response-handler.js","details":{}}
[2025-10-26T23:11:02.240Z] [INFO] {"timestamp":"2025-10-26T23:11:02.240Z","level":"INFO","message":"[DIContainer] Registered module: ReflectionStore","details":{}}
[2025-10-26T23:11:02.240Z] [INFO] {"timestamp":"2025-10-26T23:11:02.240Z","level":"INFO","message":"[CoreLogic] Registered module: ReflectionStore from /upgrades/reflection-store.js","details":{}}
[2025-10-26T23:11:02.241Z] [INFO] {"timestamp":"2025-10-26T23:11:02.241Z","level":"INFO","message":"[DIContainer] Registered module: PerformanceMonitor","details":{}}
[2025-10-26T23:11:02.241Z] [INFO] {"timestamp":"2025-10-26T23:11:02.241Z","level":"INFO","message":"[CoreLogic] Registered module: PerformanceMonitor from /upgrades/performance-monitor.js","details":{}}
[2025-10-26T23:11:02.242Z] [INFO] {"timestamp":"2025-10-26T23:11:02.242Z","level":"INFO","message":"[DIContainer] Registered module: ToastNotifications","details":{}}
[2025-10-26T23:11:02.242Z] [INFO] {"timestamp":"2025-10-26T23:11:02.242Z","level":"INFO","message":"[CoreLogic] Registered module: ToastNotifications from /upgrades/toast-notifications.js","details":{}}
[2025-10-26T23:11:02.244Z] [INFO] {"timestamp":"2025-10-26T23:11:02.244Z","level":"INFO","message":"[DIContainer] Registered module: BrowserAPIs","details":{}}
[2025-10-26T23:11:02.244Z] [INFO] {"timestamp":"2025-10-26T23:11:02.244Z","level":"INFO","message":"[CoreLogic] Registered module: BrowserAPIs from /upgrades/browser-apis.js","details":{}}
[2025-10-26T23:11:02.245Z] [INFO] {"timestamp":"2025-10-26T23:11:02.245Z","level":"INFO","message":"[DIContainer] Registered module: PyodideRuntime","details":{}}
[2025-10-26T23:11:02.245Z] [INFO] {"timestamp":"2025-10-26T23:11:02.245Z","level":"INFO","message":"[CoreLogic] Registered module: PyodideRuntime from /upgrades/pyodide-runtime.js","details":{}}
[2025-10-26T23:11:02.246Z] [INFO] {"timestamp":"2025-10-26T23:11:02.246Z","level":"INFO","message":"[DIContainer] Registered module: Introspector","details":{}}
[2025-10-26T23:11:02.246Z] [INFO] {"timestamp":"2025-10-26T23:11:02.246Z","level":"INFO","message":"[CoreLogic] Registered module: Introspector from /upgrades/introspector.js","details":{}}
[2025-10-26T23:11:02.248Z] [INFO] {"timestamp":"2025-10-26T23:11:02.248Z","level":"INFO","message":"[DIContainer] Registered module: SelfTester","details":{}}
[2025-10-26T23:11:02.248Z] [INFO] {"timestamp":"2025-10-26T23:11:02.248Z","level":"INFO","message":"[CoreLogic] Registered module: SelfTester from /upgrades/self-tester.js","details":{}}
[2025-10-26T23:11:02.249Z] [INFO] {"timestamp":"2025-10-26T23:11:02.249Z","level":"INFO","message":"[DIContainer] Registered module: VerificationManager","details":{}}
[2025-10-26T23:11:02.249Z] [INFO] {"timestamp":"2025-10-26T23:11:02.249Z","level":"INFO","message":"[CoreLogic] Registered module: VerificationManager from /upgrades/verification-manager.js","details":{}}
[2025-10-26T23:11:02.250Z] [INFO] {"timestamp":"2025-10-26T23:11:02.250Z","level":"INFO","message":"[DIContainer] Registered module: VFSExplorer","details":{}}
[2025-10-26T23:11:02.250Z] [INFO] {"timestamp":"2025-10-26T23:11:02.250Z","level":"INFO","message":"[CoreLogic] Registered module: VFSExplorer from /upgrades/vfs-explorer.js","details":{}}
[2025-10-26T23:11:02.251Z] [INFO] {"timestamp":"2025-10-26T23:11:02.251Z","level":"INFO","message":"[DIContainer] Registered module: MetricsDashboard","details":{}}
[2025-10-26T23:11:02.251Z] [INFO] {"timestamp":"2025-10-26T23:11:02.251Z","level":"INFO","message":"[CoreLogic] Registered module: MetricsDashboard from /upgrades/metrics-dashboard.js","details":{}}
[2025-10-26T23:11:02.252Z] [INFO] {"timestamp":"2025-10-26T23:11:02.252Z","level":"INFO","message":"[DIContainer] Registered module: AgentVisualizer","details":{}}
[2025-10-26T23:11:02.252Z] [INFO] {"timestamp":"2025-10-26T23:11:02.252Z","level":"INFO","message":"[CoreLogic] Registered module: AgentVisualizer from /upgrades/agent-visualizer.js","details":{}}
[2025-10-26T23:11:02.254Z] [INFO] {"timestamp":"2025-10-26T23:11:02.254Z","level":"INFO","message":"[DIContainer] Registered module: ASTVisualizer","details":{}}
[2025-10-26T23:11:02.254Z] [INFO] {"timestamp":"2025-10-26T23:11:02.254Z","level":"INFO","message":"[CoreLogic] Registered module: ASTVisualizer from /upgrades/ast-visualizer.js","details":{}}
[2025-10-26T23:11:02.255Z] [INFO] {"timestamp":"2025-10-26T23:11:02.255Z","level":"INFO","message":"[DIContainer] Registered module: ModuleGraphVisualizer","details":{}}
[2025-10-26T23:11:02.255Z] [INFO] {"timestamp":"2025-10-26T23:11:02.255Z","level":"INFO","message":"[CoreLogic] Registered module: ModuleGraphVisualizer from /upgrades/module-graph-visualizer.js","details":{}}
[2025-10-26T23:11:02.256Z] [INFO] {"timestamp":"2025-10-26T23:11:02.256Z","level":"INFO","message":"[DIContainer] Registered module: TutorialSystem","details":{}}
[2025-10-26T23:11:02.256Z] [INFO] {"timestamp":"2025-10-26T23:11:02.256Z","level":"INFO","message":"[CoreLogic] Registered module: TutorialSystem from /upgrades/tutorial-system.js","details":{}}
[2025-10-26T23:11:02.257Z] [LOG] [CoreLogic] Loaded module from /upgrades/progress-tracker.js: NO_METADATA
[2025-10-26T23:11:02.257Z] [ERROR] {"timestamp":"2025-10-26T23:11:02.257Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/progress-tracker.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T23:11:02.257Z] [WARN] {"timestamp":"2025-10-26T23:11:02.257Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/progress-tracker.js missing metadata. Module:","details":{}}
[2025-10-26T23:11:02.257Z] [LOG] [CoreLogic] Loaded module from /upgrades/log-panel.js: NO_METADATA
[2025-10-26T23:11:02.257Z] [ERROR] {"timestamp":"2025-10-26T23:11:02.257Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/log-panel.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T23:11:02.257Z] [WARN] {"timestamp":"2025-10-26T23:11:02.257Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/log-panel.js missing metadata. Module:","details":{}}
[2025-10-26T23:11:02.258Z] [LOG] [CoreLogic] Loaded module from /upgrades/status-bar.js: NO_METADATA
[2025-10-26T23:11:02.258Z] [ERROR] {"timestamp":"2025-10-26T23:11:02.258Z","level":"ERROR","message":"[CoreLogic] Module load failed for /upgrades/status-bar.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T23:11:02.258Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T23:11:02.258Z] [WARN] {"timestamp":"2025-10-26T23:11:02.258Z","level":"WARN","message":"[CoreLogic] Module at /upgrades/status-bar.js missing metadata. Module:","details":{}}
[2025-10-26T23:11:02.259Z] [INFO] {"timestamp":"2025-10-26T23:11:02.259Z","level":"INFO","message":"[DIContainer] Registered module: GoalPanel","details":{}}
[2025-10-26T23:11:02.259Z] [INFO] {"timestamp":"2025-10-26T23:11:02.259Z","level":"INFO","message":"[CoreLogic] Registered module: GoalPanel from /upgrades/goal-panel.js","details":{}}
[2025-10-26T23:11:02.260Z] [INFO] {"timestamp":"2025-10-26T23:11:02.260Z","level":"INFO","message":"[DIContainer] Registered module: ThoughtPanel","details":{}}
[2025-10-26T23:11:02.260Z] [INFO] {"timestamp":"2025-10-26T23:11:02.260Z","level":"INFO","message":"[CoreLogic] Registered module: ThoughtPanel from /upgrades/thought-panel.js","details":{}}
[2025-10-26T23:11:02.261Z] [INFO] {"timestamp":"2025-10-26T23:11:02.261Z","level":"INFO","message":"[DIContainer] Registered module: SentinelFSM","details":{}}
[2025-10-26T23:11:02.261Z] [INFO] {"timestamp":"2025-10-26T23:11:02.261Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelFSM from /upgrades/sentinel-fsm.js","details":{}}
[2025-10-26T23:11:02.263Z] [INFO] {"timestamp":"2025-10-26T23:11:02.263Z","level":"INFO","message":"[DIContainer] Registered module: SentinelTools","details":{}}
[2025-10-26T23:11:02.263Z] [INFO] {"timestamp":"2025-10-26T23:11:02.263Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelTools from /upgrades/sentinel-tools.js","details":{}}
[2025-10-26T23:11:02.264Z] [INFO] {"timestamp":"2025-10-26T23:11:02.264Z","level":"INFO","message":"[DIContainer] Registered module: SentinelPanel","details":{}}
[2025-10-26T23:11:02.264Z] [INFO] {"timestamp":"2025-10-26T23:11:02.264Z","level":"INFO","message":"[CoreLogic] Registered module: SentinelPanel from /upgrades/sentinel-panel.js","details":{}}
[2025-10-26T23:11:02.265Z] [INFO] {"timestamp":"2025-10-26T23:11:02.265Z","level":"INFO","message":"[DIContainer] Registered module: MetaToolCreator","details":{}}
[2025-10-26T23:11:02.265Z] [INFO] {"timestamp":"2025-10-26T23:11:02.265Z","level":"INFO","message":"[CoreLogic] Registered module: MetaToolCreator from /upgrades/meta-tool-creator.js","details":{}}
[2025-10-26T23:11:02.266Z] [INFO] {"timestamp":"2025-10-26T23:11:02.266Z","level":"INFO","message":"[DIContainer] Registered module: ModelArena","details":{}}
[2025-10-26T23:11:02.266Z] [INFO] {"timestamp":"2025-10-26T23:11:02.266Z","level":"INFO","message":"[CoreLogic] Registered module: ModelArena from /upgrades/model-arena.js","details":{}}
[2025-10-26T23:11:02.269Z] [LOG] [CoreLogic] Loaded module from /upgrades/ui-manager.js: UI
[2025-10-26T23:11:02.269Z] [INFO] {"timestamp":"2025-10-26T23:11:02.269Z","level":"INFO","message":"[DIContainer] Registered module: UI","details":{}}
[2025-10-26T23:11:02.269Z] [INFO] {"timestamp":"2025-10-26T23:11:02.269Z","level":"INFO","message":"[CoreLogic] Registered module: UI from /upgrades/ui-manager.js","details":{}}
[2025-10-26T23:11:02.270Z] [INFO] {"timestamp":"2025-10-26T23:11:02.270Z","level":"INFO","message":"[DIContainer] Registered module: base-persona","details":{}}
[2025-10-26T23:11:02.270Z] [INFO] {"timestamp":"2025-10-26T23:11:02.270Z","level":"INFO","message":"[CoreLogic] Registered module: base-persona from /personas/base-persona.js","details":{}}
[2025-10-26T23:11:02.270Z] [LOG] [CoreLogic] Loaded module from /personas/CodeRefactorerPersona.js: NO_METADATA
[2025-10-26T23:11:02.270Z] [ERROR] {"timestamp":"2025-10-26T23:11:02.270Z","level":"ERROR","message":"[CoreLogic] Module load failed for /personas/CodeRefactorerPersona.js:","details":{"hasModule":true,"hasMetadata":false,"moduleKeys":["register","resolve"]}}
[2025-10-26T23:11:02.270Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T23:11:02.270Z] [WARN] {"timestamp":"2025-10-26T23:11:02.270Z","level":"WARN","message":"[CoreLogic] Module at /personas/CodeRefactorerPersona.js missing metadata. Module:","details":{}}
[2025-10-26T23:11:02.270Z] [INFO] {"timestamp":"2025-10-26T23:11:02.270Z","level":"INFO","message":"[CoreLogic] All modules registered. Resolving main services.","details":{}}
[2025-10-26T23:11:02.271Z] [INFO] {"timestamp":"2025-10-26T23:11:02.271Z","level":"INFO","message":"[Storage-Git] Initializing Git-powered VFS in IndexedDB...","details":{}}
[2025-10-26T23:11:02.715Z] [INFO] {"timestamp":"2025-10-26T23:11:02.715Z","level":"INFO","message":"[Storage-Git] Existing Git repository found.","details":{}}
[2025-10-26T23:11:02.716Z] [INFO] {"timestamp":"2025-10-26T23:11:02.716Z","level":"INFO","message":"[AuditLogger] Audit logging system initialized","details":{}}
[2025-10-26T23:11:02.717Z] [INFO] {"timestamp":"2025-10-26T23:11:02.716Z","level":"INFO","message":"[StateManager-Git] Initializing state...","details":{}}
[2025-10-26T23:11:02.717Z] [WARN] {"timestamp":"2025-10-26T23:11:02.717Z","level":"WARN","message":"[StateManager-Git] No saved state found. Creating minimal state.","details":{}}
[2025-10-26T23:11:02.717Z] [INFO] {"timestamp":"2025-10-26T23:11:02.717Z","level":"INFO","message":"[RateLimiter] Created api limiter","details":{"maxTokens":5,"refillRate":0.16666666666666666}}
[2025-10-26T23:11:02.717Z] [INFO] {"timestamp":"2025-10-26T23:11:02.717Z","level":"INFO","message":"[RateLimiter] Created strict limiter","details":{"maxRequests":20,"windowMs":60000}}
[2025-10-26T23:11:02.717Z] [WARN] {"timestamp":"2025-10-26T23:11:02.717Z","level":"WARN","message":"[ApiClient] Rate limiting not available - requests unlimited","details":{}}
[2025-10-26T23:11:02.718Z] [INFO] {"timestamp":"2025-10-26T23:11:02.718Z","level":"INFO","message":"[LocalLLM] Initializing WebLLM runtime...","details":{}}
[2025-10-26T23:11:02.719Z] [WARN] {"timestamp":"2025-10-26T23:11:02.719Z","level":"WARN","message":"[LocalLLM] WebGPU unavailable: No WebGPU adapter found","details":{}}
[2025-10-26T23:11:02.719Z] [INFO] {"timestamp":"2025-10-26T23:11:02.719Z","level":"INFO","message":"[StreamingResponseHandler] Initializing streaming response handler...","details":{}}
[2025-10-26T23:11:02.719Z] [INFO] {"timestamp":"2025-10-26T23:11:02.719Z","level":"INFO","message":"[StreamingResponseHandler] Module initialized successfully","details":{}}
[2025-10-26T23:11:02.720Z] [INFO] {"timestamp":"2025-10-26T23:11:02.720Z","level":"INFO","message":"[HybridLLM] Initializing hybrid LLM provider","details":{}}
[2025-10-26T23:11:02.723Z] [INFO] {"timestamp":"2025-10-26T23:11:02.723Z","level":"INFO","message":"[ReflectionStore] Initializing reflection persistence","details":{}}
[2025-10-26T23:11:02.723Z] [INFO] {"timestamp":"2025-10-26T23:11:02.723Z","level":"INFO","message":"[ReflectionStore] Database opened successfully","details":{}}
[2025-10-26T23:11:02.724Z] [INFO] {"timestamp":"2025-10-26T23:11:02.724Z","level":"INFO","message":"[SelfTester] Initialized","details":{}}
[2025-10-26T23:11:02.724Z] [INFO] {"timestamp":"2025-10-26T23:11:02.724Z","level":"INFO","message":"[BrowserAPIs] Initializing web API integration...","details":{}}
[2025-10-26T23:11:02.724Z] [INFO] {"timestamp":"2025-10-26T23:11:02.724Z","level":"INFO","message":"[BrowserAPIs] Capabilities detected:","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-26T23:11:02.725Z] [INFO] {"timestamp":"2025-10-26T23:11:02.724Z","level":"INFO","message":"[EventBus] Emitting event: browser-apis:initialized","details":{"fileSystemAccess":true,"notifications":true,"clipboard":true,"webShare":false,"storageEstimation":true,"wakeLock":true}}
[2025-10-26T23:11:02.726Z] [ERROR] [CoreLogic] Initialization failed: Error: [DIContainer] Failed to resolve dependency 'AgentVisualizer' for module 'UI'.
Dependency chain: UI  AgentVisualizer
Original error: [DIContainer] Failed to resolve dependency 'SentinelFSM' for module 'AgentVisualizer'.
Dependency chain: AgentVisualizer  SentinelFSM
Original error: [DIContainer] Failed to resolve dependency 'GitVFS' for module 'SentinelFSM'.
Dependency chain: SentinelFSM  GitVFS
Original error: [DIContainer] Service not found: GitVFS
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI, base-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'AgentVisualizer' for module 'UI'.
Dependency chain: UI  AgentVisualizer
Original error: [DIContainer] Failed to resolve dependency 'SentinelFSM' for module 'AgentVisualizer'.
Dependency chain: AgentVisualizer  SentinelFSM
Original error: [DIContainer] Failed to resolve dependency 'GitVFS' for module 'SentinelFSM'.
Dependency chain: SentinelFSM  GitVFS
Original error: [DIContainer] Service not found: GitVFS
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI, base-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:300:16)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T23:11:02.726Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T23:11:02.726Z] [ERROR] [Boot] Failed to awaken agent: Error: [DIContainer] Failed to resolve dependency 'AgentVisualizer' for module 'UI'.
Dependency chain: UI  AgentVisualizer
Original error: [DIContainer] Failed to resolve dependency 'SentinelFSM' for module 'AgentVisualizer'.
Dependency chain: AgentVisualizer  SentinelFSM
Original error: [DIContainer] Failed to resolve dependency 'GitVFS' for module 'SentinelFSM'.
Dependency chain: SentinelFSM  GitVFS
Original error: [DIContainer] Service not found: GitVFS
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI, base-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Error: [DIContainer] Failed to resolve dependency 'AgentVisualizer' for module 'UI'.
Dependency chain: UI  AgentVisualizer
Original error: [DIContainer] Failed to resolve dependency 'SentinelFSM' for module 'AgentVisualizer'.
Dependency chain: AgentVisualizer  SentinelFSM
Original error: [DIContainer] Failed to resolve dependency 'GitVFS' for module 'SentinelFSM'.
Dependency chain: SentinelFSM  GitVFS
Original error: [DIContainer] Service not found: GitVFS
Available services: config, Persona, Utils, EventBus, DIContainer, StateHelpersPure, ToolRunnerPureHelpers, Config, Storage, AuditLogger, RateLimiter, ModelRegistry, StateManager, LocalLLM, HybridLLMProvider, ApiClient, ToolRunner, DiffGenerator, AgentLogicPureHelpers, CycleLogic, ContextManager, StreamingResponseHandler, ReflectionStore, PerformanceMonitor, ToastNotifications, BrowserAPIs, PyodideRuntime, Introspector, SelfTester, VerificationManager, VFSExplorer, MetricsDashboard, AgentVisualizer, ASTVisualizer, ModuleGraphVisualizer, TutorialSystem, GoalPanel, ThoughtPanel, SentinelFSM, SentinelTools, SentinelPanel, MetaToolCreator, ModelArena, UI, base-persona
Tip: Check module ID spelling and ensure the module is registered in config.json
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
Check for circular dependencies or missing module registrations.
    at Object.resolve (eval at CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33)), <anonymous>:102:21)
    at async CoreLogicModule (eval at awakenAgent (http://localhost:8080/boot.js:781:33), <anonymous>:300:16)
    at async HTMLButtonElement.awakenAgent (http://localhost:8080/boot.js:801:9)
[2025-10-26T23:11:02.726Z] [WARN] [LiveReload] Too many errors, disabling auto-reload to prevent loops
[2025-10-26T23:11:50.254Z] [LOG] [ConsoleMonitor] Browser console monitoring enabled
[2025-10-26T23:11:50.254Z] [LOG] [LiveReload] Auto-reload enabled
[2025-10-26T23:11:50.257Z] [LOG] [Config] Loaded settings: {
  "selectedModel": "gpt-oss:120b",
  "aiProvider": "ollama",
  "selectedMode": "cloud"
}
[2025-10-26T23:11:50.257Z] [LOG] [API] Checking server status...
[2025-10-26T23:11:50.269Z] [LOG] [API] Server online: {
  "status": "ok",
  "providers": [
    "gemini",
    "local"
  ],
  "primaryProvider": "gemini",
  "ollama": {
    "status": "running",
    "endpoint": "http://localhost:11434"
  },
  "ollamaStatus": "running",
  "timestamp": "2025-10-26T23:11:50.259Z"
}
[2025-10-26T23:11:50.269Z] [LOG] [API] WebGPU available
[2025-10-26T23:11:50.291Z] [LOG] [Boot] Proxy server available: true
[2025-10-26T23:11:50.291Z] [LOG] [Boot] Initializing model configuration UI...
[2025-10-26T23:11:50.291Z] [LOG] [ModelConfig] Initializing card-based model selector...
[2025-10-26T23:11:50.309Z] [LOG] [ModelConfig] Loaded saved models: [
  {
    "id": "gpt-oss:120b",
    "name": "gpt-oss:120b",
    "provider": "ollama",
    "hostType": "ollama-proxy",
    "queryMethod": "proxy",
    "keySource": "none",
    "keyId": null
  }
]
